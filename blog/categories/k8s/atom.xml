<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: k8s | Earlene]]></title>
  <link href="http://liyongxin.github.io/blog/categories/k8s/atom.xml" rel="self"/>
  <link href="http://liyongxin.github.io/"/>
  <updated>2019-06-27T19:27:31+08:00</updated>
  <id>http://liyongxin.github.io/</id>
  <author>
    <name><![CDATA[yxli@alauda.io]]></name>
    <email><![CDATA[yxli@alauda.io]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[记录一次高可用集群使用kubeadm v1.9.6升级v1.13 的问题]]></title>
    <link href="http://liyongxin.github.io/blog/2019/04/22/ji-lu-yi-ci-kubeadm-v1-dot-12sheng-ji-v1-dot-13-de-wen-ti/"/>
    <updated>2019-04-22T14:12:52+08:00</updated>
    <id>http://liyongxin.github.io/blog/2019/04/22/ji-lu-yi-ci-kubeadm-v1-dot-12sheng-ji-v1-dot-13-de-wen-ti</id>
    <content type="html"><![CDATA[<p>项目中使用kubeadm 将k8s版本从v1.9.6升级到1.13.4,由于无法跨版本升级，所以大致流程是</p>

<ul>
<li>v1.9.6 -> v1.10.8</li>
<li>v1.10.8 -> v1.11.5</li>
<li>v1.11.5 -> v1.12.4</li>
<li>v1.12.4 -> v1.13.4</li>
</ul>


<p>其中操作集群A从<code>v1.9.6</code>到<code>v1.10.8</code>升级过程中，执行<code>upgrade</code>的时候出现了如下问题：
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@cloud-cn-master-1 ~<span class="o">]</span><span class="c"># kubeadm upgrade apply v1.10.8 -y</span>
<span class="p">&amp;</span>hellip<span class="p">;</span>
<span class="o">[</span>upgrade/prepull<span class="o">]</span> Successfully prepulled the images <span class="k">for</span> all the control plane components
<span class="o">[</span>upgrade/apply<span class="o">]</span> Upgrading your Static Pod-hosted control plane to version <span class="p">&amp;</span>ldquo<span class="p">;</span>v1.10.8<span class="p">&amp;</span>rdquo<span class="p">;&amp;</span>hellip<span class="p">;</span>
Static pod: kube-apiserver-rz-dev-master01 <span class="nb">hash</span>: a82830fd687fdabd030b65ee6c4b4fd4
Static pod: kube-controller-manager-rz-dev-master01 <span class="nb">hash</span>: 1a23c184fadb64c889a41831476c56e8
Static pod: kube-scheduler-rz-dev-master01 <span class="nb">hash</span>: a0adc2bf23e7d5336ecd4677ce95938c
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Writing new Static Pod manifests to <span class="p">&amp;</span>ldquo<span class="p">;</span>/etc/kubernetes/tmp/kubeadm-upgraded-manifests500670946<span class="p">&amp;</span>rdquo<span class="p">;</span>
<span class="o">[</span>controlplane<span class="o">]</span> Adding extra host path mount <span class="p">&amp;</span>ldquo<span class="p">;</span>k8s<span class="p">&amp;</span>rdquo<span class="p">;</span> to <span class="p">&amp;</span>ldquo<span class="p">;</span>kube-controller-manager<span class="p">&amp;</span>rdquo<span class="p">;</span>
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> current and new manifests of kube-apiserver are equal, skipping upgrade
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Moved new manifest to <span class="p">&amp;</span>ldquo<span class="p">;</span>/etc/kubernetes/manifests/kube-controller-manager.yaml<span class="p">&amp;</span>rdquo<span class="p">;</span> and backed up old manifest to <span class="p">&amp;</span>ldquo<span class="p">;</span>/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-04-22-13-04-58/kube-controller-manager.yaml<span class="p">&amp;</span>rdquo<span class="p">;</span>
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Waiting <span class="k">for</span> the kubelet to restart the component
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> This might take a minute or longer depending on the component/version gap <span class="o">(</span>timeout 5m0s<span class="o">)</span>
Static pod: kube-controller-manager-rz-dev-master01 <span class="nb">hash</span>: 1a23c184fadb64c889a41831476c56e8
Static pod: kube-controller-manager-rz-dev-master01 <span class="nb">hash</span>: 1a23c184fadb64c889a41831476c56e8
Static pod: kube-controller-manager-rz-dev-master01 <span class="nb">hash</span>: 1a23c184fadb64c889a41831476c56e8
Static pod: kube-controller-manager-rz-dev-master01 <span class="nb">hash</span>: 1a23c184fadb64c889a41831476c56e8
Static pod: kube-controller-manager-rz-dev-master01 <span class="nb">hash</span>: 1a23c184fadb64c889a41831476c56e8
<span class="p">&amp;</span>hellip<span class="p">;</span></code></pre></div></p>

<!--more-->


<p>升级过程中一直卡在校验<code>hash</code>这步，由于之前测试没有出现类似情况，而此次出问题和测试环境唯一的区别就是该环境升级过证书，
默认证书有效期是1年，此环境更新成30年了，所以尝试先把证书还原后再次执行<code>upgrade</code>，升级成功，具体原因未排查。</p>

<p>在另一个环境从<code>v1.12</code>升级到<code>v1.13.4</code>版本时候，出现了如下情况：
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@cloud-cn-master-1 ~<span class="o">]</span><span class="c"># kubeadm upgrade apply v1.13.4 -y</span>
<span class="o">[</span>preflight<span class="o">]</span> Running pre-flight checks.
<span class="o">[</span>upgrade<span class="o">]</span> Making sure the cluster is healthy:
<span class="o">[</span>upgrade/config<span class="o">]</span> Making sure the configuration is correct:
<span class="o">[</span>upgrade/config<span class="o">]</span> Reading configuration from the cluster<span class="p">&amp;</span>hellip<span class="p">;</span>
<span class="o">[</span>upgrade/config<span class="o">]</span> FYI: You can look at this config file with <span class="p">&amp;</span>lsquo<span class="p">;</span>kubectl -n kube-system get cm kubeadm-config -oyaml<span class="p">&amp;</span>rsquo<span class="p">;</span>
FATAL: failed to get node registration: node doesn<span class="p">&amp;</span>rsquo<span class="p">;</span>t have kubeadm.alpha.kubernetes.io/cri-socket annotation</code></pre></div></p>

<p>报错提示<code>node</code>缺少<code>annotation</code>，而查看操作<code>kubeadm</code>升级的<code>master node</code>，是存在<code>cri-socket</code>的<code>annotation</code>的，但是其他两台<code>master</code>
不存在，于是尝试手动添加<code>annotation</code>，然后再次跑<code>upgrade</code>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@cloud-cn-master-1 ~<span class="o">]</span><span class="c"># kubectl annotate node &lt;nodename&gt; kubeadm.alpha.kubernetes.io/cri-socket=/var/run/dockershim.sock</span>
<span class="o">[</span>root@cloud-cn-master-1 ~<span class="o">]</span><span class="c"># kubeadm upgrade apply v1.13.4 -y</span>
<span class="o">[</span>preflight<span class="o">]</span> Running pre-flight checks.
<span class="o">[</span>upgrade<span class="o">]</span> Making sure the cluster is healthy:
<span class="o">[</span>upgrade/config<span class="o">]</span> Making sure the configuration is correct:
<span class="o">[</span>upgrade/config<span class="o">]</span> Reading configuration from the cluster<span class="p">&amp;</span>hellip<span class="p">;</span>
<span class="o">[</span>upgrade/config<span class="o">]</span> FYI: You can look at this config file with <span class="p">&amp;</span>lsquo<span class="p">;</span>kubectl -n kube-system get cm kubeadm-config -oyaml<span class="p">&amp;</span>rsquo<span class="p">;</span>
<span class="o">[</span>upgrade/config<span class="o">]</span> FATAL: failed to getAPIEndpoint: failed to get APIEndpoint information <span class="k">for</span> this node</code></pre></div>
又报错找不到<code>APIEndpoint</code>，尝试执行看下<code>kubeadm-config</code>的数据
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@cloud-cn-master-1 ~<span class="o">]</span><span class="c"># kubectl -n kube-system get cm kubeadm-config -oyaml|grep -A5 cloud-cn-master-1</span>
  ClusterStatus: <span class="p">|</span>
    apiEndpoints:
      cloud-cn-master-1:
        advertiseAddress: 10.0.128.251
        bindPort: 6443
    apiVersion: kubeadm.k8s.io/v1beta1
    kind: ClusterStatus</code></pre></div></p>

<p>发现该<code>apiEndpoints</code>是存在的，但是只存在这一个节点，于是尝试<code>edit cm</code>把另外两个节点的<code>apiEndpoint</code>也配置上，再次<code>upgrade</code>，
发现又出现了校验<code>hash</code>不通过的错误，于是尝试去看下<code>kubeadm</code>的源码，理清楚这个<code>config</code>阶段的思路，
源码可以参考<a href="https://github.com/kubernetes/kubernetes/blob/master/cmd/kubeadm/app/util/config/cluster.go">此处</a>,
<div class="highlight"><pre><code class="language-go" data-lang="go"><span class="c1">// if this isn&amp;rsquo;t a new controlplane instance (e.g. in case of kubeadm upgrades)</span>
<span class="c1">// get nodes specific information as well</span>
<span class="k">if</span> <span class="p">!</span><span class="nx">newControlPlane</span> <span class="p">{</span>
    <span class="c1">// gets the nodeRegistration for the current from the node object</span>
    <span class="k">if</span> <span class="nx">err</span> <span class="o">:=</span> <span class="nx">getNodeRegistration</span><span class="p">(</span><span class="nx">kubeconfigDir</span><span class="p">,</span> <span class="nx">client</span><span class="p">,</span> <span class="o">&amp;</span><span class="nx">amp</span><span class="p">;</span><span class="nx">initcfg</span><span class="p">.</span><span class="nx">NodeRegistration</span><span class="p">);</span> <span class="nx">err</span> <span class="o">!=</span> <span class="kc">nil</span> <span class="p">{</span>
        <span class="k">return</span> <span class="kc">nil</span><span class="p">,</span> <span class="nx">errors</span><span class="p">.</span><span class="nx">Wrap</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="o">&amp;</span><span class="nx">ldquo</span><span class="p">;</span><span class="nx">failed</span> <span class="nx">to</span> <span class="nx">get</span> <span class="nx">node</span> <span class="nx">registration</span><span class="o">&amp;</span><span class="nx">rdquo</span><span class="p">;)</span>
    <span class="p">}</span>
    <span class="c1">// gets the APIEndpoint for the current node from then ClusterStatus in the kubeadm-config ConfigMap</span>
    <span class="k">if</span> <span class="nx">err</span> <span class="o">:=</span> <span class="nx">getAPIEndpoint</span><span class="p">(</span><span class="nx">configMap</span><span class="p">.</span><span class="nx">Data</span><span class="p">,</span> <span class="nx">initcfg</span><span class="p">.</span><span class="nx">NodeRegistration</span><span class="p">.</span><span class="nx">Name</span><span class="p">,</span> <span class="o">&amp;</span><span class="nx">amp</span><span class="p">;</span><span class="nx">initcfg</span><span class="p">.</span><span class="nx">LocalAPIEndpoint</span><span class="p">);</span> <span class="nx">err</span> <span class="o">!=</span> <span class="kc">nil</span> <span class="p">{</span>
        <span class="k">return</span> <span class="kc">nil</span><span class="p">,</span> <span class="nx">errors</span><span class="p">.</span><span class="nx">Wrap</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="o">&amp;</span><span class="nx">ldquo</span><span class="p">;</span><span class="nx">failed</span> <span class="nx">to</span> <span class="nx">getAPIEndpoint</span><span class="o">&amp;</span><span class="nx">rdquo</span><span class="p">;)</span>
    <span class="p">}</span>
<span class="p">}</span></code></pre></div></p>

<p>该部分即对应执行<code>upgrade</code>的逻辑，先去<code>getNodeRegistration</code>然后去<code>getAPIEndpoint</code>，看下getAPIEndpoint的逻辑
<div class="highlight"><pre><code class="language-go" data-lang="go"><span class="c1">// getAPIEndpoint returns the APIEndpoint for the current node</span>
<span class="kd">func</span> <span class="nx">getAPIEndpoint</span><span class="p">(</span><span class="nx">data</span> <span class="kd">map</span><span class="p">[</span><span class="kt">string</span><span class="p">]</span><span class="kt">string</span><span class="p">,</span> <span class="nx">nodeName</span> <span class="kt">string</span><span class="p">,</span> <span class="nx">apiEndpoint</span> <span class="o">*</span><span class="nx">kubeadmapi</span><span class="p">.</span><span class="nx">APIEndpoint</span><span class="p">)</span> <span class="kt">error</span> <span class="p">{</span>
    <span class="c1">// gets the ClusterStatus from kubeadm-config</span>
    <span class="nx">clusterStatus</span><span class="p">,</span> <span class="nx">err</span> <span class="o">:=</span> <span class="nx">UnmarshalClusterStatus</span><span class="p">(</span><span class="nx">data</span><span class="p">)</span>
    <span class="k">if</span> <span class="nx">err</span> <span class="o">!=</span> <span class="kc">nil</span> <span class="p">{</span>
        <span class="k">return</span> <span class="nx">err</span>
    <span class="p">}&lt;</span><span class="o">/</span><span class="nx">p</span><span class="p">&gt;</span>

<span class="p">&lt;</span><span class="nx">pre</span><span class="p">&gt;&lt;</span><span class="nx">code</span><span class="p">&gt;</span><span class="c1">// gets the APIEndpoint for the current machine from the ClusterStatus</span>
<span class="nx">e</span><span class="p">,</span> <span class="nx">ok</span> <span class="o">:=</span> <span class="nx">clusterStatus</span><span class="p">.</span><span class="nx">APIEndpoints</span><span class="p">[</span><span class="nx">nodeName</span><span class="p">]</span>
<span class="k">if</span> <span class="p">!</span><span class="nx">ok</span> <span class="p">{</span>
    <span class="k">return</span> <span class="nx">errors</span><span class="p">.</span><span class="nx">New</span><span class="p">(</span><span class="s">&quot;failed to get APIEndpoint information for this node&quot;</span><span class="p">)</span>
<span class="p">}</span>

<span class="nx">apiEndpoint</span><span class="p">.</span><span class="nx">AdvertiseAddress</span> <span class="p">=</span> <span class="nx">e</span><span class="p">.</span><span class="nx">AdvertiseAddress</span>
<span class="nx">apiEndpoint</span><span class="p">.</span><span class="nx">BindPort</span> <span class="p">=</span> <span class="nx">e</span><span class="p">.</span><span class="nx">BindPort</span>
<span class="k">return</span> <span class="kc">nil</span>
<span class="p">&lt;</span><span class="o">/</span><span class="nx">code</span><span class="p">&gt;&lt;</span><span class="o">/</span><span class="nx">pre</span><span class="p">&gt;</span>

<span class="p">&lt;</span><span class="nx">p</span><span class="p">&gt;}</span></code></pre></div>
会根据<code>nodeName</code>和<code>kubeadm-config</code>这个<code>configmap</code>的数据去拿<code>APIEndpoint</code>的<code>AdvertiseAddress</code>和<code>BindPort</code>信息，但是手动确认过确实是存在
<code>APIEndpoint</code>的配置的，所以再次查看传过来的<code>nodeName</code>是否正确，由于<code>nodeName</code>是从<code>NodeRegistration</code>中获取的先看下<code>NodeRegistration</code>的获取逻辑:</p>

<p><div class="highlight"><pre><code class="language-go" data-lang="go"><span class="c1">// getNodeRegistration returns the nodeRegistration for the current node</span>
<span class="kd">func</span> <span class="nx">getNodeRegistration</span><span class="p">(</span><span class="nx">kubeconfigDir</span> <span class="kt">string</span><span class="p">,</span> <span class="nx">client</span> <span class="nx">clientset</span><span class="p">.</span><span class="nx">Interface</span><span class="p">,</span> <span class="nx">nodeRegistration</span> <span class="o">*</span><span class="nx">kubeadmapi</span><span class="p">.</span><span class="nx">NodeRegistrationOptions</span><span class="p">)</span> <span class="kt">error</span> <span class="p">{</span>
    <span class="c1">// gets the name of the current node</span>
    <span class="nx">nodeName</span><span class="p">,</span> <span class="nx">err</span> <span class="o">:=</span> <span class="nx">getNodeNameFromKubeletConfig</span><span class="p">(</span><span class="nx">kubeconfigDir</span><span class="p">)</span>
    <span class="k">if</span> <span class="nx">err</span> <span class="o">!=</span> <span class="kc">nil</span> <span class="p">{</span>
        <span class="k">return</span> <span class="nx">errors</span><span class="p">.</span><span class="nx">Wrap</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="o">&amp;</span><span class="nx">ldquo</span><span class="p">;</span><span class="nx">failed</span> <span class="nx">to</span> <span class="nx">get</span> <span class="nx">node</span> <span class="nx">name</span> <span class="nx">from</span> <span class="nx">kubelet</span> <span class="nx">config</span><span class="o">&amp;</span><span class="nx">rdquo</span><span class="p">;)</span>
    <span class="p">}&lt;</span><span class="o">/</span><span class="nx">p</span><span class="p">&gt;</span>

<span class="p">&lt;</span><span class="nx">pre</span><span class="p">&gt;&lt;</span><span class="nx">code</span><span class="p">&gt;</span><span class="c1">// gets the corresponding node and retrieves attributes stored there.</span>
<span class="nx">node</span><span class="p">,</span> <span class="nx">err</span> <span class="o">:=</span> <span class="nx">client</span><span class="p">.</span><span class="nx">CoreV1</span><span class="p">().</span><span class="nx">Nodes</span><span class="p">().</span><span class="nx">Get</span><span class="p">(</span><span class="nx">nodeName</span><span class="p">,</span> <span class="nx">metav1</span><span class="p">.</span><span class="nx">GetOptions</span><span class="p">{})</span>
<span class="k">if</span> <span class="nx">err</span> <span class="o">!=</span> <span class="kc">nil</span> <span class="p">{</span>
    <span class="k">return</span> <span class="nx">errors</span><span class="p">.</span><span class="nx">Wrap</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="s">&quot;failed to get corresponding node&quot;</span><span class="p">)</span>
<span class="p">}</span>

<span class="nx">criSocket</span><span class="p">,</span> <span class="nx">ok</span> <span class="o">:=</span> <span class="nx">node</span><span class="p">.</span><span class="nx">ObjectMeta</span><span class="p">.</span><span class="nx">Annotations</span><span class="p">[</span><span class="nx">constants</span><span class="p">.</span><span class="nx">AnnotationKubeadmCRISocket</span><span class="p">]</span>
<span class="k">if</span> <span class="p">!</span><span class="nx">ok</span> <span class="p">{</span>
    <span class="k">return</span> <span class="nx">errors</span><span class="p">.</span><span class="nx">Errorf</span><span class="p">(</span><span class="s">&quot;node %s doesn&#39;t have %s annotation&quot;</span><span class="p">,</span> <span class="nx">nodeName</span><span class="p">,</span> <span class="nx">constants</span><span class="p">.</span><span class="nx">AnnotationKubeadmCRISocket</span><span class="p">)</span>
<span class="p">}</span>

<span class="c1">// returns the nodeRegistration attributes</span>
<span class="nx">nodeRegistration</span><span class="p">.</span><span class="nx">Name</span> <span class="p">=</span> <span class="nx">nodeName</span>
<span class="nx">nodeRegistration</span><span class="p">.</span><span class="nx">CRISocket</span> <span class="p">=</span> <span class="nx">criSocket</span>
<span class="nx">nodeRegistration</span><span class="p">.</span><span class="nx">Taints</span> <span class="p">=</span> <span class="nx">node</span><span class="p">.</span><span class="nx">Spec</span><span class="p">.</span><span class="nx">Taints</span>
<span class="c1">// NB. currently nodeRegistration.KubeletExtraArgs isn&#39;t stored at node level but only in the kubeadm-flags.env</span>
<span class="c1">//     that isn&#39;t modified during upgrades</span>
<span class="c1">//     in future we might reconsider this thus enabling changes to the kubeadm-flags.env during upgrades as well</span>
<span class="k">return</span> <span class="kc">nil</span>
<span class="p">&lt;</span><span class="o">/</span><span class="nx">code</span><span class="p">&gt;&lt;</span><span class="o">/</span><span class="nx">pre</span><span class="p">&gt;</span>

<span class="p">&lt;</span><span class="nx">p</span><span class="p">&gt;}</span></code></pre></div>
发现<code>nodeName</code>是通过<code>getNodeNameFromKubeletConfig</code>获取的，也就是说读取的是<code>kubelet.conf</code>配置，看下<code>getNodeNameFromKubeletConfig</code>逻辑：
<div class="highlight"><pre><code class="language-go" data-lang="go"><span class="c1">// getNodeNameFromConfig gets the node name from a kubelet config file</span>
<span class="c1">// TODO: in future we want to switch to a more canonical way for doing this e.g. by having this</span>
<span class="c1">//       information in the local kubelet config.yaml</span>
<span class="kd">func</span> <span class="nx">getNodeNameFromKubeletConfig</span><span class="p">(</span><span class="nx">kubeconfigDir</span> <span class="kt">string</span><span class="p">)</span> <span class="p">(</span><span class="kt">string</span><span class="p">,</span> <span class="kt">error</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// loads the kubelet.conf file</span>
    <span class="nx">fileName</span> <span class="o">:=</span> <span class="nx">filepath</span><span class="p">.</span><span class="nx">Join</span><span class="p">(</span><span class="nx">kubeconfigDir</span><span class="p">,</span> <span class="nx">constants</span><span class="p">.</span><span class="nx">KubeletKubeConfigFileName</span><span class="p">)</span>
    <span class="nx">config</span><span class="p">,</span> <span class="nx">err</span> <span class="o">:=</span> <span class="nx">clientcmd</span><span class="p">.</span><span class="nx">LoadFromFile</span><span class="p">(</span><span class="nx">fileName</span><span class="p">)</span>
    <span class="k">if</span> <span class="nx">err</span> <span class="o">!=</span> <span class="kc">nil</span> <span class="p">{</span>
        <span class="k">return</span> <span class="o">&amp;</span><span class="nx">ldquo</span><span class="p">;</span><span class="o">&amp;</span><span class="nx">rdquo</span><span class="p">;,</span> <span class="nx">err</span>
    <span class="p">}&lt;</span><span class="o">/</span><span class="nx">p</span><span class="p">&gt;</span>

<span class="p">&lt;</span><span class="nx">pre</span><span class="p">&gt;&lt;</span><span class="nx">code</span><span class="p">&gt;</span><span class="c1">// gets the info about the current user</span>
<span class="nx">authInfo</span> <span class="o">:=</span> <span class="nx">config</span><span class="p">.</span><span class="nx">AuthInfos</span><span class="p">[</span><span class="nx">config</span><span class="p">.</span><span class="nx">Contexts</span><span class="p">[</span><span class="nx">config</span><span class="p">.</span><span class="nx">CurrentContext</span><span class="p">].</span><span class="nx">AuthInfo</span><span class="p">]</span>

<span class="c1">// gets the X509 certificate with current user credentials</span>
<span class="kd">var</span> <span class="nx">certs</span> <span class="p">[]</span><span class="o">*</span><span class="nx">x509</span><span class="p">.</span><span class="nx">Certificate</span>
<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nx">authInfo</span><span class="p">.</span><span class="nx">ClientCertificateData</span><span class="p">)</span> <span class="o">&amp;</span><span class="nx">gt</span><span class="p">;</span> <span class="mi">0</span> <span class="p">{</span>
    <span class="c1">// if the config file uses an embedded x509 certificate (e.g. kubelet.conf created by kubeadm), parse it</span>
    <span class="k">if</span> <span class="nx">certs</span><span class="p">,</span> <span class="nx">err</span> <span class="p">=</span> <span class="nx">certutil</span><span class="p">.</span><span class="nx">ParseCertsPEM</span><span class="p">(</span><span class="nx">authInfo</span><span class="p">.</span><span class="nx">ClientCertificateData</span><span class="p">);</span> <span class="nx">err</span> <span class="o">!=</span> <span class="kc">nil</span> <span class="p">{</span>
        <span class="k">return</span> <span class="s">&quot;&quot;</span><span class="p">,</span> <span class="nx">err</span>
    <span class="p">}</span>
<span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nx">authInfo</span><span class="p">.</span><span class="nx">ClientCertificate</span><span class="p">)</span> <span class="o">&amp;</span><span class="nx">gt</span><span class="p">;</span> <span class="mi">0</span> <span class="p">{</span>
    <span class="c1">// if the config file links an external x509 certificate (e.g. kubelet.conf created by TLS bootstrap), load it</span>
    <span class="k">if</span> <span class="nx">certs</span><span class="p">,</span> <span class="nx">err</span> <span class="p">=</span> <span class="nx">certutil</span><span class="p">.</span><span class="nx">CertsFromFile</span><span class="p">(</span><span class="nx">authInfo</span><span class="p">.</span><span class="nx">ClientCertificate</span><span class="p">);</span> <span class="nx">err</span> <span class="o">!=</span> <span class="kc">nil</span> <span class="p">{</span>
        <span class="k">return</span> <span class="s">&quot;&quot;</span><span class="p">,</span> <span class="nx">err</span>
    <span class="p">}</span>
<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="k">return</span> <span class="s">&quot;&quot;</span><span class="p">,</span> <span class="nx">errors</span><span class="p">.</span><span class="nx">New</span><span class="p">(</span><span class="s">&quot;invalid kubelet.conf. X509 certificate expected&quot;</span><span class="p">)</span>
<span class="p">}</span>

<span class="c1">// We are only putting one certificate in the certificate pem file, so it&#39;s safe to just pick the first one</span>
<span class="c1">// TODO: Support multiple certs here in order to be able to rotate certs</span>
<span class="nx">cert</span> <span class="o">:=</span> <span class="nx">certs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1">// gets the node name from the certificate common name</span>
<span class="k">return</span> <span class="nx">strings</span><span class="p">.</span><span class="nx">TrimPrefix</span><span class="p">(</span><span class="nx">cert</span><span class="p">.</span><span class="nx">Subject</span><span class="p">.</span><span class="nx">CommonName</span><span class="p">,</span> <span class="nx">constants</span><span class="p">.</span><span class="nx">NodesUserPrefix</span><span class="p">),</span> <span class="kc">nil</span>
<span class="p">&lt;</span><span class="o">/</span><span class="nx">code</span><span class="p">&gt;&lt;</span><span class="o">/</span><span class="nx">pre</span><span class="p">&gt;</span>

<span class="p">&lt;</span><span class="nx">p</span><span class="p">&gt;}</span></code></pre></div>
发现kubeadm先加载本机的<code>kubelet.conf</code>文件，然后尝试去找当前context中配置的用户的client-certificate-data数据，然后解析cert
的信息，找到subject的CommanName,来当作NodeName然后去kubeadm-config中找对应的apiEndpoint,所以尝试解析下当前的证书数据，看下CN的值：
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>

<span class="o">&lt;</span><span class="n">h1</span><span class="o">&gt;</span><span class="err">先获取</span><span class="n">client</span><span class="o">-</span><span class="n">certificate</span><span class="o">-</span><span class="n">data</span><span class="err">并做</span><span class="n">base64</span><span class="err">解密得到</span><span class="n">cert</span><span class="err">信息</span><span class="p">,</span><span class="err">$</span><span class="n">client</span><span class="o">-</span><span class="n">certificate</span><span class="o">-</span><span class="n">data</span><span class="err">为</span><span class="n">kubelet</span><span class="o">.</span><span class="n">conf</span><span class="err">中</span><span class="n">client</span><span class="o">-</span><span class="n">certificate</span><span class="o">-</span><span class="n">data</span><span class="err">对应的内容</span><span class="o">&lt;/</span><span class="n">h1</span><span class="o">&gt;</span>

<span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="p">[</span><span class="n">root</span><span class="nd">@cloud</span><span class="o">-</span><span class="n">cn</span><span class="o">-</span><span class="n">master</span><span class="o">-</span><span class="mi">1</span> <span class="o">~</span><span class="p">]</span><span class="c"># echo $client-certificate-data |base64 -d &gt; kubelet.crt</span>
<span class="p">[</span><span class="n">root</span><span class="nd">@cloud</span><span class="o">-</span><span class="n">cn</span><span class="o">-</span><span class="n">master</span><span class="o">-</span><span class="mi">1</span> <span class="o">~</span><span class="p">]</span><span class="c"># openssl x509 -in kubelet.crt -text |grep -i Subject</span>
        <span class="n">Subject</span><span class="p">:</span> <span class="n">O</span><span class="o">=</span><span class="n">system</span><span class="p">:</span><span class="n">nodes</span><span class="p">,</span> <span class="n">CN</span><span class="o">=</span><span class="n">system</span><span class="p">:</span><span class="n">node</span><span class="p">:</span><span class="n">cloud</span><span class="o">-</span><span class="n">cn</span><span class="o">-</span><span class="n">master</span><span class="o">-</span><span class="mi">2</span>
        <span class="n">Subject</span> <span class="n">Public</span> <span class="n">Key</span> <span class="n">Info</span><span class="p">:</span></code></pre></div>
👆，好吧原因定位到了，是因为获取到的<code>CommanName</code>与本机不匹配，于是在<code>cloud-cn-master-1</code>上获取到的是<code>cloud-cn-master-2</code>的<code>nodeName</code>，
然后去获取<code>annotation</code>自然也就拿不到了，然后即使手动<code>annotate</code>了，也是临时过了一步，到<code>configmap</code>中取<code>apiEndpoint</code>自然也获取不到，
哪怕再手动维护<code>apiEndpoint</code>，那么后续到校验hash也是无法通过，所以根本原因在于升级证书的时候没有<code>kubelet</code>证书被覆盖了，导致一系列的问题</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kubernetes的service cluster ip]]></title>
    <link href="http://liyongxin.github.io/blog/2019/04/03/kubernetesde-service-cluster-ip/"/>
    <updated>2019-04-03T17:17:23+08:00</updated>
    <id>http://liyongxin.github.io/blog/2019/04/03/kubernetesde-service-cluster-ip</id>
    <content type="html"><![CDATA[<p>k8s的pod可以有多个副本，但是在访问pod时，会有几个问题：</p>

<ul>
<li>客户端需要知道各个pod的地址</li>
<li>某一node上的pod如果故障，客户端需要感知</li>
</ul>


<p>为了解决这个问题，k8s引入了service的概念，用以指导客户端的流量。</p>

<h2>Service</h2>

<p>以下面的my-nginx为例。</p>

<p>pod和service的定义文件如下：
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@localhost k8s<span class="o">]</span><span class="c"># cat run-my-nginx.yaml</span>
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: my-nginx
spec:
  replicas: 2
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: nginx
        ports:
        - containerPort: 80
<span class="o">[</span>root@localhost k8s<span class="o">]</span><span class="c"># cat run-my-nginx-service.yaml</span>
apiVersion: v1
kind: Service
metadata:
  name: my-nginx
  labels:
    run: my-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
  selector:
    run: my-nginx</code></pre></div></p>

<p>pod my-nginx定义的replicas为2即2个副本，端口号为80;service my-nginx定义的selector为run:my-nginx，即该service选中所有label为run: my-nginx的pod；定义的port为80。</p>

<!--more-->


<p>使用kubectl create -f xx.yml创建后，可以在集群上看到2个pod，地址分别为10.244.1.10/10.244.2.10；可以看到1个service，IP/Port为10.11.97.177/80，其对接的Endpoints为10.244.1.10:80,10.244.2.10:80，即2个pod的服务地址，这三个URL在集群内任一节点都可以使用curl访问。
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@localhost k8s<span class="o">]</span><span class="c"># kubectl get pods -n default -o wide</span>
NAME                       READY     STATUS    RESTARTS   AGE       IP            NODE
my-nginx-379829228-3n755   1/1       Running   <span class="m">0</span>          21h       10.244.1.10   note2
my-nginx-379829228-xh214   1/1       Running   <span class="m">0</span>          21h       10.244.2.10   node1
<span class="o">[</span>root@localhost ~<span class="o">]</span><span class="c">#</span>
<span class="o">[</span>root@localhost ~<span class="o">]</span><span class="c"># kubectl describe svc my-nginx</span>
Name:                   my-nginx
Namespace:              default
Labels:                 <span class="nv">run</span><span class="o">=</span>my-nginx
Selector:               <span class="nv">run</span><span class="o">=</span>my-nginx
Type:                   ClusterIP
IP:                     10.11.97.177
Port:                   &lt;<span class="nb">unset</span>&gt; 80/TCP
Endpoints:              10.244.1.10:80,10.244.2.10:80
Session Affinity:       None</code></pre></div></p>

<p>但是，如果你去查看集群各节点的IP信息，是找不到10.11.97.177这个IP的，那么curl是如何通过这个(Virtual)IP地址访问到后端的Endpoints呢？
答案在<a href="https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies">这里</a>。</p>

<h2>kube-proxy</h2>

<p>k8s支持2种proxy模式，userspace和iptables。从v1.2版本开始，默认采用iptables proxy。那么这两种模式有什么不同吗？</p>

<p>1、userspace</p>

<p>顾名思义，userspace即用户空间。为什么这么叫呢？看下面的图。</p>

<p><img src="/images/custom/services-userspace-overview.svg" alt="" /></p>

<p>kube-proxy会为每个service随机监听一个端口(proxy port)，并增加一条iptables规则：所以到clusterIP:Port 的报文都redirect到proxy port；kube-proxy从它监听的proxy port收到报文后，走round robin（默认）或者session affinity（会话亲和力，即同一client IP都走同一链路给同一pod服务），分发给对应的pod。</p>

<p>显然userspace会造成所有报文都走一遍用户态，性能不高，现在k8s已经不再使用了。</p>

<p>2、iptables</p>

<p>我们回过头来看看userspace，既然用户态会增加性能损耗，那么有没有办法不走呢？实际上用户态也只是一个报文LB，通过iptables完全可以搞定。k8s下面这张图很清晰的说明了iptables方式与userspace方式的不同：kube-proxy只是作为controller，而不是server，真正服务的是内核的netfilter，体现在用户态则是iptables。</p>

<p>kube-proxy的iptables方式也支持round robin(默认)和session affinity。</p>

<p><img src="/images/custom/services-iptables-overview.svg" alt="" /></p>

<p>那么iptables是怎么做到LB，而且还能round-robin呢？我们通过iptables-save来看my-nginx这个服务在某一个node上的iptables规则。
<div class="highlight"><pre><code class="language-bash" data-lang="bash">-A KUBE-SERVICES -d 10.11.97.177/32 -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx: cluster IP<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp <span class="p">&amp;</span>ndash<span class="p">;</span>dport <span class="m">80</span> -j KUBE-SVC-BEPXDJBUHFCSYIC3&lt;/p&gt;

&lt;p&gt;-A KUBE-SVC-BEPXDJBUHFCSYIC3 -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx:<span class="p">&amp;</span>rdquo<span class="p">;</span> -m statistic <span class="p">&amp;</span>ndash<span class="p">;</span>mode random <span class="p">&amp;</span>ndash<span class="p">;</span>probability 0.50000000000 -j KUBE-SEP-U4UWLP4OR3LOJBXU
-A KUBE-SVC-BEPXDJBUHFCSYIC3 -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx:<span class="p">&amp;</span>rdquo<span class="p">;</span> -j KUBE-SEP-QHRWSLKOO5YUPI7O&lt;/p&gt;

&lt;p&gt;-A KUBE-SEP-U4UWLP4OR3LOJBXU -s 10.244.1.10/32 -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx:<span class="p">&amp;</span>rdquo<span class="p">;</span> -j KUBE-MARK-MASQ
-A KUBE-SEP-U4UWLP4OR3LOJBXU -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx:<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp -j DNAT <span class="p">&amp;</span>ndash<span class="p">;</span>to-destination 10.244.1.10:80&lt;/p&gt;

&lt;p&gt;-A KUBE-SEP-QHRWSLKOO5YUPI7O -s 10.244.2.10/32 -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx:<span class="p">&amp;</span>rdquo<span class="p">;</span> -j KUBE-MARK-MASQ
-A KUBE-SEP-QHRWSLKOO5YUPI7O -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx:<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp -j DNAT <span class="p">&amp;</span>ndash<span class="p">;</span>to-destination 10.244.2.10:80</code></pre></div></p>

<p>第1条规则，终于看到这个virtual IP了。node上不需要有这个ip地址，iptables在看到目的地址为virutal ip的符合规则tcp报文，会走KUBE-SVC-BEPXDJBUHFCSYIC3规则。</p>

<p>第2/3条规则，KUBE-SVC-BEPXDJBUHFCSYIC3链实现了将报文按50%的统计概率随机匹配到2条规则(round-robin)。</p>

<p>第4/5和5/6为成对的2组规则，将报文转给了真正的服务pod。</p>

<p>至此，从物理node收到目的地址为10.11.97.177、端口号为80的报文开始，到pod my-nginx收到报文并响应，描述了一个完整的链路。可以看到，整个报文链路上没有经过任何用户态进程，效率和稳定性都比较高。</p>

<h2>NodePort</h2>

<p>上面的例子里，由于10.11.97.177其实还是在集群内有效地址，由于实际上并不存在这个地址，当从集群外访问时会访问失败，这时需要将service暴漏出去。k8s给出的一个方案是NodePort，客户端根据NodePort+集群内任一物理节点的IP，就可以访问k8s的service了。这又是怎么做到的呢？</p>

<p>答案还是iptables。我们来看下面这个sock-shop的例子，其创建方法见k8s.io，不再赘述。
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@localhost k8s<span class="o">]</span><span class="c"># kubectl describe svc front-end -n sock-shop</span>
Name:                   front-end
Namespace:              sock-shop
Labels:                 <span class="nv">name</span><span class="o">=</span>front-end
Selector:               <span class="nv">name</span><span class="o">=</span>front-end
Type:                   NodePort
IP:                     10.15.9.0
Port:                   &lt;<span class="nb">unset</span>&gt; 80/TCP
NodePort:               &lt;<span class="nb">unset</span>&gt; 30001/TCP
Endpoints:              10.244.2.5:8079
Session Affinity:       None</code></pre></div></p>

<p>在任一node上查看iptables-save：
<div class="highlight"><pre><code class="language-bash" data-lang="bash">-A KUBE-NODEPORTS -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>sock-shop/front-end:<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp <span class="p">&amp;</span>ndash<span class="p">;</span>dport <span class="m">30001</span> -j KUBE-MARK-MASQ
-A KUBE-NODEPORTS -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>sock-shop/front-end:<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp <span class="p">&amp;</span>ndash<span class="p">;</span>dport <span class="m">30001</span> -j KUBE-SVC-LFMD53S3EZEAOUSJ&lt;/p&gt;

&lt;p&gt;-A KUBE-SERVICES -d 10.15.9.0/32 -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>sock-shop/front-end: cluster IP<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp <span class="p">&amp;</span>ndash<span class="p">;</span>dport <span class="m">80</span> -j KUBE-SVC-LFMD53S3EZEAOUSJ&lt;/p&gt;

&lt;p&gt;-A KUBE-SVC-LFMD53S3EZEAOUSJ -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>sock-shop/front-end:<span class="p">&amp;</span>rdquo<span class="p">;</span> -j KUBE-SEP-SM6TGF2R62ADFGQA&lt;/p&gt;

&lt;p&gt;-A KUBE-SEP-SM6TGF2R62ADFGQA -s 10.244.2.5/32 -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>sock-shop/front-end:<span class="p">&amp;</span>rdquo<span class="p">;</span> -j KUBE-MARK-MASQ
-A KUBE-SEP-SM6TGF2R62ADFGQA -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>sock-shop/front-end:<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp -j DNAT <span class="p">&amp;</span>ndash<span class="p">;</span>to-destination 10.244.2.5:8079</code></pre></div>
聪明如你，一定已经看明白了吧。</p>

<p>要是还不明白，看看这篇文章：源地址审计：<a href="https://ieevee.com/tech/2017/09/18/k8s-svc-src.html">追踪 kubernetees 服务的SNAT</a> 。</p>

<p>不过kube-proxy的iptables有个缺陷，即当pod故障时无法自动重试，需要依赖readiness probes，主要思想就是创建一个探测容器，当检测到后端pod挂了的时候，更新iptables。</p>

<p>在用NodePort的时候，经常会有人问一个问题，NodePort指定的端口(30000+)，而client建立tcp连接时，本地端口是操作系统随机选定的(30000+)，如何防止产生冲突呢？</p>

<p>解决办法是kube-proxy进程会去起一个tcp listen socket，监听端口号就是NodePort。可以把这个socket理解为“占位符”，目的是为了让操作系统跳开该端口。</p>

<p>转载自<a href="https://ieevee.com/tech/2017/01/20/k8s-service.html#kube-proxy">谈谈kubernets的service组件的Virtual IP</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[验证kubernetes的pod安全策略]]></title>
    <link href="http://liyongxin.github.io/blog/2019/03/18/yan-zheng-kubernetesde-podan-quan-ce-lue/"/>
    <updated>2019-03-18T14:19:02+08:00</updated>
    <id>http://liyongxin.github.io/blog/2019/03/18/yan-zheng-kubernetesde-podan-quan-ce-lue</id>
    <content type="html"><![CDATA[

<p>最近有客户提出需要对业务集群的pod做安全限制，不允许使用pod拥有privileged的权限，研究一番，刚好k8s的<a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">Pod Security Policies</a>可以实现该需求。</p>

<h2>什么是pod安全策略</h2>

<p>Pod Security Policy(简称psp)是集群级别的资源，该资源控制pod的spec中安全相关的方面，具体的方面参考下表：</p>

<table>
<thead>
<tr>
<th style="text-align:center;">       Control Aspect                  </th>
<th style="text-align:center;">       Field Names     </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;"> Running of privileged containers      </td>
<td style="text-align:center;"> <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#privileged">privileged</a>   </td>
</tr>
<tr>
<td style="text-align:center;"> Usage of host namespaces              </td>
<td style="text-align:center;">   <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces">hostPID, hostIPC</a>  </td>
</tr>
<tr>
<td style="text-align:center;"> Usage of host networking and ports    </td>
<td style="text-align:center;">    <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces">hostNetwork, hostPorts</a>  </td>
</tr>
<tr>
<td style="text-align:center;"> Usage of volume types                 </td>
<td style="text-align:center;"> <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems">volumes</a>   </td>
</tr>
<tr>
<td style="text-align:center;"> Usage of the host filesystem          </td>
<td style="text-align:center;">  <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems">allowedHostPaths</a>  </td>
</tr>
<tr>
<td style="text-align:center;"> White list of Flexvolume drivers      </td>
<td style="text-align:center;">   <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#flexvolume-drivers">allowedFlexVolumes</a>  </td>
</tr>
<tr>
<td style="text-align:center;"> Allocating an FSGroup that owns the pod’s volumes           </td>
<td style="text-align:center;"> <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems">fsGroup</a>   </td>
</tr>
<tr>
<td style="text-align:center;"> Requiring the use of a read only root file system             </td>
<td style="text-align:center;">   <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems">readOnlyRootFilesystem</a>  </td>
</tr>
<tr>
<td style="text-align:center;"> The user and group IDs of the container       </td>
<td style="text-align:center;">    <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#users-and-groups">runAsUser, runAsGroup, supplementalGroups</a>  </td>
</tr>
<tr>
<td style="text-align:center;"> The SELinux context of the container                      </td>
<td style="text-align:center;"> <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#selinux">seLinux</a>   </td>
</tr>
</tbody>
</table>


<!--more-->


<h2>如何开启Pod Security Policy</h2>

<ul>
<li><strong>Enable API extensions</strong></li>
</ul>


<p>For Kubernetes &lt; 1.6.0, the API Server must enable the extensions/v1beta1/podsecuritypolicy API extensions group (&ndash;runtime-config=extensions/v1beta1/podsecuritypolicy=true).</p>

<ul>
<li><strong>Enable PodSecurityPolicy admission control policy</strong></li>
</ul>


<p>The following parameter needs to be added to the API server startup argument: –admission-control=PodSecurityPolicy</p>

<p>默认psp是不开启的，若要开启，需要配置上述apiserver启动参数，默认的apiserver的yaml文件为<code>/etc/kubernetes/manifests/kube-apiserver.yaml</code>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="p">&amp;</span>hellip<span class="p">;</span>
spec:
  containers:
  - <span class="nb">command</span>:
    - kube-apiserver
    - <span class="p">&amp;</span>ndash<span class="p">;</span>authorization-mode<span class="o">=</span>Node,RBAC
    - <span class="p">&amp;</span>ndash<span class="p">;</span><span class="nb">enable</span>-admission-plugins<span class="o">=</span>Initializers,NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,ResourceQuota,PodSecurityPolicy
    - <span class="p">&amp;</span>ndash<span class="p">;</span>runtime-config<span class="o">=</span>extensions/v1beta1/podsecuritypolicy<span class="o">=</span><span class="nb">true</span>
    <span class="p">&amp;</span>hellip<span class="p">;</span></code></pre></div></p>

<h2>配置默认的psp</h2>

<p>由于已有打开了pod创建的安全策略,但此时还未创建任何的policy，所以任何pod都无法被创建，此时如果尝试去创建一个pod，会发现pod无法进行调度，
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl run &amp;ndash;image=nginx yxli</span>
deployment.apps/yxli created
<span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl get deploy yxli</span>
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
yxli      <span class="m">1</span>         <span class="m">0</span>         <span class="m">0</span>            <span class="m">0</span>           4m</code></pre></div>
查看controller的日志会发现报错forbidden: no providers available to validate pod request
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master ~<span class="o">]</span><span class="c"># kubectl logs &amp;ndash;tail=10 -f kube-controller-manager-build-master -n kube-system</span>
E0318 03:01:17.321184       <span class="m">1</span> replica_set.go:450<span class="o">]</span> Sync <span class="p">&amp;</span>ldquo<span class="p">;</span>default/yxli-6978dddc9d<span class="p">&amp;</span>rdquo<span class="p">;</span> failed with pods <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d-<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request
I0318 03:01:17.321302       <span class="m">1</span> event.go:221<span class="o">]</span> Event<span class="o">(</span>v1.ObjectReference<span class="o">{</span>Kind:<span class="p">&amp;</span>ldquo<span class="p">;</span>ReplicaSet<span class="p">&amp;</span>rdquo<span class="p">;</span>, Namespace:<span class="p">&amp;</span>ldquo<span class="p">;</span>default<span class="p">&amp;</span>rdquo<span class="p">;</span>, Name:<span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d<span class="p">&amp;</span>rdquo<span class="p">;</span>, UID:<span class="p">&amp;</span>ldquo<span class="p">;</span>0d11050f-492a-11e9-8ef0-00163e004fd2<span class="p">&amp;</span>rdquo<span class="p">;</span>, APIVersion:<span class="p">&amp;</span>ldquo<span class="p">;</span>apps/v1<span class="p">&amp;</span>rdquo<span class="p">;</span>, ResourceVersion:<span class="p">&amp;</span>ldquo<span class="p">;</span>764927<span class="p">&amp;</span>rdquo<span class="p">;</span>, FieldPath:<span class="p">&amp;</span>ldquo<span class="p">;&amp;</span>rdquo<span class="p">;</span><span class="o">})</span>: <span class="nb">type</span>: <span class="p">&amp;</span>lsquo<span class="p">;</span>Warning<span class="p">&amp;</span>rsquo<span class="p">;</span> reason: <span class="p">&amp;</span>lsquo<span class="p">;</span>FailedCreate<span class="p">&amp;</span>rsquo<span class="p">;</span> Error creating: pods <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d-<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request
E0318 03:01:37.807060       <span class="m">1</span> replica_set.go:450<span class="o">]</span> Sync <span class="p">&amp;</span>ldquo<span class="p">;</span>default/yxli-6978dddc9d<span class="p">&amp;</span>rdquo<span class="p">;</span> failed with pods <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d-<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request
I0318 03:01:37.807046       <span class="m">1</span> event.go:221<span class="o">]</span> Event<span class="o">(</span>v1.ObjectReference<span class="o">{</span>Kind:<span class="p">&amp;</span>ldquo<span class="p">;</span>ReplicaSet<span class="p">&amp;</span>rdquo<span class="p">;</span>, Namespace:<span class="p">&amp;</span>ldquo<span class="p">;</span>default<span class="p">&amp;</span>rdquo<span class="p">;</span>, Name:<span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d<span class="p">&amp;</span>rdquo<span class="p">;</span>, UID:<span class="p">&amp;</span>ldquo<span class="p">;</span>0d11050f-492a-11e9-8ef0-00163e004fd2<span class="p">&amp;</span>rdquo<span class="p">;</span>, APIVersion:<span class="p">&amp;</span>ldquo<span class="p">;</span>apps/v1<span class="p">&amp;</span>rdquo<span class="p">;</span>, ResourceVersion:<span class="p">&amp;</span>ldquo<span class="p">;</span>764927<span class="p">&amp;</span>rdquo<span class="p">;</span>, FieldPath:<span class="p">&amp;</span>ldquo<span class="p">;&amp;</span>rdquo<span class="p">;</span><span class="o">})</span>: <span class="nb">type</span>: <span class="p">&amp;</span>lsquo<span class="p">;</span>Warning<span class="p">&amp;</span>rsquo<span class="p">;</span> reason: <span class="p">&amp;</span>lsquo<span class="p">;</span>FailedCreate<span class="p">&amp;</span>rsquo<span class="p">;</span> Error creating: pods <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d-<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request
E0318 03:02:18.773092       <span class="m">1</span> replica_set.go:450<span class="o">]</span> Sync <span class="p">&amp;</span>ldquo<span class="p">;</span>default/yxli-6978dddc9d<span class="p">&amp;</span>rdquo<span class="p">;</span> failed with pods <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d-<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request</code></pre></div></p>

<p>而由于我们修改了apiserver的参数，pod受kubelet管理，自动触发了重建，此时apiserver的pod也是因为缺少权限没法创建出来
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># journalctl -fu kubelet</span>
<span class="p">&amp;</span>ndash<span class="p">;</span> Logs begin at Tue 2019-03-12 20:33:59 CST. <span class="p">&amp;</span>ndash<span class="p">;</span>
Mar <span class="m">18</span> 11:25:29 build-master kubelet<span class="o">[</span>4013<span class="o">]</span>: E0318 11:25:29.016416    <span class="m">4013</span> kubelet.go:1594<span class="o">]</span> Failed creating a mirror pod <span class="k">for</span> <span class="p">&amp;</span>ldquo<span class="p">;</span>kube-apiserver-build-master_kube-system<span class="o">(</span>77a7d13c654e7233306d4e2948aaaa78<span class="o">)</span><span class="p">&amp;</span>rdquo<span class="p">;</span>: pods <span class="p">&amp;</span>ldquo<span class="p">;</span>kube-apiserver-build-master<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request
Mar <span class="m">18</span> 11:25:30 build-master kubelet<span class="o">[</span>4013<span class="o">]</span>: W0318 11:25:30.113587    <span class="m">4013</span> kubelet.go:1579<span class="o">]</span> Deleting mirror pod <span class="p">&amp;</span>ldquo<span class="p">;</span>kube-apiserver-build-master_kube-system<span class="o">(</span>e48d84cd-46f0-11e9-8ef0-00163e004fd2<span class="o">)</span><span class="p">&amp;</span>rdquo<span class="p">;</span> because it is outdated
Mar <span class="m">18</span> 11:25:30 build-master kubelet<span class="o">[</span>4013<span class="o">]</span>: E0318 11:25:30.117242    <span class="m">4013</span> kubelet.go:1594<span class="o">]</span> Failed creating a mirror pod <span class="k">for</span> <span class="p">&amp;</span>ldquo<span class="p">;</span>kube-apiserver-build-master_kube-system<span class="o">(</span>77a7d13c654e7233306d4e2948aaaa78<span class="o">)</span><span class="p">&amp;</span>rdquo<span class="p">;</span>: pods <span class="p">&amp;</span>ldquo<span class="p">;</span>kube-apiserver-build-master<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request</code></pre></div></p>

<p>所以我们需要创建一个policy，为我们需要的受kubelet管理的静态pod以及kube-system命名空间下的pod提供权限，否则pod一旦发生重建，都将因为缺少权限导致无法正常创建出来。</p>

<p>首先新建文件 privileged.policy.yaml，权限不受限制，
<div class="highlight"><pre><code class="language-bash" data-lang="bash">apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: privileged
spec:
  allowedCapabilities:
  - <span class="p">&amp;</span>lsquo<span class="p">;</span>&lt;em&gt;<span class="p">&amp;</span>rsquo<span class="p">;</span>
  allowPrivilegeEscalation: <span class="nb">true</span>
<span class="nb">  </span>fsGroup:
    rule: <span class="p">&amp;</span>lsquo<span class="p">;</span>RunAsAny<span class="p">&amp;</span>rsquo<span class="p">;</span>
  hostIPC: <span class="nb">true</span>
<span class="nb">  </span>hostNetwork: <span class="nb">true</span>
<span class="nb">  </span>hostPID: <span class="nb">true</span>
<span class="nb">  </span>hostPorts:
  - min: 0
    max: 65535
  privileged: <span class="nb">true</span>
<span class="nb">  </span>readOnlyRootFilesystem: <span class="nb">false</span>
<span class="nb">  </span>runAsUser:
    rule: <span class="p">&amp;</span>lsquo<span class="p">;</span>RunAsAny<span class="p">&amp;</span>rsquo<span class="p">;</span>
  seLinux:
    rule: <span class="p">&amp;</span>lsquo<span class="p">;</span>RunAsAny<span class="p">&amp;</span>rsquo<span class="p">;</span>
  supplementalGroups:
    rule: <span class="p">&amp;</span>lsquo<span class="p">;</span>RunAsAny<span class="p">&amp;</span>rsquo<span class="p">;</span>
  volumes:
  - <span class="p">&amp;</span>lsquo<span class="p">;</span>&lt;/em&gt;<span class="p">&amp;</span>rsquo<span class="p">;</span></code></pre></div></p>

<p>创建并查看该policy
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl create -f privileged.policy.yaml</span>
podsecuritypolicy.policy/privileged created
<span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl get psp</span>
NAME         PRIV      CAPS      SELINUX    RUNASUSER   FSGROUP    SUPGROUP   READONLYROOTFS   VOLUMES
privileged   <span class="nb">true</span>      *         RunAsAny   RunAsAny    RunAsAny   RunAsAny   <span class="nb">false</span>            *</code></pre></div></p>

<p>然后还需要创建一个clusterrole，并且赋予该role对上述psp对使用权
<div class="highlight"><pre><code class="language-bash" data-lang="bash">&lt;/p&gt;

&lt;h1&gt;Cluster role which grants access to the privileged pod security policy&lt;/h1&gt;

&lt;p&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: privileged-psp
rules:
- apiGroups:
  - policy
  resourceNames:
  - privileged
  resources:
  - podsecuritypolicies
  verbs:
  - use</code></pre></div>
然后把clusterrole赋予serviceaccount或者对应的user、group，针对kube-system命名空间下的pod来说，都使用了kube-system下的serviceaccount或者由kubelet管理，kubelet是使用system:nodes这个组来管理的pod，所以只需要做如下binding即可为kube-system下的所有pod提供权限，
<div class="highlight"><pre><code class="language-bash" data-lang="bash">apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kube-system-psp
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: privileged-psp
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:nodes
  namespace: kube-system
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:serviceaccounts:kube-system</code></pre></div></p>

<p>都创建好之后，查看kube-system下的pod，发现apiserver已经创建成功
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl get po -n kube-system</span>
NAME                                   READY     STATUS    RESTARTS   AGE
coredns-68f5b48ccb-9hjvr               1/1       Running   <span class="m">0</span>          5d
coredns-68f5b48ccb-qrjnr               1/1       Running   <span class="m">0</span>          5d
etcd-build-master                      1/1       Running   <span class="m">0</span>          5d
kube-apiserver-build-master            1/1       Running   <span class="m">0</span>          18s</code></pre></div></p>

<blockquote><p>上面的binding只是为kube-system空间赋予了权限，若想要为别的命名空间赋予权限，可以使用ClusterRoleBinding的方式为多个namespace绑定privileged-psp的clusterrole，或者像如下方式，为所有合法的serviceaccounts和user绑定权限，当然前提是pod使用了namespace下的serviceAccount
<div class="highlight"><pre><code class="language-bash" data-lang="bash">&lt;/p&gt;

&lt;h1&gt;Authorize all service accounts in a namespace:&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;kind: Group
apiGroup: rbac.authorization.k8s.io
name: system:serviceaccounts

&lt;h1&gt;Or equivalently, all authenticated users in a namespace:&lt;/h1&gt;&lt;/li&gt;
&lt;li&gt;kind: Group
apiGroup: rbac.authorization.k8s.io
name: system:authenticated</code></pre></div></li>
</ul>
</blockquote>

<h2>验证psp的privileged限制</h2>

<p>该章节会创建一个名为psp-namespace的namespace做测试，来验证psp中对privileged的pod的限制
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl create namespace psp-example</span>
namespace/psp-example created
<span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl create serviceaccount fake-user -n psp-example</span>
serviceaccount/fake-user created
<span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl run &amp;ndash;image=nginx psp-pod-nginx-1 -n psp-example &amp;ndash;serviceaccount=fake-user</span>
deployment.apps/psp-pod-nginx-1 created
<span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl get po -n psp-example</span>
No resources found.
<span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl get deploy -n psp-example</span>
NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
psp-pod-nginx-1   <span class="m">1</span>         <span class="m">0</span>         <span class="m">0</span>            <span class="m">0</span>           23s</code></pre></div></p>

<p>如上所示，新的psp-example命名空间由于没有任何权限，所以无法创建新pod，接下来我们创建一个policy并给psp-example做authorize，但是policy中会限制无法创建privileged的pod
<div class="highlight"><pre><code class="language-bash" data-lang="bash">apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: example
spec:
  privileged: <span class="nb">false</span>  <span class="c"># Don&amp;rsquo;t allow privileged pods!</span>
  <span class="c"># The rest fills in some required fields.</span>
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  runAsUser:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  volumes:
  - <span class="p">&amp;</span>lsquo<span class="p">;</span>*<span class="p">&amp;</span>rsquo<span class="p">;</span></code></pre></div></p>

<p> 然后创建role和rolebinding，并验证fake-user是否有权限使用example的policy
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master psp-example<span class="o">]</span><span class="c"># kubectl -n psp-example create role psp:unprivileged &amp;ndash;verb=use &amp;ndash;resource=podsecuritypolicy &amp;ndash;resource-name=example</span>
role.rbac.authorization.k8s.io/psp:unprivileged created
<span class="o">[</span>root@build-master psp-example<span class="o">]</span><span class="c"># kubectl -n psp-example create rolebinding fake-user:psp:unprivileged &amp;ndash;role=psp:unprivileged &amp;ndash;serviceaccount=psp-example:fake-user</span>
rolebinding.rbac.authorization.k8s.io/fake-user:psp:unprivileged created
<span class="o">[</span>root@build-master psp-example<span class="o">]</span><span class="c">#  kubectl &amp;ndash;as=system:serviceaccount:psp-example:fake-user -n psp-example auth can-i use podsecuritypolicy/example</span>
yes</code></pre></div></p>

<p>等待片刻，再次查看刚才创建的deploy，发现已经调度成功
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master psp-example<span class="o">]</span><span class="c"># kubectl get deploy -n psp-example psp-pod-nginx-1</span>
NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
psp-pod-nginx-1   <span class="m">1</span>         <span class="m">1</span>         <span class="m">1</span>            <span class="m">1</span>           22m</code></pre></div></p>

<p>此时我们尝试在psp-example下创建一个privileged的特权容器，
<div class="highlight"><pre><code class="language-bash" data-lang="bash">apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    run: psp-pod-nginx-privileged
  name: psp-pod-nginx-privileged
  namespace: psp-example
spec:
  selector:
    matchLabels:
      run: psp-pod-nginx-privileged
  template:
    metadata:
      labels:
        run: psp-pod-nginx-privileged
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: psp-pod-nginx-privileged
      securityContext:
        privileged: <span class="nb">true</span>
<span class="nb">      </span>serviceAccount: fake-user</code></pre></div>
创建并查看结果
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master psp-example<span class="o">]</span><span class="c"># kubectl create -f privileged-pod.yaml</span>
deployment.extensions/psp-pod-nginx-privileged created
<span class="o">[</span>root@build-master psp-example<span class="o">]</span><span class="c"># kubectl get deploy -n psp-example psp-pod-nginx-privileged</span>
psp-pod-nginx-privileged   <span class="m">1</span>         <span class="m">0</span>         <span class="m">0</span>            <span class="m">0</span>           24s
<span class="o">[</span>root@build-master ~<span class="o">]</span><span class="c"># kubectl logs &amp;ndash;tail=1 -f kube-controller-manager-build-master -n kube-system</span>
I0318 04:32:42.777200       <span class="m">1</span> event.go:221<span class="o">]</span> Event<span class="o">(</span>v1.ObjectReference<span class="o">{</span>Kind:<span class="p">&amp;</span>ldquo<span class="p">;</span>ReplicaSet<span class="p">&amp;</span>rdquo<span class="p">;</span>, Namespace:<span class="p">&amp;</span>ldquo<span class="p">;</span>psp-example<span class="p">&amp;</span>rdquo<span class="p">;</span>, Name:<span class="p">&amp;</span>ldquo<span class="p">;</span>psp-pod-nginx-privileged-d7f8dbd75<span class="p">&amp;</span>rdquo<span class="p">;</span>, UID:<span class="p">&amp;</span>ldquo<span class="p">;</span>c66fb10f-4936-11e9-9c58-00163e004fd2<span class="p">&amp;</span>rdquo<span class="p">;</span>, APIVersion:<span class="p">&amp;</span>ldquo<span class="p">;</span>apps/v1<span class="p">&amp;</span>rdquo<span class="p">;</span>, ResourceVersion:<span class="p">&amp;</span>ldquo<span class="p">;</span>773292<span class="p">&amp;</span>rdquo<span class="p">;</span>, FieldPath:<span class="p">&amp;</span>ldquo<span class="p">;&amp;</span>rdquo<span class="p">;</span><span class="o">})</span>: <span class="nb">type</span>: <span class="p">&amp;</span>lsquo<span class="p">;</span>Warning<span class="p">&amp;</span>rsquo<span class="p">;</span> reason: <span class="p">&amp;</span>lsquo<span class="p">;</span>FailedCreate<span class="p">&amp;</span>rsquo<span class="p">;</span> Error creating: pods <span class="p">&amp;</span>ldquo<span class="p">;</span>psp-pod-nginx-privileged-d7f8dbd75-<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: unable to validate against any pod security policy: <span class="o">[</span>spec.containers<span class="o">[</span>0<span class="o">]</span>.securityContext.privileged: Invalid value: <span class="nb">true</span>: Privileged containers are not allowed<span class="o">]</span></code></pre></div>
查看controller-manager的日志，发现Privileged containers are not allowed</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[somethings about kubernetes]]></title>
    <link href="http://liyongxin.github.io/blog/2019/02/28/k8sxiao-zhi-shi/"/>
    <updated>2019-02-28T11:12:56+08:00</updated>
    <id>http://liyongxin.github.io/blog/2019/02/28/k8sxiao-zhi-shi</id>
    <content type="html"><![CDATA[<ul>
<li><p>k8s默认驱逐设置
<div class="highlight"><pre><code class="language-bash" data-lang="bash">// DefaultEvictionHard includes default options <span class="k">for</span> hard eviction.
var <span class="nv">DefaultEvictionHard</span> <span class="o">=</span> map<span class="o">[</span>string<span class="o">]</span>string<span class="o">{</span>
      <span class="p">&amp;</span>ldquo<span class="p">;</span>memory.available<span class="p">&amp;</span>rdquo<span class="p">;</span>:  <span class="p">&amp;</span>ldquo<span class="p">;</span>100Mi<span class="p">&amp;</span>rdquo<span class="p">;</span>,
      <span class="p">&amp;</span>ldquo<span class="p">;</span>nodefs.available<span class="p">&amp;</span>rdquo<span class="p">;</span>:  <span class="p">&amp;</span>ldquo<span class="p">;</span>10%<span class="p">&amp;</span>rdquo<span class="p">;</span>,
      <span class="p">&amp;</span>ldquo<span class="p">;</span>nodefs.inodesFree<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>5%<span class="p">&amp;</span>rdquo<span class="p">;</span>,
      <span class="p">&amp;</span>ldquo<span class="p">;</span>imagefs.available<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>15%<span class="p">&amp;</span>rdquo<span class="p">;</span>,
<span class="o">}</span></code></pre></div></p></li>
<li><p>kubernetes 滚动升级
<div class="highlight"><pre><code class="language-bash" data-lang="bash">&lt;/p&gt;

&lt;h1&gt;run <span class="nb">test </span>deploy&lt;/h1&gt;

<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl run &amp;ndash;image=nginx &amp;ndash;port=80 &amp;ndash;replicas=2 yxli-nginx</span>

&lt;h1&gt;scale replica&lt;/h1&gt;

<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl scale &amp;ndash;replicas=1 deploy/yxli-nginx</span>
<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl get deploy</span>
NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
busybox      <span class="m">2</span>         <span class="m">2</span>         <span class="m">2</span>            <span class="m">2</span>           39d
busybox1     <span class="m">1</span>         <span class="m">1</span>         <span class="m">1</span>            <span class="m">1</span>           39d
yxli-nginx   <span class="m">1</span>         <span class="m">1</span>         <span class="m">1</span>            <span class="m">1</span>           2h

&lt;h1&gt;update image&lt;/h1&gt;

<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl set image  deploy/yxli-nginx yxli-nginx=nginx:alpine</span>

&lt;h1&gt;查看升级历史&lt;/h1&gt;

<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl rollout history deploy/yxli-nginx</span>
deployments <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-nginx<span class="p">&amp;</span>rdquo<span class="p">;</span>
REVISION  CHANGE-CAUSE
<span class="m">1</span>         &lt;none&gt;
<span class="m">2</span>         &lt;none&gt;

&lt;h1&gt;回顾至上次版本&lt;/h1&gt;

<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl rollout undo deploy/yxli-nginx</span>
<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl rollout history deploy/yxli-nginx</span>
deployments <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-nginx<span class="p">&amp;</span>rdquo<span class="p">;</span>
REVISION  CHANGE-CAUSE
<span class="m">2</span>         &lt;none&gt;
<span class="m">3</span>         &lt;none&gt;

&lt;h1&gt;回滚至指定版本&lt;/h1&gt;

&lt;p&gt;<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl rolloutundo deployment/lykops-dpm &amp;ndash;to-revision=2</span></code></pre></div></p></li>
</ul>


<!--more-->


<ul>
<li>查看docker使用的cpu核心数
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@yxli-onebox docker<span class="o">]</span><span class="c"># pwd</span>
/sys/fs/cgroup/cpuset/docker
<span class="o">[</span>root@yxli-onebox docker<span class="o">]</span><span class="c"># cat cpuset.cpus</span>
0-7</code></pre></div></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kubelet的认证]]></title>
    <link href="http://liyongxin.github.io/blog/2018/08/25/kubeletde-ren-zheng/"/>
    <updated>2018-08-25T18:17:17+08:00</updated>
    <id>http://liyongxin.github.io/blog/2018/08/25/kubeletde-ren-zheng</id>
    <content type="html"><![CDATA[<p>研究完<a href="../kuberneteszhong-de-ren-zheng-xiang-guan">kubectl的认证与授权</a>，使用相同的方式去找kubelet的访问，
首先定位配置文件<code>/etc/kubernetes/kubelet.conf</code>，然后用相同的方式对<code>client-key-data</code>做base64解码，保存为kubelet.crt文件。</p>

<p>openssl查看crt证书，
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@k8s-master kubernetes<span class="o">]</span><span class="c"># openssl x509 -text -in kubelet.crt -noout</span>
Certificate:
    Data:
        Version: <span class="m">3</span> <span class="o">(</span>0x2<span class="o">)</span>
        Serial Number: <span class="m">8126553944389053218</span> <span class="o">(</span>0x70c751c18f5beb22<span class="o">)</span>
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: <span class="nv">CN</span><span class="o">=</span>kubernetes
        Validity
            Not Before: Aug <span class="m">20</span> 05:50:39 <span class="m">2018</span> GMT
            Not After : Aug <span class="m">20</span> 05:50:42 <span class="m">2019</span> GMT
        Subject: <span class="nv">O</span><span class="o">=</span>system:nodes, <span class="nv">CN</span><span class="o">=</span>system:node:k8s-master
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: <span class="o">(</span><span class="m">2048</span> bit<span class="o">)</span>
                Modulus:
                    00:9f:92:83:49:aa:cc:52:0e:de:bd:af:a6:fd:ef:
    <span class="p">&amp;</span>hellip<span class="p">;</span> <span class="p">&amp;</span>hellip<span class="p">;</span></code></pre></div></p>

<p>得到我们期望的内容：
<div class="highlight"><pre><code class="language-bash" data-lang="bash">Subject: <span class="nv">O</span><span class="o">=</span>system:nodes, <span class="nv">CN</span><span class="o">=</span>system:node:k8s-master</code></pre></div></p>

<blockquote><p>关于Subject，在k8s中可以理解为角色绑定主体，RoleBinding或者ClusterRoleBinding可以将角色绑定到角色绑定主体（Subject）。
角色绑定主体可以是用户组（Group）、用户（User）或者服务账户（Service Accounts）</p></blockquote>

<p>然后我尝试去k8s中找到一些关于<code>system:nodes</code>的RoleBindings或者ClusterRoleBindings,
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@k8s-master kubernetes<span class="o">]</span><span class="c"># for bd in &lt;code&gt;kubectl get clusterrolebindings |awk &#39;{print $1}&#39;&lt;/code&gt;; do echo $bd;kubectl get clusterrolebindings $bd -o yaml|grep &amp;lsquo;system:nodes&amp;rsquo;;done</span>
NAME
Error from server <span class="o">(</span>NotFound<span class="o">)</span>: clusterrolebindings.rbac.authorization.k8s.io <span class="p">&amp;</span>ldquo<span class="p">;</span>NAME<span class="p">&amp;</span>rdquo<span class="p">;</span> not found
cluster-admin
flannel
kubeadm:kubelet-bootstrap
kubeadm:node-autoapprove-bootstrap
kubeadm:node-autoapprove-certificate-rotation
  name: system:nodes
kubeadm:node-proxier
system:aws-cloud-provider
system:basic-user
<span class="p">&amp;</span>hellip<span class="p">;</span> <span class="p">&amp;</span>hellip<span class="p">;</span></code></pre></div></p>

<p>结局有点意外，除了<code>kubeadm:node-autoapprove-certificate-rotation</code>外，没有找到system相关的rolebindings，显然和我们的理解不一样。
尝试去找<a href="https://github.com/rootsongjc/kubernetes-handbook/blob/master/guide/rbac.md">资料</a>，发现了这么一段</p>

<p><img src="/images/rbac.png" alt="" /></p>

<p>Kubernetes 1.7开始, apiserver的启动中默认增加了<code>--authorization-mode=Node,RBAC</code>,也就是说，除了RBAC外，还有Node这种特殊的授权方式，</p>

<p>继续查找<a href="https://kubernetes.io/docs/reference/access-authn-authz/node/">kubernetes官方的信息</a>,得知
<div class="highlight"><pre><code class="language-bash" data-lang="bash">Node authorization is a special-purpose authorization mode that specifically authorizes API requests made by kubelets.</code></pre></div></p>
]]></content>
  </entry>
  
</feed>
