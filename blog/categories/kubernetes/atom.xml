<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kubernetes | Earlene]]></title>
  <link href="http://liyongxin.github.io/blog/categories/kubernetes/atom.xml" rel="self"/>
  <link href="http://liyongxin.github.io/"/>
  <updated>2019-03-04T11:19:27+08:00</updated>
  <id>http://liyongxin.github.io/</id>
  <author>
    <name><![CDATA[yxli@alauda.io]]></name>
    <email><![CDATA[yxli@alauda.io]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[somethings about kubernetes]]></title>
    <link href="http://liyongxin.github.io/blog/2019/02/28/k8sxiao-zhi-shi/"/>
    <updated>2019-02-28T11:12:56+08:00</updated>
    <id>http://liyongxin.github.io/blog/2019/02/28/k8sxiao-zhi-shi</id>
    <content type="html"><![CDATA[<ul>
<li><p>k8s默认驱逐设置
<div class="highlight"><pre><code class="language-bash" data-lang="bash">// DefaultEvictionHard includes default options <span class="k">for</span> hard eviction.
var <span class="nv">DefaultEvictionHard</span> <span class="o">=</span> map<span class="o">[</span>string<span class="o">]</span>string<span class="o">{</span>
      <span class="p">&amp;</span>ldquo<span class="p">;</span>memory.available<span class="p">&amp;</span>rdquo<span class="p">;</span>:  <span class="p">&amp;</span>ldquo<span class="p">;</span>100Mi<span class="p">&amp;</span>rdquo<span class="p">;</span>,
      <span class="p">&amp;</span>ldquo<span class="p">;</span>nodefs.available<span class="p">&amp;</span>rdquo<span class="p">;</span>:  <span class="p">&amp;</span>ldquo<span class="p">;</span>10%<span class="p">&amp;</span>rdquo<span class="p">;</span>,
      <span class="p">&amp;</span>ldquo<span class="p">;</span>nodefs.inodesFree<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>5%<span class="p">&amp;</span>rdquo<span class="p">;</span>,
      <span class="p">&amp;</span>ldquo<span class="p">;</span>imagefs.available<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>15%<span class="p">&amp;</span>rdquo<span class="p">;</span>,
<span class="o">}</span></code></pre></div></p></li>
<li><p>kubernetes 滚动升级
<div class="highlight"><pre><code class="language-bash" data-lang="bash">&lt;/p&gt;

&lt;h1&gt;run <span class="nb">test </span>deploy&lt;/h1&gt;

<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl run &amp;ndash;image=nginx &amp;ndash;port=80 &amp;ndash;replicas=2 yxli-nginx</span>

&lt;h1&gt;scale replica&lt;/h1&gt;

<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl scale &amp;ndash;replicas=1 deploy/yxli-nginx</span>
<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl get deploy</span>
NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
busybox      <span class="m">2</span>         <span class="m">2</span>         <span class="m">2</span>            <span class="m">2</span>           39d
busybox1     <span class="m">1</span>         <span class="m">1</span>         <span class="m">1</span>            <span class="m">1</span>           39d
yxli-nginx   <span class="m">1</span>         <span class="m">1</span>         <span class="m">1</span>            <span class="m">1</span>           2h

&lt;h1&gt;update image&lt;/h1&gt;

<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl set image  deploy/yxli-nginx yxli-nginx=nginx:alpine</span>

&lt;h1&gt;查看升级历史&lt;/h1&gt;

<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl rollout history deploy/yxli-nginx</span>
deployments <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-nginx<span class="p">&amp;</span>rdquo<span class="p">;</span>
REVISION  CHANGE-CAUSE
<span class="m">1</span>         &lt;none&gt;
<span class="m">2</span>         &lt;none&gt;

&lt;h1&gt;回顾至上次版本&lt;/h1&gt;

<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl rollout undo deploy/yxli-nginx</span>
<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl rollout history deploy/yxli-nginx</span>
deployments <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-nginx<span class="p">&amp;</span>rdquo<span class="p">;</span>
REVISION  CHANGE-CAUSE
<span class="m">2</span>         &lt;none&gt;
<span class="m">3</span>         &lt;none&gt;

&lt;h1&gt;回滚至指定版本&lt;/h1&gt;

&lt;p&gt;<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl rolloutundo deployment/lykops-dpm &amp;ndash;to-revision=2</span></code></pre></div></p></li>
</ul>


<!--more-->


<ul>
<li>查看docker使用的cpu核心数
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@yxli-onebox docker<span class="o">]</span><span class="c"># pwd</span>
/sys/fs/cgroup/cpuset/docker
<span class="o">[</span>root@yxli-onebox docker<span class="o">]</span><span class="c"># cat cpuset.cpus</span>
0-7</code></pre></div></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kubernetes 问题整理]]></title>
    <link href="http://liyongxin.github.io/blog/2018/08/31/kubernetes-wen-ti-zheng-li/"/>
    <updated>2018-08-31T10:36:27+08:00</updated>
    <id>http://liyongxin.github.io/blog/2018/08/31/kubernetes-wen-ti-zheng-li</id>
    <content type="html"><![CDATA[<p>本文用以kubernetes 运维过程中遇到问题汇总，方便日后回顾～</p>

<h2>kubernetes多网卡导致的问题</h2>

<p>部署机器是阿里云，有两块网卡，<code>eth0</code>外网，<code>eth1 vpc</code>内网，集群的路由信息如下
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@10 src<span class="o">]</span><span class="c"># route</span>
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
default         gateway         0.0.0.0         UG    <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> eth1
10.0.0.0        10.81.35.247    255.0.0.0       UG    <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> eth0
10.81.32.0      0.0.0.0         255.255.252.0   U     <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> eth0
39.107.40.0     0.0.0.0         255.255.252.0   U     <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> eth1
100.64.0.0      10.81.35.247    255.192.0.0     UG    <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> eth0
link-local      0.0.0.0         255.255.0.0     U     <span class="m">1002</span>   <span class="m">0</span>        <span class="m">0</span> eth0
link-local      0.0.0.0         255.255.0.0     U     <span class="m">1003</span>   <span class="m">0</span>        <span class="m">0</span> eth1
172.16.0.0      10.81.35.247    255.240.0.0     UG    <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> eth0
192.168.0.0     0.0.0.0         255.255.255.0   U     <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> cni0
192.168.0.0     0.0.0.0         255.255.240.0   U     <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> docker0
192.168.1.0     192.168.1.0     255.255.255.0   UG    <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> flannel.1
192.168.2.0     192.168.2.0     255.255.255.0   UG    <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> flannel.1</code></pre></div></p>

<ul>
<li><h4>docker0网段与cni0网段冲突问题</h4></li>
</ul>


<p>docker启动时没有指定bip，从上述路由规则发现，docker0使用了192.168的段,刚好给flannel设置的cidr段冲突，
所以需要给docker修改默认的网段，解决方法是给docker配置bip网段,然后重启docker，观察docker0的route规则</p>

<p><div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@10 ~<span class="o">]</span><span class="c"># cat /etc/docker/daemon.json</span>
<span class="o">{</span>
    <span class="p">&amp;</span>ldquo<span class="p">;</span>insecure-registries<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="o">[]</span>,
    <span class="p">&amp;</span>ldquo<span class="p">;</span>graph<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>/var/lib/docker<span class="p">&amp;</span>rdquo<span class="p">;</span>,
    <span class="p">&amp;</span>ldquo<span class="p">;</span>bip<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>172.17.0.1/16<span class="p">&amp;</span>rdquo<span class="p">;</span>,
    <span class="p">&amp;</span>ldquo<span class="p">;</span>registry-mirrors<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="o">[</span><span class="p">&amp;</span>ldquo<span class="p">;</span>&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://registry.docker-cn.com&quot;</span>&gt;https://registry.docker-cn.com&lt;/a&gt;<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">]</span>,
    <span class="p">&amp;</span>ldquo<span class="p">;</span>storage-driver<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>devicemapper<span class="p">&amp;</span>rdquo<span class="p">;</span>,
    <span class="p">&amp;</span>ldquo<span class="p">;</span>storage-opts<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="o">[</span><span class="p">&amp;</span>ldquo<span class="p">;</span>dm.use_deferred_removal<span class="o">=</span><span class="nb">true</span><span class="p">&amp;</span>rdquo<span class="p">;</span>, <span class="p">&amp;</span>ldquo<span class="p">;</span>dm.use_deferred_deletion<span class="o">=</span><span class="nb">true</span><span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">]</span>,
    <span class="p">&amp;</span>ldquo<span class="p">;</span>storage-opts<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="o">[</span>
        <span class="p">&amp;</span>ldquo<span class="p">;</span>dm.thinpooldev<span class="o">=</span>/dev/mapper/docker-thinpool<span class="p">&amp;</span>rdquo<span class="p">;</span>,
        <span class="p">&amp;</span>ldquo<span class="p">;</span>dm.min_free_space<span class="o">=</span>0%<span class="p">&amp;</span>rdquo<span class="p">;</span>,
        <span class="p">&amp;</span>ldquo<span class="p">;</span>dm.use_deferred_deletion<span class="o">=</span><span class="nb">true</span><span class="p">&amp;</span>rdquo<span class="p">;</span>,
        <span class="p">&amp;</span>ldquo<span class="p">;</span>dm.use_deferred_removal<span class="o">=</span><span class="nb">true</span><span class="p">&amp;</span>rdquo<span class="p">;</span>,
        <span class="p">&amp;</span>ldquo<span class="p">;</span>dm.fs<span class="o">=</span>ext4<span class="p">&amp;</span>rdquo<span class="p">;</span>
    <span class="o">]</span>,
    <span class="p">&amp;</span>ldquo<span class="p">;</span>log-driver<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>fluentd<span class="p">&amp;</span>rdquo<span class="p">;</span>,
    <span class="p">&amp;</span>ldquo<span class="p">;</span>log-opts<span class="p">&amp;</span>rdquo<span class="p">;</span>:
    <span class="o">{</span>
        <span class="p">&amp;</span>ldquo<span class="p">;</span>fluentd-address<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>localhost:24224<span class="p">&amp;</span>rdquo<span class="p">;</span>,
        <span class="p">&amp;</span>ldquo<span class="p">;</span>tag<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>docker.<span class="p">&amp;</span>rdquo<span class="p">;</span>,
        <span class="p">&amp;</span>ldquo<span class="p">;</span>fluentd-async-connect<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span><span class="nb">true</span><span class="p">&amp;</span>rdquo<span class="p">;</span>
    <span class="o">}</span>
<span class="o">}</span></code></pre></div></p>

<ul>
<li><h4>集群初始化问题</h4></li>
</ul>


<p>使用<code>kubeadm</code>搭建，若未指定&ndash;advertise-address地址则k8s默认拿default网卡，
而机器的default网卡刚好是外网eth0，所以初始化集群使用的地址是外网地址，导致一堆端口需要开，然后Node加入集群失败，解决办法是kubeadm初始化的
时候指定&ndash;advertise-address为内网地址,下面为kubeadm init使用的conf文件</p>

<!--more-->


<p><div class="highlight"><pre><code class="language-bash" data-lang="bash">apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
api:
  bindPort: 6443
etcd:
  endpoints:
  <span class="c">#sed -i &amp;ldquo;/#ETCD_ENDPOINTS/a\  - &lt;a href=&quot;http://123.456:2379/g&quot;&gt;http://123.456:2379/g&lt;/a&gt;&amp;rdquo; ./abc.yml</span>
  <span class="c">#ETCD ENDPOINTS</span>
  - &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;http://10.81.32.150:2379&quot;</span>&gt;http://10.81.32.150:2379&lt;/a&gt;
apiServerExtraArgs:
  apiserver-count: <span class="p">&amp;</span>ldquo<span class="p">;</span>1<span class="p">&amp;</span>rdquo<span class="p">;</span>
  insecure-port: <span class="p">&amp;</span>ldquo<span class="p">;</span>8080<span class="p">&amp;</span>rdquo<span class="p">;</span>
  advertise-address<span class="o">=</span>10.81.32.150
  service-node-port-range: <span class="p">&amp;</span>ldquo<span class="p">;</span>30000-32000<span class="p">&amp;</span>rdquo<span class="p">;</span>
  admission-control: <span class="p">&amp;</span>ldquo<span class="p">;</span>Initializers,NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,DefaultTolerationSeconds,ResourceQuota<span class="p">&amp;</span>rdquo<span class="p">;</span>
  feature-gates: <span class="p">&amp;</span>ldquo<span class="p">;</span><span class="nv">MountPropagation</span><span class="o">=</span><span class="nb">true</span><span class="p">&amp;</span>rdquo<span class="p">;</span>
  endpoint-reconciler-type: <span class="p">&amp;</span>ldquo<span class="p">;</span>lease<span class="p">&amp;</span>rdquo<span class="p">;</span>
controllerManagerExtraArgs:
  pod-eviction-timeout: <span class="p">&amp;</span>ldquo<span class="p">;</span>30s<span class="p">&amp;</span>rdquo<span class="p">;</span>
  node-monitor-period: <span class="p">&amp;</span>ldquo<span class="p">;</span>2s<span class="p">&amp;</span>rdquo<span class="p">;</span>
  node-monitor-grace-period: <span class="p">&amp;</span>ldquo<span class="p">;</span>16s<span class="p">&amp;</span>rdquo<span class="p">;</span>
controllerManagerExtraVolumes:
- name: k8s
  hostPath: /etc/kubernetes
  mountPath: /etc/kubernetes
imageRepository: index.docker.cn/claas
networking:
  podSubnet: 192.168.0.0/16
kubernetesVersion: v1.9.6
token: 8d775a.8f70da6999842a27
tokenTTL: <span class="p">&amp;</span>ldquo<span class="p">;</span>0<span class="p">&amp;</span>rdquo<span class="p">;</span>
apiServerCertSANs:
- 127.0.0.1
- amazonaws.com.cn
- amazonaws.com
- 10.81.32.150
- 10.81.32.150</code></pre></div>
- ####flannel网络问题
多网卡导致flannel网络选择网卡错误,flannel在初始化的时候会默认找defalut网卡，如果需要指定，则在flannel的
初始化yaml文件中通过&ndash;iface指定网卡，
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="p">&amp;</span>hellip<span class="p">;</span> <span class="p">&amp;</span>hellip<span class="p">;</span>
      containers:
      - name: kube-flannel
        image: index.alauda.cn/claas/flannel:v0.9.1
        <span class="nb">command</span>: <span class="o">[</span> <span class="p">&amp;</span>ldquo<span class="p">;</span>/opt/bin/flanneld<span class="p">&amp;</span>rdquo<span class="p">;</span>, <span class="p">&amp;</span>ldquo<span class="p">;&amp;</span>ndash<span class="p">;</span>ip-masq<span class="p">&amp;</span>rdquo<span class="p">;</span>, <span class="p">&amp;</span>ldquo<span class="p">;&amp;</span>ndash<span class="p">;</span>kube-subnet-mgr<span class="p">&amp;</span>rdquo<span class="p">;</span>, <span class="p">&amp;</span>ldquo<span class="p">;&amp;</span>ndash<span class="p">;</span><span class="nv">iface</span><span class="o">=</span>eth0<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">]</span>
        securityContext:
          privileged: <span class="nb">true</span>
<span class="nb">        </span>env:
  <span class="p">&amp;</span>hellip<span class="p">;</span> <span class="p">&amp;</span>hellip<span class="p">;</span></code></pre></div></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kubelet的认证]]></title>
    <link href="http://liyongxin.github.io/blog/2018/08/25/kubeletde-ren-zheng/"/>
    <updated>2018-08-25T18:17:17+08:00</updated>
    <id>http://liyongxin.github.io/blog/2018/08/25/kubeletde-ren-zheng</id>
    <content type="html"><![CDATA[<p>研究完<a href="../kuberneteszhong-de-ren-zheng-xiang-guan">kubectl的认证与授权</a>，使用相同的方式去找kubelet的访问，
首先定位配置文件<code>/etc/kubernetes/kubelet.conf</code>，然后用相同的方式对<code>client-key-data</code>做base64解码，保存为kubelet.crt文件。</p>

<p>openssl查看crt证书，
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@k8s-master kubernetes<span class="o">]</span><span class="c"># openssl x509 -text -in kubelet.crt -noout</span>
Certificate:
    Data:
        Version: <span class="m">3</span> <span class="o">(</span>0x2<span class="o">)</span>
        Serial Number: <span class="m">8126553944389053218</span> <span class="o">(</span>0x70c751c18f5beb22<span class="o">)</span>
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: <span class="nv">CN</span><span class="o">=</span>kubernetes
        Validity
            Not Before: Aug <span class="m">20</span> 05:50:39 <span class="m">2018</span> GMT
            Not After : Aug <span class="m">20</span> 05:50:42 <span class="m">2019</span> GMT
        Subject: <span class="nv">O</span><span class="o">=</span>system:nodes, <span class="nv">CN</span><span class="o">=</span>system:node:k8s-master
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: <span class="o">(</span><span class="m">2048</span> bit<span class="o">)</span>
                Modulus:
                    00:9f:92:83:49:aa:cc:52:0e:de:bd:af:a6:fd:ef:
    <span class="p">&amp;</span>hellip<span class="p">;</span> <span class="p">&amp;</span>hellip<span class="p">;</span></code></pre></div></p>

<p>得到我们期望的内容：
<div class="highlight"><pre><code class="language-bash" data-lang="bash">Subject: <span class="nv">O</span><span class="o">=</span>system:nodes, <span class="nv">CN</span><span class="o">=</span>system:node:k8s-master</code></pre></div></p>

<blockquote><p>关于Subject，在k8s中可以理解为角色绑定主体，RoleBinding或者ClusterRoleBinding可以将角色绑定到角色绑定主体（Subject）。
角色绑定主体可以是用户组（Group）、用户（User）或者服务账户（Service Accounts）</p></blockquote>

<p>然后我尝试去k8s中找到一些关于<code>system:nodes</code>的RoleBindings或者ClusterRoleBindings,
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@k8s-master kubernetes<span class="o">]</span><span class="c"># for bd in &lt;code&gt;kubectl get clusterrolebindings |awk &#39;{print $1}&#39;&lt;/code&gt;; do echo $bd;kubectl get clusterrolebindings $bd -o yaml|grep &amp;lsquo;system:nodes&amp;rsquo;;done</span>
NAME
Error from server <span class="o">(</span>NotFound<span class="o">)</span>: clusterrolebindings.rbac.authorization.k8s.io <span class="p">&amp;</span>ldquo<span class="p">;</span>NAME<span class="p">&amp;</span>rdquo<span class="p">;</span> not found
cluster-admin
flannel
kubeadm:kubelet-bootstrap
kubeadm:node-autoapprove-bootstrap
kubeadm:node-autoapprove-certificate-rotation
  name: system:nodes
kubeadm:node-proxier
system:aws-cloud-provider
system:basic-user
<span class="p">&amp;</span>hellip<span class="p">;</span> <span class="p">&amp;</span>hellip<span class="p">;</span></code></pre></div></p>

<p>结局有点意外，除了<code>kubeadm:node-autoapprove-certificate-rotation</code>外，没有找到system相关的rolebindings，显然和我们的理解不一样。
尝试去找<a href="https://github.com/rootsongjc/kubernetes-handbook/blob/master/guide/rbac.md">资料</a>，发现了这么一段</p>

<p><img src="/images/rbac.png" alt="" /></p>

<p>Kubernetes 1.7开始, apiserver的启动中默认增加了<code>--authorization-mode=Node,RBAC</code>,也就是说，除了RBAC外，还有Node这种特殊的授权方式，</p>

<p>继续查找<a href="https://kubernetes.io/docs/reference/access-authn-authz/node/">kubernetes官方的信息</a>,得知
<div class="highlight"><pre><code class="language-bash" data-lang="bash">Node authorization is a special-purpose authorization mode that specifically authorizes API requests made by kubelets.</code></pre></div></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kubectl的认证与授权]]></title>
    <link href="http://liyongxin.github.io/blog/2018/08/25/kuberneteszhong-de-ren-zheng-xiang-guan/"/>
    <updated>2018-08-25T14:41:00+08:00</updated>
    <id>http://liyongxin.github.io/blog/2018/08/25/kuberneteszhong-de-ren-zheng-xiang-guan</id>
    <content type="html"><![CDATA[<p>关于k8s认证、授权相关的基础，只简单回顾，具体内容先参考如下文章：</p>

<ul>
<li><a href="https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/">Controlling Access to the Kubernetes API</a></li>
<li><a href="https://jimmysong.io/posts/user-authentication-in-kubernetes/">Kubernetes 中的用户与身份认证授权</a></li>
<li><a href="https://github.com/rootsongjc/kubernetes-handbook/blob/master/guide/rbac.md">Kubernetes RBAC - 基于角色的访问控制</a></li>
</ul>


<h2>Kubernetes API的访问控制原理回顾</h2>

<p>Kubernetes集群的访问权限控制由<code>kube-apiserver</code>负责，<code>kube-apiserver</code>的访问权限控制由身份验证(authentication)、授权(authorization)
和准入控制（admission control）三步骤组成，这三步骤是按序进行的：
<img src="/images/k8s-apiserver-access-control-overview.svg" alt="" /></p>

<h4>身份验证（Authentication）</h4>

<p>这个环节它面对的输入是整个<code>http request</code>，它负责对来自client的请求进行身份校验，支持的方法包括：client证书验证（https双向验证）、
<code>basic auth</code>、普通token以及<code>jwt token</code>(用于serviceaccount)。</p>

<p>APIServer启动时，可以指定一种Authentication方法，也可以指定多种方法。如果指定了多种方法，那么APIServer将会逐个使用这些方法对客户端请求进行验证，
只要请求数据通过其中一种方法的验证，APIServer就会认为Authentication成功；</p>

<p>在较新版本kubeadm引导启动的k8s集群的apiserver初始配置中，默认支持<code>client证书</code>验证和<code>serviceaccount</code>两种身份验证方式。
证书认证通过设置<code>--client-ca-file</code>根证书以及<code>--tls-cert-file</code>和<code>--tls-private-key-file</code>来开启。</p>

<p>在这个环节，apiserver会通过client证书或
<code>http header</code>中的字段(比如serviceaccount的<code>jwt token</code>)来识别出请求的<code>用户身份</code>，包括”user”、”group”等，这些信息将在后面的<code>authorization</code>环节用到。</p>

<h4>授权（Authorization）</h4>

<p>这个环节面对的输入是<code>http request context</code>中的各种属性，包括：<code>user</code>、<code>group</code>、<code>request path</code>（比如：<code>/api/v1</code>、<code>/healthz</code>、<code>/version</code>等）、
<code>request verb</code>(比如：<code>get</code>、<code>list</code>、<code>create</code>等)。</p>

<p>APIServer会将这些属性值与事先配置好的访问策略(<code>access policy</code>）相比较。APIServer支持多种<code>authorization mode</code>，包括<code>Node、RBAC、Webhook</code>等。</p>

<p>APIServer启动时，可以指定一种<code>authorization mode</code>，也可以指定多种<code>authorization mode</code>，如果是后者，只要Request通过了其中一种mode的授权，
那么该环节的最终结果就是授权成功。在较新版本kubeadm引导启动的k8s集群的apiserver初始配置中，<code>authorization-mode</code>的默认配置是<code>”Node,RBAC”</code>。</p>

<p>Node授权器主要用于各个node上的kubelet访问apiserver时使用的，其他一般均由RBAC授权器来授权。</p>

<!--more-->


<h2>kubectl的授权认证</h2>

<p><code>kubeadm init</code>启动完master节点后，会默认输出类似下面的提示内容：
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="p">&amp;</span>hellip<span class="p">;</span> <span class="p">&amp;</span>hellip<span class="p">;</span>
Your Kubernetes master has initialized successfully!&lt;/p&gt;

&lt;p&gt;To start using your cluster, you need to run the following as a regular user:
  mkdir -p <span class="nv">$HOME</span>/.kube
  sudo cp -i /etc/kubernetes/admin.conf <span class="nv">$HOME</span>/.kube/config
  sudo chown <span class="k">$(</span>id -u<span class="k">)</span>:<span class="k">$(</span>id -g<span class="k">)</span> <span class="nv">$HOME</span>/.kube/config
<span class="p">&amp;</span>hellip<span class="p">;</span> <span class="p">&amp;</span>hellip<span class="p">;</span></code></pre></div>
这些信息是在告知我们如何配置<code>kubeconfig</code>文件。按照上述命令配置后，master节点上的<code>kubectl</code>就可以直接使用<code>$HOME/.kube/config</code>的信息访问<code>k8s cluster</code>了。
并且，通过这种配置方式，<code>kubectl</code>也拥有了整个集群的管理员(root)权限。</p>

<p>很多K8s初学者在这里都会有疑问：</p>

<ul>
<li>当<code>kubectl</code>使用这种<code>kubeconfig</code>方式访问集群时，<code>Kubernetes</code>的<code>kube-apiserver</code>是如何对来自<code>kubectl</code>的访问进行身份验证(<code>authentication</code>)和授权(<code>authorization</code>)的呢？</li>
<li>为什么来自<code>kubectl</code>的请求拥有最高的管理员权限呢？</li>
</ul>


<h4>kubectl的身份认证（authentication）</h4>

<p>我们先从kubectl使用的<code>kubeconfig</code>入手。kubectl使用的kubeconfig文件实质上就是<code>kubeadm init</code>过程中生成的<code>/etc/kubernetes/admin.conf</code>，
我们查看一下该kubeconfig文件的内容：
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@k8s-master kubernetes<span class="o">]</span><span class="c"># kubectl config view</span>
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://192.168.8.33:6443&quot;</span>&gt;https://192.168.8.33:6443&lt;/a&gt;
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: <span class="o">{}</span>
users:
- name: kubernetes-admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED</code></pre></div>
关于kubeconfig文件的解释，可以在<a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/">这里</a>自行脑补。在这些输出信息中，我们着重提取到两个信息：
<div class="highlight"><pre><code class="language-bash" data-lang="bash">user name: kubernetes-admin
client-certificate-data: XXXX</code></pre></div>
前面提到过apiserver的authentication支持通过<code>tls client certificate、basic auth、token</code>等方式对客户端发起的请求进行身份校验，
从kubeconfig信息来看，kubectl显然在请求中使用了<code>tls client certificate</code>的方式，即客户端的证书。另外我们知道Kubernetes是没有user这种资源的，
通过k8s API也无法创建user。</p>

<p>那么kubectl的身份信息就应该“隐藏”在<code>client-certificate</code>的数据中，我们来查看一下。</p>

<p>首先把<code>/etc/kubernetes/kubelet.conf</code>中的<code>client-key-data</code>内容保存在<code>admin-client-certificate.txt</code>中
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@k8s-master kubernetes<span class="o">]</span><span class="c"># cat admin-client-certificate.txt</span>
<span class="nv">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</span><span class="o">=</span></code></pre></div>
然后对上述内容base64解密，然后存储在<code>admin.crt</code>文件中
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@k8s-master kubernetes<span class="o">]</span><span class="c"># cat admin-client-certificate.txt |base64 -d &gt; admin.crt</span>
<span class="o">[</span>root@k8s-master kubernetes<span class="o">]</span><span class="c"># cat admin.crt</span>
<span class="p">&amp;</span>mdash<span class="p">;&amp;</span>ndash<span class="p">;</span>BEGIN CERTIFICATE<span class="p">&amp;</span>mdash<span class="p">;&amp;</span>ndash<span class="p">;</span>
MIIC8jCCAdqgAwIBAgIIQD98S9X+SVUwDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE
AxMKa3ViZXJuZXRlczAeFw0xODA4MjAwNTUwMzlaFw0xOTA4MjAwNTUwNDFaMDQx
FzAVBgNVBAoTDnN5c3RlbTptYXN0ZXJzMRkwFwYDVQQDExBrdWJlcm5ldGVzLWFk
bWluMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAs5Rpnr5a68Cp4/EC
1IeebFl1z3ixDVT3pwR5YZgfy5E98IYB42c3fFrgV6fOuewOaW3bwrS9SZ4NMqB9
6TYXlZYkwxeQIp1HE+VSdbAHww7XE8zizv9/BSEynSqDglodmmDres0Cs9/3PG2F
B9OcAVycRzvxx87iecSzRwVF5DoopFbYkPJY/OjTQMeO8LX8YvoBYCD0ZpHxu7NE
b9qdUhPKH7ExbgSsSVZ75npjNtdDzlFzD4+tyYkvpBIYttOWHMFMQhipOyG+t1Af
ydVHzzsiAxgA/00ulxNCvhx1WXN+mL3PeDeBYMSFWo6cg60ih0nnNPk3rezrHoAg
294QxwIDAQABoycwJTAOBgNVHQ8BAf8EBAMCBaAwEwYDVR0lBAwwCgYIKwYBBQUH
AwIwDQYJKoZIhvcNAQELBQADggEBAFQ9ZvPCQsQQVR1kszsHip3qqcmIwUlkJiF6
YUVRzeG/QG15dIid5i87q5ZyK+NZhsuBrROnNUDSlg77jD61iHw+jREWd1pYAoK3
OyLcFd5q73xp+0aP1yEsRDnTmb7gzvKAYnFwKue7OZOVfpzWk0qakWkaPrzx6Bzp
G62X6p6701sL+9Gru56M8+tp+3/z635Z+56VjAFErzs5Sv5Pw5eAYxA12ebigNeh
0fIpVyPSZtA1MYgkbtqvjR6qxpgQUBvTCL7unNOGmdrvZI73fDLl+tTvlcgFDWcm
jlt8d2/5x/55BAfH/6LfqzfbDfOqlicKYvogLa7QE/A0uquVjLg<span class="o">=</span>
<span class="p">&amp;</span>mdash<span class="p">;&amp;</span>ndash<span class="p">;</span>END CERTIFICATE<span class="p">&amp;</span>mdash<span class="p">;&amp;</span>ndash<span class="p">;</span></code></pre></div>
然后用openssl工具查看crt证书内容
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@k8s-master kubernetes<span class="o">]</span><span class="c"># openssl x509 -text -in admin.crt -noout</span>
Certificate:
    Data:
        Version: <span class="m">3</span> <span class="o">(</span>0x2<span class="o">)</span>
        Serial Number: <span class="m">4629555607114762581</span> <span class="o">(</span>0x403f7c4bd5fe4955<span class="o">)</span>
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: <span class="nv">CN</span><span class="o">=</span>kubernetes
        Validity
            Not Before: Aug <span class="m">20</span> 05:50:39 <span class="m">2018</span> GMT
            Not After : Aug <span class="m">20</span> 05:50:41 <span class="m">2019</span> GMT
        Subject: <span class="nv">O</span><span class="o">=</span>system:masters, <span class="nv">CN</span><span class="o">=</span>kubernetes-admin
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
            <span class="p">&amp;</span>hellip<span class="p">;</span> <span class="p">&amp;</span>hellip<span class="p">;</span>&lt;/p&gt;

&lt;p&gt;</code></pre></div>
从证书输出的信息中，我们看到了下面这行：
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Subject</span><span class="p">:</span> <span class="n">O</span><span class="o">=</span><span class="n">system</span><span class="p">:</span><span class="n">masters</span><span class="p">,</span> <span class="n">CN</span><span class="o">=</span><span class="n">kubernetes</span><span class="o">-</span><span class="n">admin</span></code></pre></div></p>

<p>说明在认证阶段，<code>apiserver</code>会首先使用<code>--client-ca-file</code>配置的CA证书去验证kubectl提供的证书的有效性,基本的方式
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@k8s-master kubernetes<span class="o">]</span><span class="c"># openssl verify -CAfile /etc/kubernetes/pki/ca.crt /etc/kubernetes/admin.crt</span>
/etc/kubernetes/admin.crt: OK</code></pre></div>
然后认证通过后，提取出签发证书时指定的CN(Common Name),<code>kubernetes-admin</code>，作为请求的用户名 (User Name),
O(Organization)， 从证书中提取该字段作为请求用户所属的组 (Group)，<code>group = system:masters</code>，然后传递给后面的授权模块</p>

<blockquote><p>X509 客户端证书
通过将 &ndash;client-ca-file=SOMEFILE 选项传递给 API server 来启用客户端证书认证。引用的文件必须包含一个或多个证书颁发机构，用于验证提交给 API server 的客户端证书。如果客户端证书已提交并验证，则使用 subject 的 Common Name（CN）作为请求的用户名。从 Kubernetes 1.4开始，客户端证书还可以使用证书的 organization 字段来指示用户的组成员身份。</p></blockquote>

<h4>kubectl的授权</h4>

<p>kubeadm在init初始引导集群启动过程中，创建了许多<code>default</code>的<code>role、clusterrole、rolebinding</code>和<code>clusterrolebinding</code>，
在k8s有关RBAC的官方文档中，我们看到下面一些<code>default clusterrole</code>列表:</p>

<p><img src="/images/kubeadm-default-clusterrole-list.png" alt="" />
其中第一个cluster-admin这个cluster role binding绑定了system:masters group，这和authentication环节传递过来的身份信息不谋而合。
沿着system:masters group对应的cluster-admin clusterrolebinding“追查”下去，真相就会浮出水面。</p>

<p>我们查看一下这一binding：
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@k8s-master kubernetes<span class="o">]</span><span class="c"># kubectl get clusterrolebinding/cluster-admin -n kube-system -o yaml</span>
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: <span class="p">&amp;</span>ldquo<span class="p">;</span><span class="nb">true</span><span class="p">&amp;</span>rdquo<span class="p">;</span>
  creationTimestamp: 2018-08-20T05:51:30Z
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: cluster-admin
  resourceVersion: <span class="p">&amp;</span>ldquo<span class="p">;</span>93<span class="p">&amp;</span>rdquo<span class="p">;</span>
  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/cluster-admin
  uid: 163adc34-a43d-11e8-89a4-000c2948e532
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:masters</code></pre></div>
 我们看到在kube-system名字空间中，一个名为cluster-admin的clusterrolebinding将cluster-admin cluster role与system:masters Group绑定到了一起，
 赋予了所有归属于system:masters Group中用户cluster-admin角色所拥有的权限。</p>

<p> 我们再来查看一下cluster-admin这个role的具体权限信息：
 <div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@k8s-master kubernetes<span class="o">]</span><span class="c"># kubectl get clusterrole/cluster-admin -n kube-system -o yaml</span>
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: <span class="p">&amp;</span>ldquo<span class="p">;</span><span class="nb">true</span><span class="p">&amp;</span>rdquo<span class="p">;</span>
  creationTimestamp: 2018-08-20T05:51:30Z
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: cluster-admin
  resourceVersion: <span class="p">&amp;</span>ldquo<span class="p">;</span>40<span class="p">&amp;</span>rdquo<span class="p">;</span>
  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/cluster-admin
  uid: 15f71927-a43d-11e8-89a4-000c2948e532
rules:
- apiGroups:
  - <span class="p">&amp;</span>lsquo<span class="p">;</span>&lt;em&gt;<span class="p">&amp;</span>rsquo<span class="p">;</span>
  resources:
  - <span class="p">&amp;</span>lsquo<span class="p">;</span>&lt;/em&gt;<span class="p">&amp;</span>rsquo<span class="p">;</span>
  verbs:
  - <span class="p">&amp;</span>lsquo<span class="p">;</span>&lt;em&gt;<span class="p">&amp;</span>rsquo<span class="p">;</span>
- nonResourceURLs:
  - <span class="p">&amp;</span>lsquo<span class="p">;</span>&lt;/em&gt;<span class="p">&amp;</span>rsquo<span class="p">;</span>
  verbs:
  - <span class="p">&amp;</span>lsquo<span class="p">;</span>*<span class="p">&amp;</span>rsquo<span class="p">;</span></code></pre></div>
从rules列表中来看，cluster-admin这个角色对所有resources、verbs、apiGroups均有无限制的操作权限，
即整个集群的root权限。于是kubectl的请求就可以操控和管理整个集群了。</p>

<h3>疑问</h3>

<p>使用kubeadm-1.11.2版本初始化集群，即使不配置.kube/config文件，也可以直接访问到kubernetes cluster,
<a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">官方文档</a>中对这块的记录如下：</p>

<p>To make kubectl work for your non-root user, run these commands, which are also part of the kubeadm init output:
 <div class="highlight"><pre><code class="language-bash" data-lang="bash">mkdir -p <span class="nv">$HOME</span>/.kube
sudo cp -i /etc/kubernetes/admin.conf <span class="nv">$HOME</span>/.kube/config
sudo chown <span class="k">$(</span>id -u<span class="k">)</span>:<span class="k">$(</span>id -g<span class="k">)</span> <span class="nv">$HOME</span>/.kube/config</code></pre></div></p>

<p>Alternatively, if you are the root user, you can run:
 <div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">export </span><span class="nv">KUBECONFIG</span><span class="o">=</span>/etc/kubernetes/admin.conf</code></pre></div></p>

<ul>
<li>关于非root用户，尝试新建了用户，且没有配置kubeconfig文件的情况下，依然可以通过kubectl直接访问集群。</li>
<li>关于<code>KUBECONFIG</code>环境变量，发现未设置该env，而且尝试把<code>/etc/kubernetes/admin.conf</code> 文件删除掉，重启apiserver的情况，依然可以访问</li>
</ul>


<p>关于以上疑问已经弄明白，当我们在master节点中使用kubectl请求时，如果没有设置$HOME/.kube/config文件，则默认是通过本地的非安全端口来访问
apiserver，
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@ip-172-31-10-236 centos<span class="o">]</span><span class="c"># kubectl get no -v 7</span>
I0930 06:55:41.036661   <span class="m">13865</span> cached_discovery.go:72<span class="o">]</span> returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/alauda.io/v3/serverresources.json
I0930 06:55:41.037250   <span class="m">13865</span> cached_discovery.go:72<span class="o">]</span> returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/v1/serverresources.json
I0930 06:55:41.037484   <span class="m">13865</span> round_trippers.go:383<span class="o">]</span> GET &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;http://localhost:8080/api/v1/nodes&quot;</span>&gt;http://localhost:8080/api/v1/nodes&lt;/a&gt;
I0930 06:55:41.037502   <span class="m">13865</span> round_trippers.go:390<span class="o">]</span> Request Headers:
I0930 06:55:41.037515   <span class="m">13865</span> round_trippers.go:393<span class="o">]</span>     Accept: application/json
I0930 06:55:41.037528   <span class="m">13865</span> round_trippers.go:393<span class="o">]</span>     User-Agent: kubectl/v1.7.3 <span class="o">(</span>linux/amd64<span class="o">)</span> kubernetes/2c2fe6e
I0930 06:55:41.046449   <span class="m">13865</span> round_trippers.go:408<span class="o">]</span> Response Status: <span class="m">200</span> OK in <span class="m">8</span> milliseconds
I0930 06:55:41.067926   <span class="m">13865</span> cached_discovery.go:119<span class="o">]</span> returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/servergroups.json
I0930 06:55:41.068015   <span class="m">13865</span> cached_discovery.go:72<span class="o">]</span> returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/apiregistration.k8s.io/v1beta1/serverresources.json</code></pre></div>
 如果</p>

<h3>小结</h3>

<p>总结一下kubectl的认证过程：</p>

<p><img src="/images/how-kubectl-be-authorized.png" alt="" /></p>
]]></content>
  </entry>
  
</feed>
