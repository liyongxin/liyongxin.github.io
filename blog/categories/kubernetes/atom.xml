<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kubernetes | Earlene]]></title>
  <link href="http://liyongxin.github.io/blog/categories/kubernetes/atom.xml" rel="self"/>
  <link href="http://liyongxin.github.io/"/>
  <updated>2019-06-27T19:20:27+08:00</updated>
  <id>http://liyongxin.github.io/</id>
  <author>
    <name><![CDATA[yxli@alauda.io]]></name>
    <email><![CDATA[yxli@alauda.io]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Kubernetes离线安装手册(非高可用版)]]></title>
    <link href="http://liyongxin.github.io/blog/2019/06/27/kuberneteschi-xian-an-zhuang-shou-ce-fei-gao-ke-yong-ban/"/>
    <updated>2019-06-27T17:58:14+08:00</updated>
    <id>http://liyongxin.github.io/blog/2019/06/27/kuberneteschi-xian-an-zhuang-shou-ce-fei-gao-ke-yong-ban</id>
    <content type="html"><![CDATA[<h2>集群信息</h2>

<h3>1. 节点规划</h3>

<p>部署k8s集群的节点按照用途可以划分为如下3类角色：</p>

<ul>
<li><strong>master</strong> ：集群的master节点，至少一台</li>
<li><strong>slave</strong>：集群的slave节点，非必须存在</li>
<li><strong>init</strong>：集群的初始化节点，提供集群初始化必要的镜像及软件源，可以与master节点使用同一台机器</li>
</ul>


<p>单个节点可以对应多个角色，比如master节点可以同时作为init节点。因为slave节点不是必须存在，因此部署非高可用集群最少仅需一台机器即可，基础配置不低于2C4G。</p>

<p><strong>本例为了演示slave节点的添加，会部署一台master+1台slave，其中master和init共用一台机器</strong>，节点规划如下：</p>

<table>
<thead>
<tr>
<th style="text-align:center;">主机名 </th>
<th style="text-align:center;">  节点ip  </th>
<th style="text-align:center;"> 角色 </th>
<th style="text-align:center;"> 部署组件  </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">k8s-init  </td>
<td style="text-align:center;"> 10.0.129.84  </td>
<td style="text-align:center;">  init   </td>
<td style="text-align:center;"> registry,  httpd  </td>
</tr>
<tr>
<td style="text-align:center;">k8s-slave </td>
<td style="text-align:center;"> 10.0.128.240 </td>
<td style="text-align:center;"> slave </td>
<td style="text-align:center;">  kubectl, kubeadm, kubelet, kube-proxy, flannel  </td>
</tr>
<tr>
<td style="text-align:center;">k8s-master</td>
<td style="text-align:center;"> 10.0.129.84  </td>
<td style="text-align:center;"> master </td>
<td style="text-align:center;">  etcd, kube-apiserver, kube-controller-manager, kubectl, kubeadm, kubelet, kube-proxy, flannel</td>
</tr>
</tbody>
</table>


<h3>2. 组件版本</h3>

<table>
<thead>
<tr>
<th style="text-align:center;"> 组件      </th>
<th style="text-align:center;">    版本 </th>
<th style="text-align:left;"> 说明  </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;"> CentOS  </td>
<td style="text-align:center;"> 7.5.1804 </td>
<td style="text-align:left;">     </td>
</tr>
<tr>
<td style="text-align:center;"> Kernel  </td>
<td style="text-align:center;"> Linux 3.10.0-862.el7.x86_64 </td>
<td style="text-align:left;">   </td>
</tr>
<tr>
<td style="text-align:center;"> etcd    </td>
<td style="text-align:center;"> 3.2.24 </td>
<td style="text-align:left;"> 使用容器方式部署，默认数据挂载到本地路径 </td>
</tr>
<tr>
<td style="text-align:center;"> coredns </td>
<td style="text-align:center;"> 1.2.6 </td>
<td style="text-align:left;">  </td>
</tr>
<tr>
<td style="text-align:center;"> kubeadm </td>
<td style="text-align:center;"> v1.13.3 </td>
<td style="text-align:left;">  </td>
</tr>
<tr>
<td style="text-align:center;"> kubectl </td>
<td style="text-align:center;"> v1.13.3 </td>
<td style="text-align:left;">  </td>
</tr>
<tr>
<td style="text-align:center;"> kubelet </td>
<td style="text-align:center;"> v1.13.3 </td>
<td style="text-align:left;">  </td>
</tr>
<tr>
<td style="text-align:center;"> kube-proxy </td>
<td style="text-align:center;"> v1.13.3 </td>
<td style="text-align:left;">  </td>
</tr>
<tr>
<td style="text-align:center;"> flannel </td>
<td style="text-align:center;"> v0.11.0 </td>
<td style="text-align:left;"> 使用vxlan作为backend </td>
</tr>
<tr>
<td style="text-align:center;"> httpd </td>
<td style="text-align:center;"> v2.4.6 </td>
<td style="text-align:left;"> 部署在init节点，默认使用80端口提供服务 </td>
</tr>
<tr>
<td style="text-align:center;"> registry </td>
<td style="text-align:center;"> v2.3.1 </td>
<td style="text-align:left;"> 部署在init节点，默认使用60080端口提供服务 </td>
</tr>
</tbody>
</table>


<h2>安装前准备工作</h2>

<h3>1. 设置hosts解析</h3>

<p>操作节点：所有节点（<code>k8s-init，k8s-master，k8s-slave</code>）均需执行
- <strong>修改hostname</strong>
hostname必须只能包含小写字母、数字、",&ldquo;、&rdquo;-&ldquo;，且开头结尾必须是小写字母或数字
<div class="highlight"><pre><code class="language-bash" data-lang="bash">&lt;/p&gt;

&lt;h1&gt;在master节点&lt;/h1&gt;

&lt;p&gt;<span class="nv">$ </span>hostnamectl <span class="nb">set</span>-hostname k8s-master <span class="c">#设置master节点的hostname&lt;/p&gt;</span>

&lt;h1&gt;在slave节点&lt;/h1&gt;

&lt;p&gt;<span class="nv">$ </span>hostnamectl <span class="nb">set</span>-hostname k8s-slave <span class="c">#设置slave节点的hostname</span></code></pre></div>
- <strong>添加hosts解析</strong>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>cat &gt;&gt;/etc/hosts<span class="p">&amp;</span>lt<span class="p">;&amp;</span>lt<span class="p">;</span>EOF
10.0.129.84 k8s-master k8s-init
10.0.128.240 k8s-slave
EOF</code></pre></div></p>

<h3>2. 调整系统配置</h3>

<p>操作节点： 所有的master和slave节点（<code>k8s-master,k8s-slave</code>）需要执行</p>

<blockquote><p>本章下述操作均以k8s-master为例，其他节点均是相同的操作（ip和hostname的值换成对应机器的真实值）</p></blockquote>

<ul>
<li><p><strong>设置安全组开放端口</strong>
如果节点间无安全组限制（内网机器间可以任意访问），可以忽略，否则，至少保证如下端口可通：
k8s-init节点：TCP：7443，60080，60081，UDP协议端口全部打开
k8s-master节点：TCP：6443，2379，2380，UDP协议端口全部打开</p></li>
<li><p><strong>设置iptables</strong>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span> iptables -P FORWARD ACCEPT</code></pre></div></p></li>
<li><p><strong>关闭swap</strong>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>swapoff -a&lt;/p&gt;

&lt;h1&gt;防止开机自动挂载 swap 分区&lt;/h1&gt;

&lt;p&gt;<span class="nv">$ </span> sed -i <span class="p">&amp;</span>lsquo<span class="p">;</span>/ swap / s/^<span class="o">(</span>.*<span class="o">)</span><span class="nv">$/</span><span class="c">#\1/g&amp;rsquo; /etc/fstab</span></code></pre></div></p></li>
<li><strong>关闭selinux和防火墙</strong>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sed -ri <span class="p">&amp;</span>rsquo<span class="p">;</span>s#<span class="o">(</span><span class="nv">SELINUX</span><span class="o">=)</span>.*#<span class="se">\1</span>disabled#<span class="p">&amp;</span>lsquo<span class="p">;</span> /etc/selinux/config
<span class="nv">$ </span>setenforce 0
<span class="nv">$ </span>systemctl disable firewalld <span class="p">&amp;</span>amp<span class="p">;&amp;</span>amp<span class="p">;</span> systemctl stop firewalld</code></pre></div></li>
<li><strong>修改内核参数</strong>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>cat <span class="p">&amp;</span>lt<span class="p">;</span>&lt;EOF &gt;  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables <span class="o">=</span> 1
net.bridge.bridge-nf-call-iptables <span class="o">=</span> 1
net.ipv4.ip_forward<span class="o">=</span>1
EOF
<span class="nv">$ </span>modprobe br_netfilter
<span class="nv">$ </span>sysctl -p /etc/sysctl.d/k8s.conf</code></pre></div></li>
<li><strong>加载ipvs模块</strong>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>cat &gt; /etc/sysconfig/modules/ipvs.modules <span class="p">&amp;</span>lt<span class="p">;&amp;</span>lt<span class="p">;</span>EOF

&lt;h1&gt;!/bin/bash&lt;/h1&gt;

&lt;p&gt;modprobe <span class="p">&amp;</span>ndash<span class="p">;</span> ip_vs
modprobe <span class="p">&amp;</span>ndash<span class="p">;</span> ip_vs_rr
modprobe <span class="p">&amp;</span>ndash<span class="p">;</span> ip_vs_wrr
modprobe <span class="p">&amp;</span>ndash<span class="p">;</span> ip_vs_sh
modprobe <span class="p">&amp;</span>ndash<span class="p">;</span> nf_conntrack_ipv4
EOF
<span class="nv">$ </span>chmod <span class="m">755</span> /etc/sysconfig/modules/ipvs.modules <span class="p">&amp;</span>amp<span class="p">;&amp;</span>amp<span class="p">;</span> bash /etc/sysconfig/modules/ipvs.modules <span class="p">&amp;</span>amp<span class="p">;&amp;</span>amp<span class="p">;</span> lsmod <span class="p">|</span> grep -e ip_vs -e nf_conntrack_ipv4</code></pre></div></p></li>
</ul>


<h3>3. 拷贝安装包</h3>

<p>操作节点： <code>k8s-init</code>节点
<div class="highlight"><pre><code class="language-bash" data-lang="bash">&lt;/p&gt;

&lt;h1&gt;安装包拷贝到k8s-init节点的/opt目录&lt;/h1&gt;

&lt;p&gt;<span class="nv">$ </span>scp k8s-installer.tar.gz root@k8s-init:/opt&lt;/p&gt;

&lt;h1&gt;解压并查看安装包&lt;/h1&gt;

&lt;p&gt;<span class="nv">$ </span>tar -zxf /opt/k8s-installer.tar.gz -C /opt
<span class="nv">$ </span>ls -lh /opt/k8s-installer  <span class="c"># 查看安装包，会包含如下4项</span>
total 337M
drwxr-xr-x <span class="m">3</span> root root 4.0K Jun <span class="m">16</span> 21:00 docker-ce
-rw-r<span class="p">&amp;</span>ndash<span class="p">;</span>r<span class="p">&amp;</span>ndash<span class="p">;</span> <span class="m">1</span> root root  13K Jun <span class="m">16</span> 14:00 kube-flannel.yml
drwxr-xr-x <span class="m">3</span> root root 4.0K Jun <span class="m">15</span> 15:19 registry
-rw<span class="p">&amp;</span>mdash<span class="p">;&amp;</span>mdash<span class="p">;</span>- <span class="m">1</span> root root 337M Jun <span class="m">16</span> 10:24 registry-image.tar</code></pre></div></p>

<h3>4. 部署yum仓库</h3>

<p>操作节点： <code>k8s-init</code></p>

<ul>
<li><strong>配置本地repo文件</strong>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>cat <span class="p">&amp;</span>lt<span class="p">;</span>&lt;EOF &gt; /etc/yum.repos.d/local.repo
<span class="o">[</span><span class="nb">local</span><span class="o">]</span>
<span class="nv">name</span><span class="o">=</span><span class="nb">local</span>
<span class="nv">baseurl</span><span class="o">=</span>file:///opt/k8s-installer/docker-ce
<span class="nv">gpgcheck</span><span class="o">=</span>0
<span class="nv">enabled</span><span class="o">=</span>1
EOF
<span class="nv">$ </span>yum clean all <span class="p">&amp;</span>amp<span class="p">;&amp;</span>amp<span class="p">;</span> yum makecache</code></pre></div></li>
<li><strong>安装并配置httpd服务</strong>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install -y  httpd <span class="p">&amp;</span>ndash<span class="p">;</span><span class="nv">disablerepo</span><span class="o">=</span>*  <span class="p">&amp;</span>ndash<span class="p">;</span><span class="nv">enablerepo</span><span class="o">=</span><span class="nb">local</span></code></pre></div>
httpd默认使用80端口，为避免端口冲突，默认修改为60081端口
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sed -i <span class="p">&amp;</span>rsquo<span class="p">;</span>s/Listen 80/Listen 60081/g<span class="err">&#39;</span> /etc/httpd/conf/httpd.conf</code></pre></div>
将安装包拷贝到服务目录中，服务目录默认使用<code>/var/www/html</code>，
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>cp -r /opt/k8s-installer/docker-ce/ /var/www/html/
<span class="nv">$ </span>systemctl <span class="nb">enable </span>httpd <span class="p">&amp;</span>amp<span class="p">;&amp;</span>amp<span class="p">;</span> systemctl start httpd</code></pre></div></li>
</ul>


<h3>5. 安装并配置docker</h3>

<p>操作节点： 所有节点（<code>k8s-init，k8s-master，k8s-slave</code>）均需执行
- <strong>配置yum repo</strong>
其中60081端口若有修改，需要替换为httpd实际使用的端口
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>cat <span class="p">&amp;</span>lt<span class="p">;</span>&lt;EOF &gt; /etc/yum.repos.d/local-http.repo
<span class="o">[</span><span class="nb">local</span>-http<span class="o">]</span>
<span class="nv">name</span><span class="o">=</span><span class="nb">local</span>-http
<span class="nv">baseurl</span><span class="o">=</span>&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;http://10.0.129.84:60081/docker-ce&quot;</span>&gt;http://10.0.129.84:60081/docker-ce&lt;/a&gt;
<span class="nv">gpgcheck</span><span class="o">=</span>0
<span class="nv">enabled</span><span class="o">=</span>1
EOF
<span class="nv">$ </span>yum clean all <span class="p">&amp;</span>amp<span class="p">;&amp;</span>amp<span class="p">;</span> yum makecache</code></pre></div>
- <strong>配置docker daemon文件</strong>
其中60080端口为镜像仓库的端口，如使用其他端口，需在下章节中替换为实际使用的端口
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>mkdir /etc/docker
<span class="nv">$ </span>cat <span class="p">&amp;</span>lt<span class="p">;</span>&lt;EOF &gt; /etc/docker/daemon.json
<span class="o">{</span>
    <span class="p">&amp;</span>ldquo<span class="p">;</span>insecure-registries<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="o">[</span>
        <span class="p">&amp;</span>ldquo<span class="p">;</span>10.0.129.84:60080<span class="p">&amp;</span>rdquo<span class="p">;</span>
    <span class="o">]</span>,
    <span class="p">&amp;</span>ldquo<span class="p">;</span>storage-driver<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>overlay2<span class="p">&amp;</span>rdquo<span class="p">;</span>
<span class="o">}</span>
EOF</code></pre></div>
- <strong>安装并启动docker</strong>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install -y docker-ce docker-ce-cli containerd.io  <span class="p">&amp;</span>ndash<span class="p">;</span><span class="nv">disablerepo</span><span class="o">=</span>*  <span class="p">&amp;</span>ndash<span class="p">;</span><span class="nv">enablerepo</span><span class="o">=</span><span class="nb">local</span>-http
<span class="nv">$ </span>systemctl <span class="nb">enable </span>docker <span class="p">&amp;</span>amp<span class="p">;&amp;</span>amp<span class="p">;</span> systemctl start docker</code></pre></div></p>

<h3>6. 配置镜像仓库</h3>

<blockquote><p>该仓库存储k8s部署所需的<code>kube-apiserver</code>、<code>kube-controller-manager</code>、<code>kube-scheduler</code>、<code>etcd</code>、<code>flannel</code>、<code>coredns</code>等组件的镜像，使用docker run的方式部署，默认暴漏机器的<code>60080</code>端口提供服务。</p></blockquote>

<p>操作节点： 只在<code>k8s-init</code>节点执行
- <strong>加载镜像到本地</strong>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>docker load -i /opt/k8s-installer/registry-image.tar
<span class="nv">$ </span>docker images <span class="c"># 查看加载成功的registry镜像</span>
REPOSITORY                               TAG                 IMAGE ID            CREATED             SIZE
index.alauda.cn/alaudaorg/distribution   latest              2aee66f2203d        <span class="m">2</span> years ago         347MB</code></pre></div>
- <strong>启动registry镜像仓库</strong>
默认使用<code>60080</code>作为registry对外的服务端口，如需修改，需将各节点的<code>/etc/docker/daemon.json</code>中的<code>insecure-registries</code>中配置的端口一并改掉
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>docker run -d <span class="p">&amp;</span>ndash<span class="p">;</span><span class="nv">restart</span><span class="o">=</span>always <span class="p">&amp;</span>ndash<span class="p">;</span>name pkg-registry -p 60080:5000 -v /opt/k8s-installer/registry/:/var/lib/registry index.alauda.cn/alaudaorg/distribution:latest</code></pre></div></p>

<h2>部署kubernetes</h2>

<h3>1. 安装 kubeadm, kubelet 和 kubectl</h3>

<p>操作节点： 所有的master和slave节点(<code>k8s-master,k8s-slave</code>) 需要执行
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install -y  kubeadm kubectl kubelet <span class="p">&amp;</span>ndash<span class="p">;</span><span class="nv">disablerepo</span><span class="o">=</span>*  <span class="p">&amp;</span>ndash<span class="p">;</span><span class="nv">enablerepo</span><span class="o">=</span><span class="nb">local</span>-http</code></pre></div></p>

<h3>2. 配置kubelet</h3>

<p>操作节点： 所有的master和slave节点(<code>k8s-master,k8s-slave</code>) 需要执行
- <strong>设置kubelet开机启动</strong>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>systemctl <span class="nb">enable </span>kubelet</code></pre></div>
- <strong>配置kubelet</strong>
配置文件<code>/etc/systemd/system/kubelet.service</code>，注意需要将&ndash;pod-infra-container-image地址设置为实际的镜像仓库地址（默认是k8s-init机器ip:60080）
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>cat <span class="p">&amp;</span>lt<span class="p">;</span>&lt;EOF &gt; /etc/systemd/system/kubelet.service
<span class="o">[</span>Unit<span class="o">]</span>
<span class="nv">Description</span><span class="o">=</span>kubelet: The Kubernetes Node Agent
<span class="nv">Documentation</span><span class="o">=</span>&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://kubernetes.io/docs/&quot;</span>&gt;https://kubernetes.io/docs/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;<span class="o">[</span>Service<span class="o">]</span>
<span class="nv">Environment</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span><span class="nv">KUBELET_SYSTEM_PODS_ARGS</span><span class="o">=</span><span class="p">&amp;</span>ndash<span class="p">;</span>pod-manifest-path<span class="o">=</span>/etc/kubernetes/manifests <span class="p">&amp;</span>ndash<span class="p">;</span>allow-privileged<span class="o">=</span><span class="nb">true</span><span class="p">&amp;</span>rdquo<span class="p">;</span>
<span class="nv">Environment</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span><span class="nv">KUBELET_INFRA_CONTAINER_IMAGE</span><span class="o">=</span><span class="p">&amp;</span>ndash<span class="p">;</span>pod-infra-container-image<span class="o">=</span>10.0.129.84:60080/k8s/pause:3.1<span class="p">&amp;</span>rdquo<span class="p">;</span>
<span class="nv">ExecStart</span><span class="o">=</span>/usr/bin/kubelet <span class="nv">$KUBELET_SYSTEM_PODS_ARGS</span> <span class="nv">$KUBELET_INFRA_CONTAINER_IMAGE</span>
<span class="nv">Restart</span><span class="o">=</span>always
<span class="nv">StartLimitInterval</span><span class="o">=</span>0
<span class="nv">RestartSec</span><span class="o">=</span>10&lt;/p&gt;

&lt;p&gt;<span class="o">[</span>Install<span class="o">]</span>
<span class="nv">WantedBy</span><span class="o">=</span>multi-user.target
EOF</code></pre></div></p>

<h3>3. 配置kubeadm初始化文件</h3>

<p>操作节点：只在master节点（<code>k8s-master</code>）执行
需要修改如下两处:
- <strong>advertiseAddress：修改为<code>k8s-master</code>的内网ip地址</strong>
-  <strong>imageRepository：修改为<code>k8s-init</code>的内网ip地址</strong>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>cat <span class="p">&amp;</span>lt<span class="p">;</span>&lt;EOF &gt; /opt/kubeadm.conf
apiVersion: kubeadm.k8s.io/v1beta1
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.129.84&lt;/p&gt;

&lt;h2&gt;  bindPort: 6443&lt;/h2&gt;

&lt;p&gt;apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: <span class="p">&amp;</span>ldquo<span class="p">;&amp;</span>rdquo<span class="p">;</span>
controllerManager: <span class="o">{}</span>
dns:
  <span class="nb">type</span>: CoreDNS
etcd:
  <span class="nb">local</span>:
    dataDir: /var/lib/etcd
imageRepository: 10.0.129.84:60080/k8s
kind: ClusterConfiguration
kubernetesVersion: v1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: <span class="p">&amp;</span>ldquo<span class="p">;</span>10.244.0.0/16<span class="p">&amp;</span>rdquo<span class="p">;</span>
  serviceSubnet: 10.96.0.0/12
scheduler: <span class="o">{}</span>
EOF</code></pre></div></p>

<h3>4. 提前下载镜像</h3>

<p>操作节点：只在master节点（<code>k8s-master</code>）执行
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># 查看需要使用的镜像列表,若无问题，将得到如下列表</span>
<span class="nv">$ </span>kubeadm config images list <span class="p">&amp;</span>ndash<span class="p">;</span>config /opt/kubeadm.conf
10.0.129.84:60080/k8s/kube-apiserver:v1.13.3
10.0.129.84:60080/k8s/kube-controller-manager:v1.13.3
10.0.129.84:60080/k8s/kube-scheduler:v1.13.3
10.0.129.84:60080/k8s/kube-proxy:v1.13.3
10.0.129.84:60080/k8s/pause:3.1
10.0.129.84:60080/k8s/etcd:3.2.24
10.0.129.84:60080/k8s/coredns:1.2.6
  <span class="c"># 提前下载镜像到本地</span>
<span class="nv">$ </span>kubeadm config images pull <span class="p">&amp;</span>ndash<span class="p">;</span>config /opt/kubeadm.conf</code></pre></div></p>

<h3>5. 初始化master节点</h3>

<p>操作节点：只在master节点（<code>k8s-master</code>）执行
<div class="highlight"><pre><code class="language-bash" data-lang="bash">kubeadm init <span class="p">&amp;</span>ndash<span class="p">;</span>config /opt/kubeadm.conf</code></pre></div>
若初始化成功后，最后会提示如下信息：
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="p">&amp;</span>hellip<span class="p">;</span>
Your Kubernetes master has initialized successfully!&lt;/p&gt;

&lt;p&gt;To start using your cluster, you need to run the following as a regular user:&lt;/p&gt;

&lt;p&gt;  mkdir -p <span class="nv">$HOME</span>/.kube
  sudo cp -i /etc/kubernetes/admin.conf <span class="nv">$HOME</span>/.kube/config
  sudo chown <span class="k">$(</span>id -u<span class="k">)</span>:<span class="k">$(</span>id -g<span class="k">)</span> <span class="nv">$HOME</span>/.kube/config&lt;/p&gt;

&lt;p&gt;You should now deploy a pod network to the cluster.
Run <span class="p">&amp;</span>ldquo<span class="p">;</span>kubectl apply -f <span class="o">[</span>podnetwork<span class="o">]</span>.yaml<span class="p">&amp;</span>rdquo<span class="p">;</span> with one of the options listed at:
  &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://kubernetes.io/docs/concepts/cluster-administration/addons/&quot;</span>&gt;https://kubernetes.io/docs/concepts/cluster-administration/addons/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can now join any number of machines by running the following on each node
as root:&lt;/p&gt;

&lt;p&gt;  kubeadm join 10.0.129.84:6443 <span class="p">&amp;</span>ndash<span class="p">;</span>token abcdef.0123456789abcdef <span class="p">&amp;</span>ndash<span class="p">;</span>discovery-token-ca-cert-hash sha256:6bb7e2646f1f846efddf2525c012505b76831ff9453329d0203d010814783a51</code></pre></div>
接下来按照上述提示信息操作，配置kubectl客户端的认证
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>mkdir -p <span class="nv">$HOME</span>/.kube
  <span class="nv">$ </span>sudo cp -i /etc/kubernetes/admin.conf <span class="nv">$HOME</span>/.kube/config
  <span class="nv">$ </span>sudo chown <span class="k">$(</span>id -u<span class="k">)</span>:<span class="k">$(</span>id -g<span class="k">)</span> <span class="nv">$HOME</span>/.kube/config</code></pre></div></p>

<blockquote><p>此时使用 kubectl get nodes查看节点应该处于notReady状态，因为还未配置网络插件</p>

<h3>6. 添加slave节点到集群中</h3>

<p>操作节点：所有的slave节点（<code>k8s-slave</code>）需要执行
在每台slave节点，执行如下命令，该命令是在kubeadm init成功后提示信息中打印出来的，需要替换成实际init后打印出的命令。
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>kubeadm join 10.0.129.84:6443 <span class="p">&amp;</span>ndash<span class="p">;</span>token abcdef.0123456789abcdef <span class="p">&amp;</span>ndash<span class="p">;</span>discovery-token-ca-cert-hash sha256:6bb7e2646f1f846efddf2525c012505b76831ff9453329d0203d010814783a51</code></pre></div></p>

<h3>7. 安装flannel插件</h3></blockquote>

<p>操作节点：只在master节点（<code>k8s-master</code>）执行
- <strong>拷贝kube-flannel.yml文件</strong>
把kube-flannel.yml拷贝到master节点的/opt目录
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>cp /opt/k8s-installer/kube-flannel.yml /opt</code></pre></div></p>

<blockquote><p><strong>⚠️注意：</strong>如果k8s-master和k8s-init节点不是同一台机器，需要把kube-flannel.yml从k8s-init节点远程拷贝到master节点的/opt目录
$ scp root@k8s-init:/opt/k8s-installer/kube-flannel.yml /opt</p></blockquote>

<ul>
<li><strong>替换flannel镜像地址</strong>
其中10.0.129.84:60080需要替换为实际的镜像仓库地址(k8s-init节点ip:60080)
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sed  -i <span class="p">&amp;</span>ldquo<span class="p">;</span>s#quay.io/coreos#10.0.129.84:60080/k8s#g<span class="p">&amp;</span>rdquo<span class="p">;</span> /opt/kube-flannel.yml</code></pre></div>
若配置kubeadm初始化文件章节中，podSubnet使用了非10.244.0.0/16的值，则需要对应的修改kube-flannel.yml文件中如下部分，保持一致即可，否则会造成flannel无法启动。
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="m">125</span>   net-conf.json: <span class="p">|</span>
<span class="m">126</span>     <span class="o">{</span>
<span class="m">127</span>       <span class="p">&amp;</span>ldquo<span class="p">;</span>Network<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>10.244.0.0/16<span class="p">&amp;</span>rdquo<span class="p">;</span>,
<span class="m">128</span>       <span class="p">&amp;</span>ldquo<span class="p">;</span>Backend<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="o">{</span>
<span class="m">129</span>         <span class="p">&amp;</span>ldquo<span class="p">;</span>Type<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>vxlan<span class="p">&amp;</span>rdquo<span class="p">;</span>
<span class="m">130</span>       <span class="o">}</span>
<span class="m">131</span>     <span class="o">}</span></code></pre></div></li>
<li><strong>创建flannel相关资源</strong>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>kubectl create -f /opt/kube-flannel.yml</code></pre></div>

<h3>8.  设置master节点是否可调度（可选）</h3>

操作节点：<code>k8s-master</code>
默认部署成功后，master节点无法调度业务pod，如需设置master节点也可以参与pod的调度，需执行：
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>kubectl taint node k8s-master node-role.kubernetes.io/master:NoSchedule-</code></pre></div>

<h3>9. 验证集群</h3>

<p>操作节点： 在master节点（<code>k8s-master</code>）执行
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>kubectl get nodes  <span class="c">#观察集群节点是否全部Ready</span>
NAME                STATUS   ROLES    AGE   VERSION
k8s-master   Ready    master   22h   v1.13.3
k8s-slave   Ready    &lt;none&gt;   22h   v1.13.3</code></pre></div>
创建测试nginx服务，需要将10.0.129.84替换为实际k8s-init节点的ip地址
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>kubectl run  <span class="nb">test</span>-nginx <span class="p">&amp;</span>ndash<span class="p">;</span><span class="nv">image</span><span class="o">=</span>10.0.129.84:60080/k8s/nginx</code></pre></div>
查看pod是否创建成功，并访问pod ip测试是否可用
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>kubectl get po -o wide <span class="p">|</span>grep <span class="nb">test</span>-nginx
<span class="nb">test</span>-nginx-7d65ddddc9-lcg9z       1/1     Running            <span class="m">0</span>          12s     10.244.1.3   k8s-slave    &lt;none&gt;           &lt;none&gt;
<span class="nv">$ </span>curl 10.244.1.3 <span class="c"># 验证是否服务可通</span>
<span class="p">&amp;</span>hellip<span class="p">;</span>&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;


&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;




&lt;p&gt;For online documentation and support please refer to
&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;http://nginx.org/&quot;</span>&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;http://nginx.com/&quot;</span>&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;




&lt;p&gt;&lt;em&gt;Thank you <span class="k">for</span> using nginx.&lt;/em&gt;&lt;/p&gt;


&lt;p&gt;<span class="p">&amp;</span>hellip<span class="p">;</span></code></pre></div></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[记录一次高可用集群使用kubeadm v1.9.6升级v1.13 的问题]]></title>
    <link href="http://liyongxin.github.io/blog/2019/04/22/ji-lu-yi-ci-kubeadm-v1-dot-12sheng-ji-v1-dot-13-de-wen-ti/"/>
    <updated>2019-04-22T14:12:52+08:00</updated>
    <id>http://liyongxin.github.io/blog/2019/04/22/ji-lu-yi-ci-kubeadm-v1-dot-12sheng-ji-v1-dot-13-de-wen-ti</id>
    <content type="html"><![CDATA[<p>项目中使用kubeadm 将k8s版本从v1.9.6升级到1.13.4,由于无法跨版本升级，所以大致流程是</p>

<ul>
<li>v1.9.6 -> v1.10.8</li>
<li>v1.10.8 -> v1.11.5</li>
<li>v1.11.5 -> v1.12.4</li>
<li>v1.12.4 -> v1.13.4</li>
</ul>


<p>其中操作集群A从<code>v1.9.6</code>到<code>v1.10.8</code>升级过程中，执行<code>upgrade</code>的时候出现了如下问题：
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@cloud-cn-master-1 ~<span class="o">]</span><span class="c"># kubeadm upgrade apply v1.10.8 -y</span>
<span class="p">&amp;</span>hellip<span class="p">;</span>
<span class="o">[</span>upgrade/prepull<span class="o">]</span> Successfully prepulled the images <span class="k">for</span> all the control plane components
<span class="o">[</span>upgrade/apply<span class="o">]</span> Upgrading your Static Pod-hosted control plane to version <span class="p">&amp;</span>ldquo<span class="p">;</span>v1.10.8<span class="p">&amp;</span>rdquo<span class="p">;&amp;</span>hellip<span class="p">;</span>
Static pod: kube-apiserver-rz-dev-master01 <span class="nb">hash</span>: a82830fd687fdabd030b65ee6c4b4fd4
Static pod: kube-controller-manager-rz-dev-master01 <span class="nb">hash</span>: 1a23c184fadb64c889a41831476c56e8
Static pod: kube-scheduler-rz-dev-master01 <span class="nb">hash</span>: a0adc2bf23e7d5336ecd4677ce95938c
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Writing new Static Pod manifests to <span class="p">&amp;</span>ldquo<span class="p">;</span>/etc/kubernetes/tmp/kubeadm-upgraded-manifests500670946<span class="p">&amp;</span>rdquo<span class="p">;</span>
<span class="o">[</span>controlplane<span class="o">]</span> Adding extra host path mount <span class="p">&amp;</span>ldquo<span class="p">;</span>k8s<span class="p">&amp;</span>rdquo<span class="p">;</span> to <span class="p">&amp;</span>ldquo<span class="p">;</span>kube-controller-manager<span class="p">&amp;</span>rdquo<span class="p">;</span>
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> current and new manifests of kube-apiserver are equal, skipping upgrade
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Moved new manifest to <span class="p">&amp;</span>ldquo<span class="p">;</span>/etc/kubernetes/manifests/kube-controller-manager.yaml<span class="p">&amp;</span>rdquo<span class="p">;</span> and backed up old manifest to <span class="p">&amp;</span>ldquo<span class="p">;</span>/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-04-22-13-04-58/kube-controller-manager.yaml<span class="p">&amp;</span>rdquo<span class="p">;</span>
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> Waiting <span class="k">for</span> the kubelet to restart the component
<span class="o">[</span>upgrade/staticpods<span class="o">]</span> This might take a minute or longer depending on the component/version gap <span class="o">(</span>timeout 5m0s<span class="o">)</span>
Static pod: kube-controller-manager-rz-dev-master01 <span class="nb">hash</span>: 1a23c184fadb64c889a41831476c56e8
Static pod: kube-controller-manager-rz-dev-master01 <span class="nb">hash</span>: 1a23c184fadb64c889a41831476c56e8
Static pod: kube-controller-manager-rz-dev-master01 <span class="nb">hash</span>: 1a23c184fadb64c889a41831476c56e8
Static pod: kube-controller-manager-rz-dev-master01 <span class="nb">hash</span>: 1a23c184fadb64c889a41831476c56e8
Static pod: kube-controller-manager-rz-dev-master01 <span class="nb">hash</span>: 1a23c184fadb64c889a41831476c56e8
<span class="p">&amp;</span>hellip<span class="p">;</span></code></pre></div></p>

<!--more-->


<p>升级过程中一直卡在校验<code>hash</code>这步，由于之前测试没有出现类似情况，而此次出问题和测试环境唯一的区别就是该环境升级过证书，
默认证书有效期是1年，此环境更新成30年了，所以尝试先把证书还原后再次执行<code>upgrade</code>，升级成功，具体原因未排查。</p>

<p>在另一个环境从<code>v1.12</code>升级到<code>v1.13.4</code>版本时候，出现了如下情况：
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@cloud-cn-master-1 ~<span class="o">]</span><span class="c"># kubeadm upgrade apply v1.13.4 -y</span>
<span class="o">[</span>preflight<span class="o">]</span> Running pre-flight checks.
<span class="o">[</span>upgrade<span class="o">]</span> Making sure the cluster is healthy:
<span class="o">[</span>upgrade/config<span class="o">]</span> Making sure the configuration is correct:
<span class="o">[</span>upgrade/config<span class="o">]</span> Reading configuration from the cluster<span class="p">&amp;</span>hellip<span class="p">;</span>
<span class="o">[</span>upgrade/config<span class="o">]</span> FYI: You can look at this config file with <span class="p">&amp;</span>lsquo<span class="p">;</span>kubectl -n kube-system get cm kubeadm-config -oyaml<span class="p">&amp;</span>rsquo<span class="p">;</span>
FATAL: failed to get node registration: node doesn<span class="p">&amp;</span>rsquo<span class="p">;</span>t have kubeadm.alpha.kubernetes.io/cri-socket annotation</code></pre></div></p>

<p>报错提示<code>node</code>缺少<code>annotation</code>，而查看操作<code>kubeadm</code>升级的<code>master node</code>，是存在<code>cri-socket</code>的<code>annotation</code>的，但是其他两台<code>master</code>
不存在，于是尝试手动添加<code>annotation</code>，然后再次跑<code>upgrade</code>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@cloud-cn-master-1 ~<span class="o">]</span><span class="c"># kubectl annotate node &lt;nodename&gt; kubeadm.alpha.kubernetes.io/cri-socket=/var/run/dockershim.sock</span>
<span class="o">[</span>root@cloud-cn-master-1 ~<span class="o">]</span><span class="c"># kubeadm upgrade apply v1.13.4 -y</span>
<span class="o">[</span>preflight<span class="o">]</span> Running pre-flight checks.
<span class="o">[</span>upgrade<span class="o">]</span> Making sure the cluster is healthy:
<span class="o">[</span>upgrade/config<span class="o">]</span> Making sure the configuration is correct:
<span class="o">[</span>upgrade/config<span class="o">]</span> Reading configuration from the cluster<span class="p">&amp;</span>hellip<span class="p">;</span>
<span class="o">[</span>upgrade/config<span class="o">]</span> FYI: You can look at this config file with <span class="p">&amp;</span>lsquo<span class="p">;</span>kubectl -n kube-system get cm kubeadm-config -oyaml<span class="p">&amp;</span>rsquo<span class="p">;</span>
<span class="o">[</span>upgrade/config<span class="o">]</span> FATAL: failed to getAPIEndpoint: failed to get APIEndpoint information <span class="k">for</span> this node</code></pre></div>
又报错找不到<code>APIEndpoint</code>，尝试执行看下<code>kubeadm-config</code>的数据
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@cloud-cn-master-1 ~<span class="o">]</span><span class="c"># kubectl -n kube-system get cm kubeadm-config -oyaml|grep -A5 cloud-cn-master-1</span>
  ClusterStatus: <span class="p">|</span>
    apiEndpoints:
      cloud-cn-master-1:
        advertiseAddress: 10.0.128.251
        bindPort: 6443
    apiVersion: kubeadm.k8s.io/v1beta1
    kind: ClusterStatus</code></pre></div></p>

<p>发现该<code>apiEndpoints</code>是存在的，但是只存在这一个节点，于是尝试<code>edit cm</code>把另外两个节点的<code>apiEndpoint</code>也配置上，再次<code>upgrade</code>，
发现又出现了校验<code>hash</code>不通过的错误，于是尝试去看下<code>kubeadm</code>的源码，理清楚这个<code>config</code>阶段的思路，
源码可以参考<a href="https://github.com/kubernetes/kubernetes/blob/master/cmd/kubeadm/app/util/config/cluster.go">此处</a>,
<div class="highlight"><pre><code class="language-go" data-lang="go"><span class="c1">// if this isn&amp;rsquo;t a new controlplane instance (e.g. in case of kubeadm upgrades)</span>
<span class="c1">// get nodes specific information as well</span>
<span class="k">if</span> <span class="p">!</span><span class="nx">newControlPlane</span> <span class="p">{</span>
    <span class="c1">// gets the nodeRegistration for the current from the node object</span>
    <span class="k">if</span> <span class="nx">err</span> <span class="o">:=</span> <span class="nx">getNodeRegistration</span><span class="p">(</span><span class="nx">kubeconfigDir</span><span class="p">,</span> <span class="nx">client</span><span class="p">,</span> <span class="o">&amp;</span><span class="nx">amp</span><span class="p">;</span><span class="nx">initcfg</span><span class="p">.</span><span class="nx">NodeRegistration</span><span class="p">);</span> <span class="nx">err</span> <span class="o">!=</span> <span class="kc">nil</span> <span class="p">{</span>
        <span class="k">return</span> <span class="kc">nil</span><span class="p">,</span> <span class="nx">errors</span><span class="p">.</span><span class="nx">Wrap</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="o">&amp;</span><span class="nx">ldquo</span><span class="p">;</span><span class="nx">failed</span> <span class="nx">to</span> <span class="nx">get</span> <span class="nx">node</span> <span class="nx">registration</span><span class="o">&amp;</span><span class="nx">rdquo</span><span class="p">;)</span>
    <span class="p">}</span>
    <span class="c1">// gets the APIEndpoint for the current node from then ClusterStatus in the kubeadm-config ConfigMap</span>
    <span class="k">if</span> <span class="nx">err</span> <span class="o">:=</span> <span class="nx">getAPIEndpoint</span><span class="p">(</span><span class="nx">configMap</span><span class="p">.</span><span class="nx">Data</span><span class="p">,</span> <span class="nx">initcfg</span><span class="p">.</span><span class="nx">NodeRegistration</span><span class="p">.</span><span class="nx">Name</span><span class="p">,</span> <span class="o">&amp;</span><span class="nx">amp</span><span class="p">;</span><span class="nx">initcfg</span><span class="p">.</span><span class="nx">LocalAPIEndpoint</span><span class="p">);</span> <span class="nx">err</span> <span class="o">!=</span> <span class="kc">nil</span> <span class="p">{</span>
        <span class="k">return</span> <span class="kc">nil</span><span class="p">,</span> <span class="nx">errors</span><span class="p">.</span><span class="nx">Wrap</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="o">&amp;</span><span class="nx">ldquo</span><span class="p">;</span><span class="nx">failed</span> <span class="nx">to</span> <span class="nx">getAPIEndpoint</span><span class="o">&amp;</span><span class="nx">rdquo</span><span class="p">;)</span>
    <span class="p">}</span>
<span class="p">}</span></code></pre></div></p>

<p>该部分即对应执行<code>upgrade</code>的逻辑，先去<code>getNodeRegistration</code>然后去<code>getAPIEndpoint</code>，看下getAPIEndpoint的逻辑
<div class="highlight"><pre><code class="language-go" data-lang="go"><span class="c1">// getAPIEndpoint returns the APIEndpoint for the current node</span>
<span class="kd">func</span> <span class="nx">getAPIEndpoint</span><span class="p">(</span><span class="nx">data</span> <span class="kd">map</span><span class="p">[</span><span class="kt">string</span><span class="p">]</span><span class="kt">string</span><span class="p">,</span> <span class="nx">nodeName</span> <span class="kt">string</span><span class="p">,</span> <span class="nx">apiEndpoint</span> <span class="o">*</span><span class="nx">kubeadmapi</span><span class="p">.</span><span class="nx">APIEndpoint</span><span class="p">)</span> <span class="kt">error</span> <span class="p">{</span>
    <span class="c1">// gets the ClusterStatus from kubeadm-config</span>
    <span class="nx">clusterStatus</span><span class="p">,</span> <span class="nx">err</span> <span class="o">:=</span> <span class="nx">UnmarshalClusterStatus</span><span class="p">(</span><span class="nx">data</span><span class="p">)</span>
    <span class="k">if</span> <span class="nx">err</span> <span class="o">!=</span> <span class="kc">nil</span> <span class="p">{</span>
        <span class="k">return</span> <span class="nx">err</span>
    <span class="p">}&lt;</span><span class="o">/</span><span class="nx">p</span><span class="p">&gt;</span>

<span class="p">&lt;</span><span class="nx">pre</span><span class="p">&gt;&lt;</span><span class="nx">code</span><span class="p">&gt;</span><span class="c1">// gets the APIEndpoint for the current machine from the ClusterStatus</span>
<span class="nx">e</span><span class="p">,</span> <span class="nx">ok</span> <span class="o">:=</span> <span class="nx">clusterStatus</span><span class="p">.</span><span class="nx">APIEndpoints</span><span class="p">[</span><span class="nx">nodeName</span><span class="p">]</span>
<span class="k">if</span> <span class="p">!</span><span class="nx">ok</span> <span class="p">{</span>
    <span class="k">return</span> <span class="nx">errors</span><span class="p">.</span><span class="nx">New</span><span class="p">(</span><span class="s">&quot;failed to get APIEndpoint information for this node&quot;</span><span class="p">)</span>
<span class="p">}</span>

<span class="nx">apiEndpoint</span><span class="p">.</span><span class="nx">AdvertiseAddress</span> <span class="p">=</span> <span class="nx">e</span><span class="p">.</span><span class="nx">AdvertiseAddress</span>
<span class="nx">apiEndpoint</span><span class="p">.</span><span class="nx">BindPort</span> <span class="p">=</span> <span class="nx">e</span><span class="p">.</span><span class="nx">BindPort</span>
<span class="k">return</span> <span class="kc">nil</span>
<span class="p">&lt;</span><span class="o">/</span><span class="nx">code</span><span class="p">&gt;&lt;</span><span class="o">/</span><span class="nx">pre</span><span class="p">&gt;</span>

<span class="p">&lt;</span><span class="nx">p</span><span class="p">&gt;}</span></code></pre></div>
会根据<code>nodeName</code>和<code>kubeadm-config</code>这个<code>configmap</code>的数据去拿<code>APIEndpoint</code>的<code>AdvertiseAddress</code>和<code>BindPort</code>信息，但是手动确认过确实是存在
<code>APIEndpoint</code>的配置的，所以再次查看传过来的<code>nodeName</code>是否正确，由于<code>nodeName</code>是从<code>NodeRegistration</code>中获取的先看下<code>NodeRegistration</code>的获取逻辑:</p>

<p><div class="highlight"><pre><code class="language-go" data-lang="go"><span class="c1">// getNodeRegistration returns the nodeRegistration for the current node</span>
<span class="kd">func</span> <span class="nx">getNodeRegistration</span><span class="p">(</span><span class="nx">kubeconfigDir</span> <span class="kt">string</span><span class="p">,</span> <span class="nx">client</span> <span class="nx">clientset</span><span class="p">.</span><span class="nx">Interface</span><span class="p">,</span> <span class="nx">nodeRegistration</span> <span class="o">*</span><span class="nx">kubeadmapi</span><span class="p">.</span><span class="nx">NodeRegistrationOptions</span><span class="p">)</span> <span class="kt">error</span> <span class="p">{</span>
    <span class="c1">// gets the name of the current node</span>
    <span class="nx">nodeName</span><span class="p">,</span> <span class="nx">err</span> <span class="o">:=</span> <span class="nx">getNodeNameFromKubeletConfig</span><span class="p">(</span><span class="nx">kubeconfigDir</span><span class="p">)</span>
    <span class="k">if</span> <span class="nx">err</span> <span class="o">!=</span> <span class="kc">nil</span> <span class="p">{</span>
        <span class="k">return</span> <span class="nx">errors</span><span class="p">.</span><span class="nx">Wrap</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="o">&amp;</span><span class="nx">ldquo</span><span class="p">;</span><span class="nx">failed</span> <span class="nx">to</span> <span class="nx">get</span> <span class="nx">node</span> <span class="nx">name</span> <span class="nx">from</span> <span class="nx">kubelet</span> <span class="nx">config</span><span class="o">&amp;</span><span class="nx">rdquo</span><span class="p">;)</span>
    <span class="p">}&lt;</span><span class="o">/</span><span class="nx">p</span><span class="p">&gt;</span>

<span class="p">&lt;</span><span class="nx">pre</span><span class="p">&gt;&lt;</span><span class="nx">code</span><span class="p">&gt;</span><span class="c1">// gets the corresponding node and retrieves attributes stored there.</span>
<span class="nx">node</span><span class="p">,</span> <span class="nx">err</span> <span class="o">:=</span> <span class="nx">client</span><span class="p">.</span><span class="nx">CoreV1</span><span class="p">().</span><span class="nx">Nodes</span><span class="p">().</span><span class="nx">Get</span><span class="p">(</span><span class="nx">nodeName</span><span class="p">,</span> <span class="nx">metav1</span><span class="p">.</span><span class="nx">GetOptions</span><span class="p">{})</span>
<span class="k">if</span> <span class="nx">err</span> <span class="o">!=</span> <span class="kc">nil</span> <span class="p">{</span>
    <span class="k">return</span> <span class="nx">errors</span><span class="p">.</span><span class="nx">Wrap</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="s">&quot;failed to get corresponding node&quot;</span><span class="p">)</span>
<span class="p">}</span>

<span class="nx">criSocket</span><span class="p">,</span> <span class="nx">ok</span> <span class="o">:=</span> <span class="nx">node</span><span class="p">.</span><span class="nx">ObjectMeta</span><span class="p">.</span><span class="nx">Annotations</span><span class="p">[</span><span class="nx">constants</span><span class="p">.</span><span class="nx">AnnotationKubeadmCRISocket</span><span class="p">]</span>
<span class="k">if</span> <span class="p">!</span><span class="nx">ok</span> <span class="p">{</span>
    <span class="k">return</span> <span class="nx">errors</span><span class="p">.</span><span class="nx">Errorf</span><span class="p">(</span><span class="s">&quot;node %s doesn&#39;t have %s annotation&quot;</span><span class="p">,</span> <span class="nx">nodeName</span><span class="p">,</span> <span class="nx">constants</span><span class="p">.</span><span class="nx">AnnotationKubeadmCRISocket</span><span class="p">)</span>
<span class="p">}</span>

<span class="c1">// returns the nodeRegistration attributes</span>
<span class="nx">nodeRegistration</span><span class="p">.</span><span class="nx">Name</span> <span class="p">=</span> <span class="nx">nodeName</span>
<span class="nx">nodeRegistration</span><span class="p">.</span><span class="nx">CRISocket</span> <span class="p">=</span> <span class="nx">criSocket</span>
<span class="nx">nodeRegistration</span><span class="p">.</span><span class="nx">Taints</span> <span class="p">=</span> <span class="nx">node</span><span class="p">.</span><span class="nx">Spec</span><span class="p">.</span><span class="nx">Taints</span>
<span class="c1">// NB. currently nodeRegistration.KubeletExtraArgs isn&#39;t stored at node level but only in the kubeadm-flags.env</span>
<span class="c1">//     that isn&#39;t modified during upgrades</span>
<span class="c1">//     in future we might reconsider this thus enabling changes to the kubeadm-flags.env during upgrades as well</span>
<span class="k">return</span> <span class="kc">nil</span>
<span class="p">&lt;</span><span class="o">/</span><span class="nx">code</span><span class="p">&gt;&lt;</span><span class="o">/</span><span class="nx">pre</span><span class="p">&gt;</span>

<span class="p">&lt;</span><span class="nx">p</span><span class="p">&gt;}</span></code></pre></div>
发现<code>nodeName</code>是通过<code>getNodeNameFromKubeletConfig</code>获取的，也就是说读取的是<code>kubelet.conf</code>配置，看下<code>getNodeNameFromKubeletConfig</code>逻辑：
<div class="highlight"><pre><code class="language-go" data-lang="go"><span class="c1">// getNodeNameFromConfig gets the node name from a kubelet config file</span>
<span class="c1">// TODO: in future we want to switch to a more canonical way for doing this e.g. by having this</span>
<span class="c1">//       information in the local kubelet config.yaml</span>
<span class="kd">func</span> <span class="nx">getNodeNameFromKubeletConfig</span><span class="p">(</span><span class="nx">kubeconfigDir</span> <span class="kt">string</span><span class="p">)</span> <span class="p">(</span><span class="kt">string</span><span class="p">,</span> <span class="kt">error</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// loads the kubelet.conf file</span>
    <span class="nx">fileName</span> <span class="o">:=</span> <span class="nx">filepath</span><span class="p">.</span><span class="nx">Join</span><span class="p">(</span><span class="nx">kubeconfigDir</span><span class="p">,</span> <span class="nx">constants</span><span class="p">.</span><span class="nx">KubeletKubeConfigFileName</span><span class="p">)</span>
    <span class="nx">config</span><span class="p">,</span> <span class="nx">err</span> <span class="o">:=</span> <span class="nx">clientcmd</span><span class="p">.</span><span class="nx">LoadFromFile</span><span class="p">(</span><span class="nx">fileName</span><span class="p">)</span>
    <span class="k">if</span> <span class="nx">err</span> <span class="o">!=</span> <span class="kc">nil</span> <span class="p">{</span>
        <span class="k">return</span> <span class="o">&amp;</span><span class="nx">ldquo</span><span class="p">;</span><span class="o">&amp;</span><span class="nx">rdquo</span><span class="p">;,</span> <span class="nx">err</span>
    <span class="p">}&lt;</span><span class="o">/</span><span class="nx">p</span><span class="p">&gt;</span>

<span class="p">&lt;</span><span class="nx">pre</span><span class="p">&gt;&lt;</span><span class="nx">code</span><span class="p">&gt;</span><span class="c1">// gets the info about the current user</span>
<span class="nx">authInfo</span> <span class="o">:=</span> <span class="nx">config</span><span class="p">.</span><span class="nx">AuthInfos</span><span class="p">[</span><span class="nx">config</span><span class="p">.</span><span class="nx">Contexts</span><span class="p">[</span><span class="nx">config</span><span class="p">.</span><span class="nx">CurrentContext</span><span class="p">].</span><span class="nx">AuthInfo</span><span class="p">]</span>

<span class="c1">// gets the X509 certificate with current user credentials</span>
<span class="kd">var</span> <span class="nx">certs</span> <span class="p">[]</span><span class="o">*</span><span class="nx">x509</span><span class="p">.</span><span class="nx">Certificate</span>
<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nx">authInfo</span><span class="p">.</span><span class="nx">ClientCertificateData</span><span class="p">)</span> <span class="o">&amp;</span><span class="nx">gt</span><span class="p">;</span> <span class="mi">0</span> <span class="p">{</span>
    <span class="c1">// if the config file uses an embedded x509 certificate (e.g. kubelet.conf created by kubeadm), parse it</span>
    <span class="k">if</span> <span class="nx">certs</span><span class="p">,</span> <span class="nx">err</span> <span class="p">=</span> <span class="nx">certutil</span><span class="p">.</span><span class="nx">ParseCertsPEM</span><span class="p">(</span><span class="nx">authInfo</span><span class="p">.</span><span class="nx">ClientCertificateData</span><span class="p">);</span> <span class="nx">err</span> <span class="o">!=</span> <span class="kc">nil</span> <span class="p">{</span>
        <span class="k">return</span> <span class="s">&quot;&quot;</span><span class="p">,</span> <span class="nx">err</span>
    <span class="p">}</span>
<span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nx">authInfo</span><span class="p">.</span><span class="nx">ClientCertificate</span><span class="p">)</span> <span class="o">&amp;</span><span class="nx">gt</span><span class="p">;</span> <span class="mi">0</span> <span class="p">{</span>
    <span class="c1">// if the config file links an external x509 certificate (e.g. kubelet.conf created by TLS bootstrap), load it</span>
    <span class="k">if</span> <span class="nx">certs</span><span class="p">,</span> <span class="nx">err</span> <span class="p">=</span> <span class="nx">certutil</span><span class="p">.</span><span class="nx">CertsFromFile</span><span class="p">(</span><span class="nx">authInfo</span><span class="p">.</span><span class="nx">ClientCertificate</span><span class="p">);</span> <span class="nx">err</span> <span class="o">!=</span> <span class="kc">nil</span> <span class="p">{</span>
        <span class="k">return</span> <span class="s">&quot;&quot;</span><span class="p">,</span> <span class="nx">err</span>
    <span class="p">}</span>
<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="k">return</span> <span class="s">&quot;&quot;</span><span class="p">,</span> <span class="nx">errors</span><span class="p">.</span><span class="nx">New</span><span class="p">(</span><span class="s">&quot;invalid kubelet.conf. X509 certificate expected&quot;</span><span class="p">)</span>
<span class="p">}</span>

<span class="c1">// We are only putting one certificate in the certificate pem file, so it&#39;s safe to just pick the first one</span>
<span class="c1">// TODO: Support multiple certs here in order to be able to rotate certs</span>
<span class="nx">cert</span> <span class="o">:=</span> <span class="nx">certs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1">// gets the node name from the certificate common name</span>
<span class="k">return</span> <span class="nx">strings</span><span class="p">.</span><span class="nx">TrimPrefix</span><span class="p">(</span><span class="nx">cert</span><span class="p">.</span><span class="nx">Subject</span><span class="p">.</span><span class="nx">CommonName</span><span class="p">,</span> <span class="nx">constants</span><span class="p">.</span><span class="nx">NodesUserPrefix</span><span class="p">),</span> <span class="kc">nil</span>
<span class="p">&lt;</span><span class="o">/</span><span class="nx">code</span><span class="p">&gt;&lt;</span><span class="o">/</span><span class="nx">pre</span><span class="p">&gt;</span>

<span class="p">&lt;</span><span class="nx">p</span><span class="p">&gt;}</span></code></pre></div>
发现kubeadm先加载本机的<code>kubelet.conf</code>文件，然后尝试去找当前context中配置的用户的client-certificate-data数据，然后解析cert
的信息，找到subject的CommanName,来当作NodeName然后去kubeadm-config中找对应的apiEndpoint,所以尝试解析下当前的证书数据，看下CN的值：
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>

<span class="o">&lt;</span><span class="n">h1</span><span class="o">&gt;</span><span class="err">先获取</span><span class="n">client</span><span class="o">-</span><span class="n">certificate</span><span class="o">-</span><span class="n">data</span><span class="err">并做</span><span class="n">base64</span><span class="err">解密得到</span><span class="n">cert</span><span class="err">信息</span><span class="p">,</span><span class="err">$</span><span class="n">client</span><span class="o">-</span><span class="n">certificate</span><span class="o">-</span><span class="n">data</span><span class="err">为</span><span class="n">kubelet</span><span class="o">.</span><span class="n">conf</span><span class="err">中</span><span class="n">client</span><span class="o">-</span><span class="n">certificate</span><span class="o">-</span><span class="n">data</span><span class="err">对应的内容</span><span class="o">&lt;/</span><span class="n">h1</span><span class="o">&gt;</span>

<span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="p">[</span><span class="n">root</span><span class="nd">@cloud</span><span class="o">-</span><span class="n">cn</span><span class="o">-</span><span class="n">master</span><span class="o">-</span><span class="mi">1</span> <span class="o">~</span><span class="p">]</span><span class="c"># echo $client-certificate-data |base64 -d &gt; kubelet.crt</span>
<span class="p">[</span><span class="n">root</span><span class="nd">@cloud</span><span class="o">-</span><span class="n">cn</span><span class="o">-</span><span class="n">master</span><span class="o">-</span><span class="mi">1</span> <span class="o">~</span><span class="p">]</span><span class="c"># openssl x509 -in kubelet.crt -text |grep -i Subject</span>
        <span class="n">Subject</span><span class="p">:</span> <span class="n">O</span><span class="o">=</span><span class="n">system</span><span class="p">:</span><span class="n">nodes</span><span class="p">,</span> <span class="n">CN</span><span class="o">=</span><span class="n">system</span><span class="p">:</span><span class="n">node</span><span class="p">:</span><span class="n">cloud</span><span class="o">-</span><span class="n">cn</span><span class="o">-</span><span class="n">master</span><span class="o">-</span><span class="mi">2</span>
        <span class="n">Subject</span> <span class="n">Public</span> <span class="n">Key</span> <span class="n">Info</span><span class="p">:</span></code></pre></div>
👆，好吧原因定位到了，是因为获取到的<code>CommanName</code>与本机不匹配，于是在<code>cloud-cn-master-1</code>上获取到的是<code>cloud-cn-master-2</code>的<code>nodeName</code>，
然后去获取<code>annotation</code>自然也就拿不到了，然后即使手动<code>annotate</code>了，也是临时过了一步，到<code>configmap</code>中取<code>apiEndpoint</code>自然也获取不到，
哪怕再手动维护<code>apiEndpoint</code>，那么后续到校验hash也是无法通过，所以根本原因在于升级证书的时候没有<code>kubelet</code>证书被覆盖了，导致一系列的问题</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kubernetes的service cluster ip]]></title>
    <link href="http://liyongxin.github.io/blog/2019/04/03/kubernetesde-service-cluster-ip/"/>
    <updated>2019-04-03T17:17:23+08:00</updated>
    <id>http://liyongxin.github.io/blog/2019/04/03/kubernetesde-service-cluster-ip</id>
    <content type="html"><![CDATA[<p>k8s的pod可以有多个副本，但是在访问pod时，会有几个问题：</p>

<ul>
<li>客户端需要知道各个pod的地址</li>
<li>某一node上的pod如果故障，客户端需要感知</li>
</ul>


<p>为了解决这个问题，k8s引入了service的概念，用以指导客户端的流量。</p>

<h2>Service</h2>

<p>以下面的my-nginx为例。</p>

<p>pod和service的定义文件如下：
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@localhost k8s<span class="o">]</span><span class="c"># cat run-my-nginx.yaml</span>
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: my-nginx
spec:
  replicas: 2
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: nginx
        ports:
        - containerPort: 80
<span class="o">[</span>root@localhost k8s<span class="o">]</span><span class="c"># cat run-my-nginx-service.yaml</span>
apiVersion: v1
kind: Service
metadata:
  name: my-nginx
  labels:
    run: my-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
  selector:
    run: my-nginx</code></pre></div></p>

<p>pod my-nginx定义的replicas为2即2个副本，端口号为80;service my-nginx定义的selector为run:my-nginx，即该service选中所有label为run: my-nginx的pod；定义的port为80。</p>

<!--more-->


<p>使用kubectl create -f xx.yml创建后，可以在集群上看到2个pod，地址分别为10.244.1.10/10.244.2.10；可以看到1个service，IP/Port为10.11.97.177/80，其对接的Endpoints为10.244.1.10:80,10.244.2.10:80，即2个pod的服务地址，这三个URL在集群内任一节点都可以使用curl访问。
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@localhost k8s<span class="o">]</span><span class="c"># kubectl get pods -n default -o wide</span>
NAME                       READY     STATUS    RESTARTS   AGE       IP            NODE
my-nginx-379829228-3n755   1/1       Running   <span class="m">0</span>          21h       10.244.1.10   note2
my-nginx-379829228-xh214   1/1       Running   <span class="m">0</span>          21h       10.244.2.10   node1
<span class="o">[</span>root@localhost ~<span class="o">]</span><span class="c">#</span>
<span class="o">[</span>root@localhost ~<span class="o">]</span><span class="c"># kubectl describe svc my-nginx</span>
Name:                   my-nginx
Namespace:              default
Labels:                 <span class="nv">run</span><span class="o">=</span>my-nginx
Selector:               <span class="nv">run</span><span class="o">=</span>my-nginx
Type:                   ClusterIP
IP:                     10.11.97.177
Port:                   &lt;<span class="nb">unset</span>&gt; 80/TCP
Endpoints:              10.244.1.10:80,10.244.2.10:80
Session Affinity:       None</code></pre></div></p>

<p>但是，如果你去查看集群各节点的IP信息，是找不到10.11.97.177这个IP的，那么curl是如何通过这个(Virtual)IP地址访问到后端的Endpoints呢？
答案在<a href="https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies">这里</a>。</p>

<h2>kube-proxy</h2>

<p>k8s支持2种proxy模式，userspace和iptables。从v1.2版本开始，默认采用iptables proxy。那么这两种模式有什么不同吗？</p>

<p>1、userspace</p>

<p>顾名思义，userspace即用户空间。为什么这么叫呢？看下面的图。</p>

<p><img src="/images/custom/services-userspace-overview.svg" alt="" /></p>

<p>kube-proxy会为每个service随机监听一个端口(proxy port)，并增加一条iptables规则：所以到clusterIP:Port 的报文都redirect到proxy port；kube-proxy从它监听的proxy port收到报文后，走round robin（默认）或者session affinity（会话亲和力，即同一client IP都走同一链路给同一pod服务），分发给对应的pod。</p>

<p>显然userspace会造成所有报文都走一遍用户态，性能不高，现在k8s已经不再使用了。</p>

<p>2、iptables</p>

<p>我们回过头来看看userspace，既然用户态会增加性能损耗，那么有没有办法不走呢？实际上用户态也只是一个报文LB，通过iptables完全可以搞定。k8s下面这张图很清晰的说明了iptables方式与userspace方式的不同：kube-proxy只是作为controller，而不是server，真正服务的是内核的netfilter，体现在用户态则是iptables。</p>

<p>kube-proxy的iptables方式也支持round robin(默认)和session affinity。</p>

<p><img src="/images/custom/services-iptables-overview.svg" alt="" /></p>

<p>那么iptables是怎么做到LB，而且还能round-robin呢？我们通过iptables-save来看my-nginx这个服务在某一个node上的iptables规则。
<div class="highlight"><pre><code class="language-bash" data-lang="bash">-A KUBE-SERVICES -d 10.11.97.177/32 -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx: cluster IP<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp <span class="p">&amp;</span>ndash<span class="p">;</span>dport <span class="m">80</span> -j KUBE-SVC-BEPXDJBUHFCSYIC3&lt;/p&gt;

&lt;p&gt;-A KUBE-SVC-BEPXDJBUHFCSYIC3 -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx:<span class="p">&amp;</span>rdquo<span class="p">;</span> -m statistic <span class="p">&amp;</span>ndash<span class="p">;</span>mode random <span class="p">&amp;</span>ndash<span class="p">;</span>probability 0.50000000000 -j KUBE-SEP-U4UWLP4OR3LOJBXU
-A KUBE-SVC-BEPXDJBUHFCSYIC3 -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx:<span class="p">&amp;</span>rdquo<span class="p">;</span> -j KUBE-SEP-QHRWSLKOO5YUPI7O&lt;/p&gt;

&lt;p&gt;-A KUBE-SEP-U4UWLP4OR3LOJBXU -s 10.244.1.10/32 -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx:<span class="p">&amp;</span>rdquo<span class="p">;</span> -j KUBE-MARK-MASQ
-A KUBE-SEP-U4UWLP4OR3LOJBXU -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx:<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp -j DNAT <span class="p">&amp;</span>ndash<span class="p">;</span>to-destination 10.244.1.10:80&lt;/p&gt;

&lt;p&gt;-A KUBE-SEP-QHRWSLKOO5YUPI7O -s 10.244.2.10/32 -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx:<span class="p">&amp;</span>rdquo<span class="p">;</span> -j KUBE-MARK-MASQ
-A KUBE-SEP-QHRWSLKOO5YUPI7O -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx:<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp -j DNAT <span class="p">&amp;</span>ndash<span class="p">;</span>to-destination 10.244.2.10:80</code></pre></div></p>

<p>第1条规则，终于看到这个virtual IP了。node上不需要有这个ip地址，iptables在看到目的地址为virutal ip的符合规则tcp报文，会走KUBE-SVC-BEPXDJBUHFCSYIC3规则。</p>

<p>第2/3条规则，KUBE-SVC-BEPXDJBUHFCSYIC3链实现了将报文按50%的统计概率随机匹配到2条规则(round-robin)。</p>

<p>第4/5和5/6为成对的2组规则，将报文转给了真正的服务pod。</p>

<p>至此，从物理node收到目的地址为10.11.97.177、端口号为80的报文开始，到pod my-nginx收到报文并响应，描述了一个完整的链路。可以看到，整个报文链路上没有经过任何用户态进程，效率和稳定性都比较高。</p>

<h2>NodePort</h2>

<p>上面的例子里，由于10.11.97.177其实还是在集群内有效地址，由于实际上并不存在这个地址，当从集群外访问时会访问失败，这时需要将service暴漏出去。k8s给出的一个方案是NodePort，客户端根据NodePort+集群内任一物理节点的IP，就可以访问k8s的service了。这又是怎么做到的呢？</p>

<p>答案还是iptables。我们来看下面这个sock-shop的例子，其创建方法见k8s.io，不再赘述。
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@localhost k8s<span class="o">]</span><span class="c"># kubectl describe svc front-end -n sock-shop</span>
Name:                   front-end
Namespace:              sock-shop
Labels:                 <span class="nv">name</span><span class="o">=</span>front-end
Selector:               <span class="nv">name</span><span class="o">=</span>front-end
Type:                   NodePort
IP:                     10.15.9.0
Port:                   &lt;<span class="nb">unset</span>&gt; 80/TCP
NodePort:               &lt;<span class="nb">unset</span>&gt; 30001/TCP
Endpoints:              10.244.2.5:8079
Session Affinity:       None</code></pre></div></p>

<p>在任一node上查看iptables-save：
<div class="highlight"><pre><code class="language-bash" data-lang="bash">-A KUBE-NODEPORTS -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>sock-shop/front-end:<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp <span class="p">&amp;</span>ndash<span class="p">;</span>dport <span class="m">30001</span> -j KUBE-MARK-MASQ
-A KUBE-NODEPORTS -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>sock-shop/front-end:<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp <span class="p">&amp;</span>ndash<span class="p">;</span>dport <span class="m">30001</span> -j KUBE-SVC-LFMD53S3EZEAOUSJ&lt;/p&gt;

&lt;p&gt;-A KUBE-SERVICES -d 10.15.9.0/32 -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>sock-shop/front-end: cluster IP<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp <span class="p">&amp;</span>ndash<span class="p">;</span>dport <span class="m">80</span> -j KUBE-SVC-LFMD53S3EZEAOUSJ&lt;/p&gt;

&lt;p&gt;-A KUBE-SVC-LFMD53S3EZEAOUSJ -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>sock-shop/front-end:<span class="p">&amp;</span>rdquo<span class="p">;</span> -j KUBE-SEP-SM6TGF2R62ADFGQA&lt;/p&gt;

&lt;p&gt;-A KUBE-SEP-SM6TGF2R62ADFGQA -s 10.244.2.5/32 -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>sock-shop/front-end:<span class="p">&amp;</span>rdquo<span class="p">;</span> -j KUBE-MARK-MASQ
-A KUBE-SEP-SM6TGF2R62ADFGQA -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>sock-shop/front-end:<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp -j DNAT <span class="p">&amp;</span>ndash<span class="p">;</span>to-destination 10.244.2.5:8079</code></pre></div>
聪明如你，一定已经看明白了吧。</p>

<p>要是还不明白，看看这篇文章：源地址审计：<a href="https://ieevee.com/tech/2017/09/18/k8s-svc-src.html">追踪 kubernetees 服务的SNAT</a> 。</p>

<p>不过kube-proxy的iptables有个缺陷，即当pod故障时无法自动重试，需要依赖readiness probes，主要思想就是创建一个探测容器，当检测到后端pod挂了的时候，更新iptables。</p>

<p>在用NodePort的时候，经常会有人问一个问题，NodePort指定的端口(30000+)，而client建立tcp连接时，本地端口是操作系统随机选定的(30000+)，如何防止产生冲突呢？</p>

<p>解决办法是kube-proxy进程会去起一个tcp listen socket，监听端口号就是NodePort。可以把这个socket理解为“占位符”，目的是为了让操作系统跳开该端口。</p>

<p>转载自<a href="https://ieevee.com/tech/2017/01/20/k8s-service.html#kube-proxy">谈谈kubernets的service组件的Virtual IP</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[验证kubernetes的pod安全策略]]></title>
    <link href="http://liyongxin.github.io/blog/2019/03/18/yan-zheng-kubernetesde-podan-quan-ce-lue/"/>
    <updated>2019-03-18T14:19:02+08:00</updated>
    <id>http://liyongxin.github.io/blog/2019/03/18/yan-zheng-kubernetesde-podan-quan-ce-lue</id>
    <content type="html"><![CDATA[

<p>最近有客户提出需要对业务集群的pod做安全限制，不允许使用pod拥有privileged的权限，研究一番，刚好k8s的<a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">Pod Security Policies</a>可以实现该需求。</p>

<h2>什么是pod安全策略</h2>

<p>Pod Security Policy(简称psp)是集群级别的资源，该资源控制pod的spec中安全相关的方面，具体的方面参考下表：</p>

<table>
<thead>
<tr>
<th style="text-align:center;">       Control Aspect                  </th>
<th style="text-align:center;">       Field Names     </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;"> Running of privileged containers      </td>
<td style="text-align:center;"> <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#privileged">privileged</a>   </td>
</tr>
<tr>
<td style="text-align:center;"> Usage of host namespaces              </td>
<td style="text-align:center;">   <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces">hostPID, hostIPC</a>  </td>
</tr>
<tr>
<td style="text-align:center;"> Usage of host networking and ports    </td>
<td style="text-align:center;">    <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces">hostNetwork, hostPorts</a>  </td>
</tr>
<tr>
<td style="text-align:center;"> Usage of volume types                 </td>
<td style="text-align:center;"> <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems">volumes</a>   </td>
</tr>
<tr>
<td style="text-align:center;"> Usage of the host filesystem          </td>
<td style="text-align:center;">  <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems">allowedHostPaths</a>  </td>
</tr>
<tr>
<td style="text-align:center;"> White list of Flexvolume drivers      </td>
<td style="text-align:center;">   <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#flexvolume-drivers">allowedFlexVolumes</a>  </td>
</tr>
<tr>
<td style="text-align:center;"> Allocating an FSGroup that owns the pod’s volumes           </td>
<td style="text-align:center;"> <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems">fsGroup</a>   </td>
</tr>
<tr>
<td style="text-align:center;"> Requiring the use of a read only root file system             </td>
<td style="text-align:center;">   <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems">readOnlyRootFilesystem</a>  </td>
</tr>
<tr>
<td style="text-align:center;"> The user and group IDs of the container       </td>
<td style="text-align:center;">    <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#users-and-groups">runAsUser, runAsGroup, supplementalGroups</a>  </td>
</tr>
<tr>
<td style="text-align:center;"> The SELinux context of the container                      </td>
<td style="text-align:center;"> <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#selinux">seLinux</a>   </td>
</tr>
</tbody>
</table>


<!--more-->


<h2>如何开启Pod Security Policy</h2>

<ul>
<li><strong>Enable API extensions</strong></li>
</ul>


<p>For Kubernetes &lt; 1.6.0, the API Server must enable the extensions/v1beta1/podsecuritypolicy API extensions group (&ndash;runtime-config=extensions/v1beta1/podsecuritypolicy=true).</p>

<ul>
<li><strong>Enable PodSecurityPolicy admission control policy</strong></li>
</ul>


<p>The following parameter needs to be added to the API server startup argument: –admission-control=PodSecurityPolicy</p>

<p>默认psp是不开启的，若要开启，需要配置上述apiserver启动参数，默认的apiserver的yaml文件为<code>/etc/kubernetes/manifests/kube-apiserver.yaml</code>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="p">&amp;</span>hellip<span class="p">;</span>
spec:
  containers:
  - <span class="nb">command</span>:
    - kube-apiserver
    - <span class="p">&amp;</span>ndash<span class="p">;</span>authorization-mode<span class="o">=</span>Node,RBAC
    - <span class="p">&amp;</span>ndash<span class="p">;</span><span class="nb">enable</span>-admission-plugins<span class="o">=</span>Initializers,NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,ResourceQuota,PodSecurityPolicy
    - <span class="p">&amp;</span>ndash<span class="p">;</span>runtime-config<span class="o">=</span>extensions/v1beta1/podsecuritypolicy<span class="o">=</span><span class="nb">true</span>
    <span class="p">&amp;</span>hellip<span class="p">;</span></code></pre></div></p>

<h2>配置默认的psp</h2>

<p>由于已有打开了pod创建的安全策略,但此时还未创建任何的policy，所以任何pod都无法被创建，此时如果尝试去创建一个pod，会发现pod无法进行调度，
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl run &amp;ndash;image=nginx yxli</span>
deployment.apps/yxli created
<span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl get deploy yxli</span>
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
yxli      <span class="m">1</span>         <span class="m">0</span>         <span class="m">0</span>            <span class="m">0</span>           4m</code></pre></div>
查看controller的日志会发现报错forbidden: no providers available to validate pod request
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master ~<span class="o">]</span><span class="c"># kubectl logs &amp;ndash;tail=10 -f kube-controller-manager-build-master -n kube-system</span>
E0318 03:01:17.321184       <span class="m">1</span> replica_set.go:450<span class="o">]</span> Sync <span class="p">&amp;</span>ldquo<span class="p">;</span>default/yxli-6978dddc9d<span class="p">&amp;</span>rdquo<span class="p">;</span> failed with pods <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d-<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request
I0318 03:01:17.321302       <span class="m">1</span> event.go:221<span class="o">]</span> Event<span class="o">(</span>v1.ObjectReference<span class="o">{</span>Kind:<span class="p">&amp;</span>ldquo<span class="p">;</span>ReplicaSet<span class="p">&amp;</span>rdquo<span class="p">;</span>, Namespace:<span class="p">&amp;</span>ldquo<span class="p">;</span>default<span class="p">&amp;</span>rdquo<span class="p">;</span>, Name:<span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d<span class="p">&amp;</span>rdquo<span class="p">;</span>, UID:<span class="p">&amp;</span>ldquo<span class="p">;</span>0d11050f-492a-11e9-8ef0-00163e004fd2<span class="p">&amp;</span>rdquo<span class="p">;</span>, APIVersion:<span class="p">&amp;</span>ldquo<span class="p">;</span>apps/v1<span class="p">&amp;</span>rdquo<span class="p">;</span>, ResourceVersion:<span class="p">&amp;</span>ldquo<span class="p">;</span>764927<span class="p">&amp;</span>rdquo<span class="p">;</span>, FieldPath:<span class="p">&amp;</span>ldquo<span class="p">;&amp;</span>rdquo<span class="p">;</span><span class="o">})</span>: <span class="nb">type</span>: <span class="p">&amp;</span>lsquo<span class="p">;</span>Warning<span class="p">&amp;</span>rsquo<span class="p">;</span> reason: <span class="p">&amp;</span>lsquo<span class="p">;</span>FailedCreate<span class="p">&amp;</span>rsquo<span class="p">;</span> Error creating: pods <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d-<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request
E0318 03:01:37.807060       <span class="m">1</span> replica_set.go:450<span class="o">]</span> Sync <span class="p">&amp;</span>ldquo<span class="p">;</span>default/yxli-6978dddc9d<span class="p">&amp;</span>rdquo<span class="p">;</span> failed with pods <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d-<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request
I0318 03:01:37.807046       <span class="m">1</span> event.go:221<span class="o">]</span> Event<span class="o">(</span>v1.ObjectReference<span class="o">{</span>Kind:<span class="p">&amp;</span>ldquo<span class="p">;</span>ReplicaSet<span class="p">&amp;</span>rdquo<span class="p">;</span>, Namespace:<span class="p">&amp;</span>ldquo<span class="p">;</span>default<span class="p">&amp;</span>rdquo<span class="p">;</span>, Name:<span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d<span class="p">&amp;</span>rdquo<span class="p">;</span>, UID:<span class="p">&amp;</span>ldquo<span class="p">;</span>0d11050f-492a-11e9-8ef0-00163e004fd2<span class="p">&amp;</span>rdquo<span class="p">;</span>, APIVersion:<span class="p">&amp;</span>ldquo<span class="p">;</span>apps/v1<span class="p">&amp;</span>rdquo<span class="p">;</span>, ResourceVersion:<span class="p">&amp;</span>ldquo<span class="p">;</span>764927<span class="p">&amp;</span>rdquo<span class="p">;</span>, FieldPath:<span class="p">&amp;</span>ldquo<span class="p">;&amp;</span>rdquo<span class="p">;</span><span class="o">})</span>: <span class="nb">type</span>: <span class="p">&amp;</span>lsquo<span class="p">;</span>Warning<span class="p">&amp;</span>rsquo<span class="p">;</span> reason: <span class="p">&amp;</span>lsquo<span class="p">;</span>FailedCreate<span class="p">&amp;</span>rsquo<span class="p">;</span> Error creating: pods <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d-<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request
E0318 03:02:18.773092       <span class="m">1</span> replica_set.go:450<span class="o">]</span> Sync <span class="p">&amp;</span>ldquo<span class="p">;</span>default/yxli-6978dddc9d<span class="p">&amp;</span>rdquo<span class="p">;</span> failed with pods <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d-<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request</code></pre></div></p>

<p>而由于我们修改了apiserver的参数，pod受kubelet管理，自动触发了重建，此时apiserver的pod也是因为缺少权限没法创建出来
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># journalctl -fu kubelet</span>
<span class="p">&amp;</span>ndash<span class="p">;</span> Logs begin at Tue 2019-03-12 20:33:59 CST. <span class="p">&amp;</span>ndash<span class="p">;</span>
Mar <span class="m">18</span> 11:25:29 build-master kubelet<span class="o">[</span>4013<span class="o">]</span>: E0318 11:25:29.016416    <span class="m">4013</span> kubelet.go:1594<span class="o">]</span> Failed creating a mirror pod <span class="k">for</span> <span class="p">&amp;</span>ldquo<span class="p">;</span>kube-apiserver-build-master_kube-system<span class="o">(</span>77a7d13c654e7233306d4e2948aaaa78<span class="o">)</span><span class="p">&amp;</span>rdquo<span class="p">;</span>: pods <span class="p">&amp;</span>ldquo<span class="p">;</span>kube-apiserver-build-master<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request
Mar <span class="m">18</span> 11:25:30 build-master kubelet<span class="o">[</span>4013<span class="o">]</span>: W0318 11:25:30.113587    <span class="m">4013</span> kubelet.go:1579<span class="o">]</span> Deleting mirror pod <span class="p">&amp;</span>ldquo<span class="p">;</span>kube-apiserver-build-master_kube-system<span class="o">(</span>e48d84cd-46f0-11e9-8ef0-00163e004fd2<span class="o">)</span><span class="p">&amp;</span>rdquo<span class="p">;</span> because it is outdated
Mar <span class="m">18</span> 11:25:30 build-master kubelet<span class="o">[</span>4013<span class="o">]</span>: E0318 11:25:30.117242    <span class="m">4013</span> kubelet.go:1594<span class="o">]</span> Failed creating a mirror pod <span class="k">for</span> <span class="p">&amp;</span>ldquo<span class="p">;</span>kube-apiserver-build-master_kube-system<span class="o">(</span>77a7d13c654e7233306d4e2948aaaa78<span class="o">)</span><span class="p">&amp;</span>rdquo<span class="p">;</span>: pods <span class="p">&amp;</span>ldquo<span class="p">;</span>kube-apiserver-build-master<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request</code></pre></div></p>

<p>所以我们需要创建一个policy，为我们需要的受kubelet管理的静态pod以及kube-system命名空间下的pod提供权限，否则pod一旦发生重建，都将因为缺少权限导致无法正常创建出来。</p>

<p>首先新建文件 privileged.policy.yaml，权限不受限制，
<div class="highlight"><pre><code class="language-bash" data-lang="bash">apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: privileged
spec:
  allowedCapabilities:
  - <span class="p">&amp;</span>lsquo<span class="p">;</span>&lt;em&gt;<span class="p">&amp;</span>rsquo<span class="p">;</span>
  allowPrivilegeEscalation: <span class="nb">true</span>
<span class="nb">  </span>fsGroup:
    rule: <span class="p">&amp;</span>lsquo<span class="p">;</span>RunAsAny<span class="p">&amp;</span>rsquo<span class="p">;</span>
  hostIPC: <span class="nb">true</span>
<span class="nb">  </span>hostNetwork: <span class="nb">true</span>
<span class="nb">  </span>hostPID: <span class="nb">true</span>
<span class="nb">  </span>hostPorts:
  - min: 0
    max: 65535
  privileged: <span class="nb">true</span>
<span class="nb">  </span>readOnlyRootFilesystem: <span class="nb">false</span>
<span class="nb">  </span>runAsUser:
    rule: <span class="p">&amp;</span>lsquo<span class="p">;</span>RunAsAny<span class="p">&amp;</span>rsquo<span class="p">;</span>
  seLinux:
    rule: <span class="p">&amp;</span>lsquo<span class="p">;</span>RunAsAny<span class="p">&amp;</span>rsquo<span class="p">;</span>
  supplementalGroups:
    rule: <span class="p">&amp;</span>lsquo<span class="p">;</span>RunAsAny<span class="p">&amp;</span>rsquo<span class="p">;</span>
  volumes:
  - <span class="p">&amp;</span>lsquo<span class="p">;</span>&lt;/em&gt;<span class="p">&amp;</span>rsquo<span class="p">;</span></code></pre></div></p>

<p>创建并查看该policy
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl create -f privileged.policy.yaml</span>
podsecuritypolicy.policy/privileged created
<span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl get psp</span>
NAME         PRIV      CAPS      SELINUX    RUNASUSER   FSGROUP    SUPGROUP   READONLYROOTFS   VOLUMES
privileged   <span class="nb">true</span>      *         RunAsAny   RunAsAny    RunAsAny   RunAsAny   <span class="nb">false</span>            *</code></pre></div></p>

<p>然后还需要创建一个clusterrole，并且赋予该role对上述psp对使用权
<div class="highlight"><pre><code class="language-bash" data-lang="bash">&lt;/p&gt;

&lt;h1&gt;Cluster role which grants access to the privileged pod security policy&lt;/h1&gt;

&lt;p&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: privileged-psp
rules:
- apiGroups:
  - policy
  resourceNames:
  - privileged
  resources:
  - podsecuritypolicies
  verbs:
  - use</code></pre></div>
然后把clusterrole赋予serviceaccount或者对应的user、group，针对kube-system命名空间下的pod来说，都使用了kube-system下的serviceaccount或者由kubelet管理，kubelet是使用system:nodes这个组来管理的pod，所以只需要做如下binding即可为kube-system下的所有pod提供权限，
<div class="highlight"><pre><code class="language-bash" data-lang="bash">apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kube-system-psp
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: privileged-psp
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:nodes
  namespace: kube-system
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:serviceaccounts:kube-system</code></pre></div></p>

<p>都创建好之后，查看kube-system下的pod，发现apiserver已经创建成功
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl get po -n kube-system</span>
NAME                                   READY     STATUS    RESTARTS   AGE
coredns-68f5b48ccb-9hjvr               1/1       Running   <span class="m">0</span>          5d
coredns-68f5b48ccb-qrjnr               1/1       Running   <span class="m">0</span>          5d
etcd-build-master                      1/1       Running   <span class="m">0</span>          5d
kube-apiserver-build-master            1/1       Running   <span class="m">0</span>          18s</code></pre></div></p>

<blockquote><p>上面的binding只是为kube-system空间赋予了权限，若想要为别的命名空间赋予权限，可以使用ClusterRoleBinding的方式为多个namespace绑定privileged-psp的clusterrole，或者像如下方式，为所有合法的serviceaccounts和user绑定权限，当然前提是pod使用了namespace下的serviceAccount
<div class="highlight"><pre><code class="language-bash" data-lang="bash">&lt;/p&gt;

&lt;h1&gt;Authorize all service accounts in a namespace:&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;kind: Group
apiGroup: rbac.authorization.k8s.io
name: system:serviceaccounts

&lt;h1&gt;Or equivalently, all authenticated users in a namespace:&lt;/h1&gt;&lt;/li&gt;
&lt;li&gt;kind: Group
apiGroup: rbac.authorization.k8s.io
name: system:authenticated</code></pre></div></li>
</ul>
</blockquote>

<h2>验证psp的privileged限制</h2>

<p>该章节会创建一个名为psp-namespace的namespace做测试，来验证psp中对privileged的pod的限制
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl create namespace psp-example</span>
namespace/psp-example created
<span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl create serviceaccount fake-user -n psp-example</span>
serviceaccount/fake-user created
<span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl run &amp;ndash;image=nginx psp-pod-nginx-1 -n psp-example &amp;ndash;serviceaccount=fake-user</span>
deployment.apps/psp-pod-nginx-1 created
<span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl get po -n psp-example</span>
No resources found.
<span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl get deploy -n psp-example</span>
NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
psp-pod-nginx-1   <span class="m">1</span>         <span class="m">0</span>         <span class="m">0</span>            <span class="m">0</span>           23s</code></pre></div></p>

<p>如上所示，新的psp-example命名空间由于没有任何权限，所以无法创建新pod，接下来我们创建一个policy并给psp-example做authorize，但是policy中会限制无法创建privileged的pod
<div class="highlight"><pre><code class="language-bash" data-lang="bash">apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: example
spec:
  privileged: <span class="nb">false</span>  <span class="c"># Don&amp;rsquo;t allow privileged pods!</span>
  <span class="c"># The rest fills in some required fields.</span>
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  runAsUser:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  volumes:
  - <span class="p">&amp;</span>lsquo<span class="p">;</span>*<span class="p">&amp;</span>rsquo<span class="p">;</span></code></pre></div></p>

<p> 然后创建role和rolebinding，并验证fake-user是否有权限使用example的policy
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master psp-example<span class="o">]</span><span class="c"># kubectl -n psp-example create role psp:unprivileged &amp;ndash;verb=use &amp;ndash;resource=podsecuritypolicy &amp;ndash;resource-name=example</span>
role.rbac.authorization.k8s.io/psp:unprivileged created
<span class="o">[</span>root@build-master psp-example<span class="o">]</span><span class="c"># kubectl -n psp-example create rolebinding fake-user:psp:unprivileged &amp;ndash;role=psp:unprivileged &amp;ndash;serviceaccount=psp-example:fake-user</span>
rolebinding.rbac.authorization.k8s.io/fake-user:psp:unprivileged created
<span class="o">[</span>root@build-master psp-example<span class="o">]</span><span class="c">#  kubectl &amp;ndash;as=system:serviceaccount:psp-example:fake-user -n psp-example auth can-i use podsecuritypolicy/example</span>
yes</code></pre></div></p>

<p>等待片刻，再次查看刚才创建的deploy，发现已经调度成功
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master psp-example<span class="o">]</span><span class="c"># kubectl get deploy -n psp-example psp-pod-nginx-1</span>
NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
psp-pod-nginx-1   <span class="m">1</span>         <span class="m">1</span>         <span class="m">1</span>            <span class="m">1</span>           22m</code></pre></div></p>

<p>此时我们尝试在psp-example下创建一个privileged的特权容器，
<div class="highlight"><pre><code class="language-bash" data-lang="bash">apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    run: psp-pod-nginx-privileged
  name: psp-pod-nginx-privileged
  namespace: psp-example
spec:
  selector:
    matchLabels:
      run: psp-pod-nginx-privileged
  template:
    metadata:
      labels:
        run: psp-pod-nginx-privileged
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: psp-pod-nginx-privileged
      securityContext:
        privileged: <span class="nb">true</span>
<span class="nb">      </span>serviceAccount: fake-user</code></pre></div>
创建并查看结果
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master psp-example<span class="o">]</span><span class="c"># kubectl create -f privileged-pod.yaml</span>
deployment.extensions/psp-pod-nginx-privileged created
<span class="o">[</span>root@build-master psp-example<span class="o">]</span><span class="c"># kubectl get deploy -n psp-example psp-pod-nginx-privileged</span>
psp-pod-nginx-privileged   <span class="m">1</span>         <span class="m">0</span>         <span class="m">0</span>            <span class="m">0</span>           24s
<span class="o">[</span>root@build-master ~<span class="o">]</span><span class="c"># kubectl logs &amp;ndash;tail=1 -f kube-controller-manager-build-master -n kube-system</span>
I0318 04:32:42.777200       <span class="m">1</span> event.go:221<span class="o">]</span> Event<span class="o">(</span>v1.ObjectReference<span class="o">{</span>Kind:<span class="p">&amp;</span>ldquo<span class="p">;</span>ReplicaSet<span class="p">&amp;</span>rdquo<span class="p">;</span>, Namespace:<span class="p">&amp;</span>ldquo<span class="p">;</span>psp-example<span class="p">&amp;</span>rdquo<span class="p">;</span>, Name:<span class="p">&amp;</span>ldquo<span class="p">;</span>psp-pod-nginx-privileged-d7f8dbd75<span class="p">&amp;</span>rdquo<span class="p">;</span>, UID:<span class="p">&amp;</span>ldquo<span class="p">;</span>c66fb10f-4936-11e9-9c58-00163e004fd2<span class="p">&amp;</span>rdquo<span class="p">;</span>, APIVersion:<span class="p">&amp;</span>ldquo<span class="p">;</span>apps/v1<span class="p">&amp;</span>rdquo<span class="p">;</span>, ResourceVersion:<span class="p">&amp;</span>ldquo<span class="p">;</span>773292<span class="p">&amp;</span>rdquo<span class="p">;</span>, FieldPath:<span class="p">&amp;</span>ldquo<span class="p">;&amp;</span>rdquo<span class="p">;</span><span class="o">})</span>: <span class="nb">type</span>: <span class="p">&amp;</span>lsquo<span class="p">;</span>Warning<span class="p">&amp;</span>rsquo<span class="p">;</span> reason: <span class="p">&amp;</span>lsquo<span class="p">;</span>FailedCreate<span class="p">&amp;</span>rsquo<span class="p">;</span> Error creating: pods <span class="p">&amp;</span>ldquo<span class="p">;</span>psp-pod-nginx-privileged-d7f8dbd75-<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: unable to validate against any pod security policy: <span class="o">[</span>spec.containers<span class="o">[</span>0<span class="o">]</span>.securityContext.privileged: Invalid value: <span class="nb">true</span>: Privileged containers are not allowed<span class="o">]</span></code></pre></div>
查看controller-manager的日志，发现Privileged containers are not allowed</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[somethings about kubernetes]]></title>
    <link href="http://liyongxin.github.io/blog/2019/02/28/k8sxiao-zhi-shi/"/>
    <updated>2019-02-28T11:12:56+08:00</updated>
    <id>http://liyongxin.github.io/blog/2019/02/28/k8sxiao-zhi-shi</id>
    <content type="html"><![CDATA[<ul>
<li><p>k8s默认驱逐设置
<div class="highlight"><pre><code class="language-bash" data-lang="bash">// DefaultEvictionHard includes default options <span class="k">for</span> hard eviction.
var <span class="nv">DefaultEvictionHard</span> <span class="o">=</span> map<span class="o">[</span>string<span class="o">]</span>string<span class="o">{</span>
      <span class="p">&amp;</span>ldquo<span class="p">;</span>memory.available<span class="p">&amp;</span>rdquo<span class="p">;</span>:  <span class="p">&amp;</span>ldquo<span class="p">;</span>100Mi<span class="p">&amp;</span>rdquo<span class="p">;</span>,
      <span class="p">&amp;</span>ldquo<span class="p">;</span>nodefs.available<span class="p">&amp;</span>rdquo<span class="p">;</span>:  <span class="p">&amp;</span>ldquo<span class="p">;</span>10%<span class="p">&amp;</span>rdquo<span class="p">;</span>,
      <span class="p">&amp;</span>ldquo<span class="p">;</span>nodefs.inodesFree<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>5%<span class="p">&amp;</span>rdquo<span class="p">;</span>,
      <span class="p">&amp;</span>ldquo<span class="p">;</span>imagefs.available<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>15%<span class="p">&amp;</span>rdquo<span class="p">;</span>,
<span class="o">}</span></code></pre></div></p></li>
<li><p>kubernetes 滚动升级
<div class="highlight"><pre><code class="language-bash" data-lang="bash">&lt;/p&gt;

&lt;h1&gt;run <span class="nb">test </span>deploy&lt;/h1&gt;

<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl run &amp;ndash;image=nginx &amp;ndash;port=80 &amp;ndash;replicas=2 yxli-nginx</span>

&lt;h1&gt;scale replica&lt;/h1&gt;

<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl scale &amp;ndash;replicas=1 deploy/yxli-nginx</span>
<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl get deploy</span>
NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
busybox      <span class="m">2</span>         <span class="m">2</span>         <span class="m">2</span>            <span class="m">2</span>           39d
busybox1     <span class="m">1</span>         <span class="m">1</span>         <span class="m">1</span>            <span class="m">1</span>           39d
yxli-nginx   <span class="m">1</span>         <span class="m">1</span>         <span class="m">1</span>            <span class="m">1</span>           2h

&lt;h1&gt;update image&lt;/h1&gt;

<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl set image  deploy/yxli-nginx yxli-nginx=nginx:alpine</span>

&lt;h1&gt;查看升级历史&lt;/h1&gt;

<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl rollout history deploy/yxli-nginx</span>
deployments <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-nginx<span class="p">&amp;</span>rdquo<span class="p">;</span>
REVISION  CHANGE-CAUSE
<span class="m">1</span>         &lt;none&gt;
<span class="m">2</span>         &lt;none&gt;

&lt;h1&gt;回顾至上次版本&lt;/h1&gt;

<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl rollout undo deploy/yxli-nginx</span>
<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl rollout history deploy/yxli-nginx</span>
deployments <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-nginx<span class="p">&amp;</span>rdquo<span class="p">;</span>
REVISION  CHANGE-CAUSE
<span class="m">2</span>         &lt;none&gt;
<span class="m">3</span>         &lt;none&gt;

&lt;h1&gt;回滚至指定版本&lt;/h1&gt;

&lt;p&gt;<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl rolloutundo deployment/lykops-dpm &amp;ndash;to-revision=2</span></code></pre></div></p></li>
</ul>


<!--more-->


<ul>
<li>查看docker使用的cpu核心数
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@yxli-onebox docker<span class="o">]</span><span class="c"># pwd</span>
/sys/fs/cgroup/cpuset/docker
<span class="o">[</span>root@yxli-onebox docker<span class="o">]</span><span class="c"># cat cpuset.cpus</span>
0-7</code></pre></div></li>
</ul>

]]></content>
  </entry>
  
</feed>
