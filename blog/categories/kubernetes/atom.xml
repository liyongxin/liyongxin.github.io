<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kubernetes | Earlene]]></title>
  <link href="http://liyongxin.github.io/blog/categories/kubernetes/atom.xml" rel="self"/>
  <link href="http://liyongxin.github.io/"/>
  <updated>2019-04-03T17:32:56+08:00</updated>
  <id>http://liyongxin.github.io/</id>
  <author>
    <name><![CDATA[yxli@alauda.io]]></name>
    <email><![CDATA[yxli@alauda.io]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[kubernetes的service cluster ip]]></title>
    <link href="http://liyongxin.github.io/blog/2019/04/03/kubernetesde-service-cluster-ip/"/>
    <updated>2019-04-03T17:17:23+08:00</updated>
    <id>http://liyongxin.github.io/blog/2019/04/03/kubernetesde-service-cluster-ip</id>
    <content type="html"><![CDATA[<p>k8s的pod可以有多个副本，但是在访问pod时，会有几个问题：</p>

<ul>
<li>客户端需要知道各个pod的地址</li>
<li>某一node上的pod如果故障，客户端需要感知</li>
</ul>


<p>为了解决这个问题，k8s引入了service的概念，用以指导客户端的流量。</p>

<h2>Service</h2>

<p>以下面的my-nginx为例。</p>

<p>pod和service的定义文件如下：
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@localhost k8s<span class="o">]</span><span class="c"># cat run-my-nginx.yaml</span>
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: my-nginx
spec:
  replicas: 2
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: nginx
        ports:
        - containerPort: 80
<span class="o">[</span>root@localhost k8s<span class="o">]</span><span class="c"># cat run-my-nginx-service.yaml</span>
apiVersion: v1
kind: Service
metadata:
  name: my-nginx
  labels:
    run: my-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
  selector:
    run: my-nginx</code></pre></div></p>

<p>pod my-nginx定义的replicas为2即2个副本，端口号为80;service my-nginx定义的selector为run:my-nginx，即该service选中所有label为run: my-nginx的pod；定义的port为80。</p>

<p>使用kubectl create -f xx.yml创建后，可以在集群上看到2个pod，地址分别为10.244.1.10/10.244.2.10；可以看到1个service，IP/Port为10.11.97.177/80，其对接的Endpoints为10.244.1.10:80,10.244.2.10:80，即2个pod的服务地址，这三个URL在集群内任一节点都可以使用curl访问。
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@localhost k8s<span class="o">]</span><span class="c"># kubectl get pods -n default -o wide</span>
NAME                       READY     STATUS    RESTARTS   AGE       IP            NODE
my-nginx-379829228-3n755   1/1       Running   <span class="m">0</span>          21h       10.244.1.10   note2
my-nginx-379829228-xh214   1/1       Running   <span class="m">0</span>          21h       10.244.2.10   node1
<span class="o">[</span>root@localhost ~<span class="o">]</span><span class="c">#</span>
<span class="o">[</span>root@localhost ~<span class="o">]</span><span class="c"># kubectl describe svc my-nginx</span>
Name:                   my-nginx
Namespace:              default
Labels:                 <span class="nv">run</span><span class="o">=</span>my-nginx
Selector:               <span class="nv">run</span><span class="o">=</span>my-nginx
Type:                   ClusterIP
IP:                     10.11.97.177
Port:                   &lt;<span class="nb">unset</span>&gt; 80/TCP
Endpoints:              10.244.1.10:80,10.244.2.10:80
Session Affinity:       None</code></pre></div></p>

<p>但是，如果你去查看集群各节点的IP信息，是找不到10.11.97.177这个IP的，那么curl是如何通过这个(Virtual)IP地址访问到后端的Endpoints呢？
答案在<a href="https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies">这里</a>。</p>

<h2>kube-proxy</h2>

<p>k8s支持2种proxy模式，userspace和iptables。从v1.2版本开始，默认采用iptables proxy。那么这两种模式有什么不同吗？</p>

<p>1、userspace</p>

<p>顾名思义，userspace即用户空间。为什么这么叫呢？看下面的图。</p>

<p><img src="/images/custom/services-userspace-overview.svg" alt="" /></p>

<p>kube-proxy会为每个service随机监听一个端口(proxy port)，并增加一条iptables规则：所以到clusterIP:Port 的报文都redirect到proxy port；kube-proxy从它监听的proxy port收到报文后，走round robin（默认）或者session affinity（会话亲和力，即同一client IP都走同一链路给同一pod服务），分发给对应的pod。</p>

<p>显然userspace会造成所有报文都走一遍用户态，性能不高，现在k8s已经不再使用了。</p>

<p>2、iptables</p>

<p>我们回过头来看看userspace，既然用户态会增加性能损耗，那么有没有办法不走呢？实际上用户态也只是一个报文LB，通过iptables完全可以搞定。k8s下面这张图很清晰的说明了iptables方式与userspace方式的不同：kube-proxy只是作为controller，而不是server，真正服务的是内核的netfilter，体现在用户态则是iptables。</p>

<p>kube-proxy的iptables方式也支持round robin(默认)和session affinity。</p>

<p><img src="/images/custom/services-iptables-overview.svg" alt="" /></p>

<p>那么iptables是怎么做到LB，而且还能round-robin呢？我们通过iptables-save来看my-nginx这个服务在某一个node上的iptables规则。
<div class="highlight"><pre><code class="language-bash" data-lang="bash">-A KUBE-SERVICES -d 10.11.97.177/32 -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx: cluster IP<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp <span class="p">&amp;</span>ndash<span class="p">;</span>dport <span class="m">80</span> -j KUBE-SVC-BEPXDJBUHFCSYIC3&lt;/p&gt;

&lt;p&gt;-A KUBE-SVC-BEPXDJBUHFCSYIC3 -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx:<span class="p">&amp;</span>rdquo<span class="p">;</span> -m statistic <span class="p">&amp;</span>ndash<span class="p">;</span>mode random <span class="p">&amp;</span>ndash<span class="p">;</span>probability 0.50000000000 -j KUBE-SEP-U4UWLP4OR3LOJBXU
-A KUBE-SVC-BEPXDJBUHFCSYIC3 -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx:<span class="p">&amp;</span>rdquo<span class="p">;</span> -j KUBE-SEP-QHRWSLKOO5YUPI7O&lt;/p&gt;

&lt;p&gt;-A KUBE-SEP-U4UWLP4OR3LOJBXU -s 10.244.1.10/32 -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx:<span class="p">&amp;</span>rdquo<span class="p">;</span> -j KUBE-MARK-MASQ
-A KUBE-SEP-U4UWLP4OR3LOJBXU -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx:<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp -j DNAT <span class="p">&amp;</span>ndash<span class="p">;</span>to-destination 10.244.1.10:80&lt;/p&gt;

&lt;p&gt;-A KUBE-SEP-QHRWSLKOO5YUPI7O -s 10.244.2.10/32 -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx:<span class="p">&amp;</span>rdquo<span class="p">;</span> -j KUBE-MARK-MASQ
-A KUBE-SEP-QHRWSLKOO5YUPI7O -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>default/my-nginx:<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp -j DNAT <span class="p">&amp;</span>ndash<span class="p">;</span>to-destination 10.244.2.10:80</code></pre></div></p>

<p>第1条规则，终于看到这个virtual IP了。node上不需要有这个ip地址，iptables在看到目的地址为virutal ip的符合规则tcp报文，会走KUBE-SVC-BEPXDJBUHFCSYIC3规则。</p>

<p>第2/3条规则，KUBE-SVC-BEPXDJBUHFCSYIC3链实现了将报文按50%的统计概率随机匹配到2条规则(round-robin)。</p>

<p>第4/5和5/6为成对的2组规则，将报文转给了真正的服务pod。</p>

<p>至此，从物理node收到目的地址为10.11.97.177、端口号为80的报文开始，到pod my-nginx收到报文并响应，描述了一个完整的链路。可以看到，整个报文链路上没有经过任何用户态进程，效率和稳定性都比较高。</p>

<h2>NodePort</h2>

<p>上面的例子里，由于10.11.97.177其实还是在集群内有效地址，由于实际上并不存在这个地址，当从集群外访问时会访问失败，这时需要将service暴漏出去。k8s给出的一个方案是NodePort，客户端根据NodePort+集群内任一物理节点的IP，就可以访问k8s的service了。这又是怎么做到的呢？</p>

<p>答案还是iptables。我们来看下面这个sock-shop的例子，其创建方法见k8s.io，不再赘述。
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@localhost k8s<span class="o">]</span><span class="c"># kubectl describe svc front-end -n sock-shop</span>
Name:                   front-end
Namespace:              sock-shop
Labels:                 <span class="nv">name</span><span class="o">=</span>front-end
Selector:               <span class="nv">name</span><span class="o">=</span>front-end
Type:                   NodePort
IP:                     10.15.9.0
Port:                   &lt;<span class="nb">unset</span>&gt; 80/TCP
NodePort:               &lt;<span class="nb">unset</span>&gt; 30001/TCP
Endpoints:              10.244.2.5:8079
Session Affinity:       None</code></pre></div></p>

<p>在任一node上查看iptables-save：
<div class="highlight"><pre><code class="language-bash" data-lang="bash">-A KUBE-NODEPORTS -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>sock-shop/front-end:<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp <span class="p">&amp;</span>ndash<span class="p">;</span>dport <span class="m">30001</span> -j KUBE-MARK-MASQ
-A KUBE-NODEPORTS -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>sock-shop/front-end:<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp <span class="p">&amp;</span>ndash<span class="p">;</span>dport <span class="m">30001</span> -j KUBE-SVC-LFMD53S3EZEAOUSJ&lt;/p&gt;

&lt;p&gt;-A KUBE-SERVICES -d 10.15.9.0/32 -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>sock-shop/front-end: cluster IP<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp <span class="p">&amp;</span>ndash<span class="p">;</span>dport <span class="m">80</span> -j KUBE-SVC-LFMD53S3EZEAOUSJ&lt;/p&gt;

&lt;p&gt;-A KUBE-SVC-LFMD53S3EZEAOUSJ -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>sock-shop/front-end:<span class="p">&amp;</span>rdquo<span class="p">;</span> -j KUBE-SEP-SM6TGF2R62ADFGQA&lt;/p&gt;

&lt;p&gt;-A KUBE-SEP-SM6TGF2R62ADFGQA -s 10.244.2.5/32 -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>sock-shop/front-end:<span class="p">&amp;</span>rdquo<span class="p">;</span> -j KUBE-MARK-MASQ
-A KUBE-SEP-SM6TGF2R62ADFGQA -p tcp -m comment <span class="p">&amp;</span>ndash<span class="p">;</span>comment <span class="p">&amp;</span>ldquo<span class="p">;</span>sock-shop/front-end:<span class="p">&amp;</span>rdquo<span class="p">;</span> -m tcp -j DNAT <span class="p">&amp;</span>ndash<span class="p">;</span>to-destination 10.244.2.5:8079</code></pre></div>
聪明如你，一定已经看明白了吧。</p>

<p>要是还不明白，看看这篇文章：源地址审计：<a href="https://ieevee.com/tech/2017/09/18/k8s-svc-src.html">追踪 kubernetees 服务的SNAT</a> 。</p>

<p>不过kube-proxy的iptables有个缺陷，即当pod故障时无法自动重试，需要依赖readiness probes，主要思想就是创建一个探测容器，当检测到后端pod挂了的时候，更新iptables。</p>

<p>在用NodePort的时候，经常会有人问一个问题，NodePort指定的端口(30000+)，而client建立tcp连接时，本地端口是操作系统随机选定的(30000+)，如何防止产生冲突呢？</p>

<p>解决办法是kube-proxy进程会去起一个tcp listen socket，监听端口号就是NodePort。可以把这个socket理解为“占位符”，目的是为了让操作系统跳开该端口。</p>

<p>转载自<a href="https://ieevee.com/tech/2017/01/20/k8s-service.html#kube-proxy">谈谈kubernets的service组件的Virtual IP</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[验证kubernetes的pod安全策略]]></title>
    <link href="http://liyongxin.github.io/blog/2019/03/18/yan-zheng-kubernetesde-podan-quan-ce-lue/"/>
    <updated>2019-03-18T14:19:02+08:00</updated>
    <id>http://liyongxin.github.io/blog/2019/03/18/yan-zheng-kubernetesde-podan-quan-ce-lue</id>
    <content type="html"><![CDATA[

<p>最近有客户提出需要对业务集群的pod做安全限制，不允许使用pod拥有privileged的权限，研究一番，刚好k8s的<a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">Pod Security Policies</a>可以实现该需求。</p>

<h2>什么是pod安全策略</h2>

<p>Pod Security Policy(简称psp)是集群级别的资源，该资源控制pod的spec中安全相关的方面，具体的方面参考下表：</p>

<table>
<thead>
<tr>
<th style="text-align:center;">       Control Aspect                  </th>
<th style="text-align:center;">       Field Names     </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;"> Running of privileged containers      </td>
<td style="text-align:center;"> <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#privileged">privileged</a>   </td>
</tr>
<tr>
<td style="text-align:center;"> Usage of host namespaces              </td>
<td style="text-align:center;">   <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces">hostPID, hostIPC</a>  </td>
</tr>
<tr>
<td style="text-align:center;"> Usage of host networking and ports    </td>
<td style="text-align:center;">    <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces">hostNetwork, hostPorts</a>  </td>
</tr>
<tr>
<td style="text-align:center;"> Usage of volume types                 </td>
<td style="text-align:center;"> <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems">volumes</a>   </td>
</tr>
<tr>
<td style="text-align:center;"> Usage of the host filesystem          </td>
<td style="text-align:center;">  <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems">allowedHostPaths</a>  </td>
</tr>
<tr>
<td style="text-align:center;"> White list of Flexvolume drivers      </td>
<td style="text-align:center;">   <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#flexvolume-drivers">allowedFlexVolumes</a>  </td>
</tr>
<tr>
<td style="text-align:center;"> Allocating an FSGroup that owns the pod’s volumes           </td>
<td style="text-align:center;"> <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems">fsGroup</a>   </td>
</tr>
<tr>
<td style="text-align:center;"> Requiring the use of a read only root file system             </td>
<td style="text-align:center;">   <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems">readOnlyRootFilesystem</a>  </td>
</tr>
<tr>
<td style="text-align:center;"> The user and group IDs of the container       </td>
<td style="text-align:center;">    <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#users-and-groups">runAsUser, runAsGroup, supplementalGroups</a>  </td>
</tr>
<tr>
<td style="text-align:center;"> The SELinux context of the container                      </td>
<td style="text-align:center;"> <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#selinux">seLinux</a>   </td>
</tr>
</tbody>
</table>


<!--more-->


<h2>如何开启Pod Security Policy</h2>

<ul>
<li><strong>Enable API extensions</strong></li>
</ul>


<p>For Kubernetes &lt; 1.6.0, the API Server must enable the extensions/v1beta1/podsecuritypolicy API extensions group (&ndash;runtime-config=extensions/v1beta1/podsecuritypolicy=true).</p>

<ul>
<li><strong>Enable PodSecurityPolicy admission control policy</strong></li>
</ul>


<p>The following parameter needs to be added to the API server startup argument: –admission-control=PodSecurityPolicy</p>

<p>默认psp是不开启的，若要开启，需要配置上述apiserver启动参数，默认的apiserver的yaml文件为<code>/etc/kubernetes/manifests/kube-apiserver.yaml</code>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="p">&amp;</span>hellip<span class="p">;</span>
spec:
  containers:
  - <span class="nb">command</span>:
    - kube-apiserver
    - <span class="p">&amp;</span>ndash<span class="p">;</span>authorization-mode<span class="o">=</span>Node,RBAC
    - <span class="p">&amp;</span>ndash<span class="p">;</span><span class="nb">enable</span>-admission-plugins<span class="o">=</span>Initializers,NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,ResourceQuota,PodSecurityPolicy
    - <span class="p">&amp;</span>ndash<span class="p">;</span>runtime-config<span class="o">=</span>extensions/v1beta1/podsecuritypolicy<span class="o">=</span><span class="nb">true</span>
    <span class="p">&amp;</span>hellip<span class="p">;</span></code></pre></div></p>

<h2>配置默认的psp</h2>

<p>由于已有打开了pod创建的安全策略,但此时还未创建任何的policy，所以任何pod都无法被创建，此时如果尝试去创建一个pod，会发现pod无法进行调度，
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl run &amp;ndash;image=nginx yxli</span>
deployment.apps/yxli created
<span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl get deploy yxli</span>
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
yxli      <span class="m">1</span>         <span class="m">0</span>         <span class="m">0</span>            <span class="m">0</span>           4m</code></pre></div>
查看controller的日志会发现报错forbidden: no providers available to validate pod request
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master ~<span class="o">]</span><span class="c"># kubectl logs &amp;ndash;tail=10 -f kube-controller-manager-build-master -n kube-system</span>
E0318 03:01:17.321184       <span class="m">1</span> replica_set.go:450<span class="o">]</span> Sync <span class="p">&amp;</span>ldquo<span class="p">;</span>default/yxli-6978dddc9d<span class="p">&amp;</span>rdquo<span class="p">;</span> failed with pods <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d-<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request
I0318 03:01:17.321302       <span class="m">1</span> event.go:221<span class="o">]</span> Event<span class="o">(</span>v1.ObjectReference<span class="o">{</span>Kind:<span class="p">&amp;</span>ldquo<span class="p">;</span>ReplicaSet<span class="p">&amp;</span>rdquo<span class="p">;</span>, Namespace:<span class="p">&amp;</span>ldquo<span class="p">;</span>default<span class="p">&amp;</span>rdquo<span class="p">;</span>, Name:<span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d<span class="p">&amp;</span>rdquo<span class="p">;</span>, UID:<span class="p">&amp;</span>ldquo<span class="p">;</span>0d11050f-492a-11e9-8ef0-00163e004fd2<span class="p">&amp;</span>rdquo<span class="p">;</span>, APIVersion:<span class="p">&amp;</span>ldquo<span class="p">;</span>apps/v1<span class="p">&amp;</span>rdquo<span class="p">;</span>, ResourceVersion:<span class="p">&amp;</span>ldquo<span class="p">;</span>764927<span class="p">&amp;</span>rdquo<span class="p">;</span>, FieldPath:<span class="p">&amp;</span>ldquo<span class="p">;&amp;</span>rdquo<span class="p">;</span><span class="o">})</span>: <span class="nb">type</span>: <span class="p">&amp;</span>lsquo<span class="p">;</span>Warning<span class="p">&amp;</span>rsquo<span class="p">;</span> reason: <span class="p">&amp;</span>lsquo<span class="p">;</span>FailedCreate<span class="p">&amp;</span>rsquo<span class="p">;</span> Error creating: pods <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d-<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request
E0318 03:01:37.807060       <span class="m">1</span> replica_set.go:450<span class="o">]</span> Sync <span class="p">&amp;</span>ldquo<span class="p">;</span>default/yxli-6978dddc9d<span class="p">&amp;</span>rdquo<span class="p">;</span> failed with pods <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d-<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request
I0318 03:01:37.807046       <span class="m">1</span> event.go:221<span class="o">]</span> Event<span class="o">(</span>v1.ObjectReference<span class="o">{</span>Kind:<span class="p">&amp;</span>ldquo<span class="p">;</span>ReplicaSet<span class="p">&amp;</span>rdquo<span class="p">;</span>, Namespace:<span class="p">&amp;</span>ldquo<span class="p">;</span>default<span class="p">&amp;</span>rdquo<span class="p">;</span>, Name:<span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d<span class="p">&amp;</span>rdquo<span class="p">;</span>, UID:<span class="p">&amp;</span>ldquo<span class="p">;</span>0d11050f-492a-11e9-8ef0-00163e004fd2<span class="p">&amp;</span>rdquo<span class="p">;</span>, APIVersion:<span class="p">&amp;</span>ldquo<span class="p">;</span>apps/v1<span class="p">&amp;</span>rdquo<span class="p">;</span>, ResourceVersion:<span class="p">&amp;</span>ldquo<span class="p">;</span>764927<span class="p">&amp;</span>rdquo<span class="p">;</span>, FieldPath:<span class="p">&amp;</span>ldquo<span class="p">;&amp;</span>rdquo<span class="p">;</span><span class="o">})</span>: <span class="nb">type</span>: <span class="p">&amp;</span>lsquo<span class="p">;</span>Warning<span class="p">&amp;</span>rsquo<span class="p">;</span> reason: <span class="p">&amp;</span>lsquo<span class="p">;</span>FailedCreate<span class="p">&amp;</span>rsquo<span class="p">;</span> Error creating: pods <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d-<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request
E0318 03:02:18.773092       <span class="m">1</span> replica_set.go:450<span class="o">]</span> Sync <span class="p">&amp;</span>ldquo<span class="p">;</span>default/yxli-6978dddc9d<span class="p">&amp;</span>rdquo<span class="p">;</span> failed with pods <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-6978dddc9d-<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request</code></pre></div></p>

<p>而由于我们修改了apiserver的参数，pod受kubelet管理，自动触发了重建，此时apiserver的pod也是因为缺少权限没法创建出来
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># journalctl -fu kubelet</span>
<span class="p">&amp;</span>ndash<span class="p">;</span> Logs begin at Tue 2019-03-12 20:33:59 CST. <span class="p">&amp;</span>ndash<span class="p">;</span>
Mar <span class="m">18</span> 11:25:29 build-master kubelet<span class="o">[</span>4013<span class="o">]</span>: E0318 11:25:29.016416    <span class="m">4013</span> kubelet.go:1594<span class="o">]</span> Failed creating a mirror pod <span class="k">for</span> <span class="p">&amp;</span>ldquo<span class="p">;</span>kube-apiserver-build-master_kube-system<span class="o">(</span>77a7d13c654e7233306d4e2948aaaa78<span class="o">)</span><span class="p">&amp;</span>rdquo<span class="p">;</span>: pods <span class="p">&amp;</span>ldquo<span class="p">;</span>kube-apiserver-build-master<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request
Mar <span class="m">18</span> 11:25:30 build-master kubelet<span class="o">[</span>4013<span class="o">]</span>: W0318 11:25:30.113587    <span class="m">4013</span> kubelet.go:1579<span class="o">]</span> Deleting mirror pod <span class="p">&amp;</span>ldquo<span class="p">;</span>kube-apiserver-build-master_kube-system<span class="o">(</span>e48d84cd-46f0-11e9-8ef0-00163e004fd2<span class="o">)</span><span class="p">&amp;</span>rdquo<span class="p">;</span> because it is outdated
Mar <span class="m">18</span> 11:25:30 build-master kubelet<span class="o">[</span>4013<span class="o">]</span>: E0318 11:25:30.117242    <span class="m">4013</span> kubelet.go:1594<span class="o">]</span> Failed creating a mirror pod <span class="k">for</span> <span class="p">&amp;</span>ldquo<span class="p">;</span>kube-apiserver-build-master_kube-system<span class="o">(</span>77a7d13c654e7233306d4e2948aaaa78<span class="o">)</span><span class="p">&amp;</span>rdquo<span class="p">;</span>: pods <span class="p">&amp;</span>ldquo<span class="p">;</span>kube-apiserver-build-master<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: no providers available to validate pod request</code></pre></div></p>

<p>所以我们需要创建一个policy，为我们需要的受kubelet管理的静态pod以及kube-system命名空间下的pod提供权限，否则pod一旦发生重建，都将因为缺少权限导致无法正常创建出来。</p>

<p>首先新建文件 privileged.policy.yaml，权限不受限制，
<div class="highlight"><pre><code class="language-bash" data-lang="bash">apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: privileged
spec:
  allowedCapabilities:
  - <span class="p">&amp;</span>lsquo<span class="p">;</span>&lt;em&gt;<span class="p">&amp;</span>rsquo<span class="p">;</span>
  allowPrivilegeEscalation: <span class="nb">true</span>
<span class="nb">  </span>fsGroup:
    rule: <span class="p">&amp;</span>lsquo<span class="p">;</span>RunAsAny<span class="p">&amp;</span>rsquo<span class="p">;</span>
  hostIPC: <span class="nb">true</span>
<span class="nb">  </span>hostNetwork: <span class="nb">true</span>
<span class="nb">  </span>hostPID: <span class="nb">true</span>
<span class="nb">  </span>hostPorts:
  - min: 0
    max: 65535
  privileged: <span class="nb">true</span>
<span class="nb">  </span>readOnlyRootFilesystem: <span class="nb">false</span>
<span class="nb">  </span>runAsUser:
    rule: <span class="p">&amp;</span>lsquo<span class="p">;</span>RunAsAny<span class="p">&amp;</span>rsquo<span class="p">;</span>
  seLinux:
    rule: <span class="p">&amp;</span>lsquo<span class="p">;</span>RunAsAny<span class="p">&amp;</span>rsquo<span class="p">;</span>
  supplementalGroups:
    rule: <span class="p">&amp;</span>lsquo<span class="p">;</span>RunAsAny<span class="p">&amp;</span>rsquo<span class="p">;</span>
  volumes:
  - <span class="p">&amp;</span>lsquo<span class="p">;</span>&lt;/em&gt;<span class="p">&amp;</span>rsquo<span class="p">;</span></code></pre></div></p>

<p>创建并查看该policy
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl create -f privileged.policy.yaml</span>
podsecuritypolicy.policy/privileged created
<span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl get psp</span>
NAME         PRIV      CAPS      SELINUX    RUNASUSER   FSGROUP    SUPGROUP   READONLYROOTFS   VOLUMES
privileged   <span class="nb">true</span>      *         RunAsAny   RunAsAny    RunAsAny   RunAsAny   <span class="nb">false</span>            *</code></pre></div></p>

<p>然后还需要创建一个clusterrole，并且赋予该role对上述psp对使用权
<div class="highlight"><pre><code class="language-bash" data-lang="bash">&lt;/p&gt;

&lt;h1&gt;Cluster role which grants access to the privileged pod security policy&lt;/h1&gt;

&lt;p&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: privileged-psp
rules:
- apiGroups:
  - policy
  resourceNames:
  - privileged
  resources:
  - podsecuritypolicies
  verbs:
  - use</code></pre></div>
然后把clusterrole赋予serviceaccount或者对应的user、group，针对kube-system命名空间下的pod来说，都使用了kube-system下的serviceaccount或者由kubelet管理，kubelet是使用system:nodes这个组来管理的pod，所以只需要做如下binding即可为kube-system下的所有pod提供权限，
<div class="highlight"><pre><code class="language-bash" data-lang="bash">apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kube-system-psp
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: privileged-psp
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:nodes
  namespace: kube-system
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:serviceaccounts:kube-system</code></pre></div></p>

<p>都创建好之后，查看kube-system下的pod，发现apiserver已经创建成功
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl get po -n kube-system</span>
NAME                                   READY     STATUS    RESTARTS   AGE
coredns-68f5b48ccb-9hjvr               1/1       Running   <span class="m">0</span>          5d
coredns-68f5b48ccb-qrjnr               1/1       Running   <span class="m">0</span>          5d
etcd-build-master                      1/1       Running   <span class="m">0</span>          5d
kube-apiserver-build-master            1/1       Running   <span class="m">0</span>          18s</code></pre></div></p>

<blockquote><p>上面的binding只是为kube-system空间赋予了权限，若想要为别的命名空间赋予权限，可以使用ClusterRoleBinding的方式为多个namespace绑定privileged-psp的clusterrole，或者像如下方式，为所有合法的serviceaccounts和user绑定权限，当然前提是pod使用了namespace下的serviceAccount
<div class="highlight"><pre><code class="language-bash" data-lang="bash">&lt;/p&gt;

&lt;h1&gt;Authorize all service accounts in a namespace:&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;kind: Group
apiGroup: rbac.authorization.k8s.io
name: system:serviceaccounts

&lt;h1&gt;Or equivalently, all authenticated users in a namespace:&lt;/h1&gt;&lt;/li&gt;
&lt;li&gt;kind: Group
apiGroup: rbac.authorization.k8s.io
name: system:authenticated</code></pre></div></li>
</ul>
</blockquote>

<h2>验证psp的privileged限制</h2>

<p>该章节会创建一个名为psp-namespace的namespace做测试，来验证psp中对privileged的pod的限制
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl create namespace psp-example</span>
namespace/psp-example created
<span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl create serviceaccount fake-user -n psp-example</span>
serviceaccount/fake-user created
<span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl run &amp;ndash;image=nginx psp-pod-nginx-1 -n psp-example &amp;ndash;serviceaccount=fake-user</span>
deployment.apps/psp-pod-nginx-1 created
<span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl get po -n psp-example</span>
No resources found.
<span class="o">[</span>root@build-master apiserver<span class="o">]</span><span class="c"># kubectl get deploy -n psp-example</span>
NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
psp-pod-nginx-1   <span class="m">1</span>         <span class="m">0</span>         <span class="m">0</span>            <span class="m">0</span>           23s</code></pre></div></p>

<p>如上所示，新的psp-example命名空间由于没有任何权限，所以无法创建新pod，接下来我们创建一个policy并给psp-example做authorize，但是policy中会限制无法创建privileged的pod
<div class="highlight"><pre><code class="language-bash" data-lang="bash">apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: example
spec:
  privileged: <span class="nb">false</span>  <span class="c"># Don&amp;rsquo;t allow privileged pods!</span>
  <span class="c"># The rest fills in some required fields.</span>
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  runAsUser:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  volumes:
  - <span class="p">&amp;</span>lsquo<span class="p">;</span>*<span class="p">&amp;</span>rsquo<span class="p">;</span></code></pre></div></p>

<p> 然后创建role和rolebinding，并验证fake-user是否有权限使用example的policy
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master psp-example<span class="o">]</span><span class="c"># kubectl -n psp-example create role psp:unprivileged &amp;ndash;verb=use &amp;ndash;resource=podsecuritypolicy &amp;ndash;resource-name=example</span>
role.rbac.authorization.k8s.io/psp:unprivileged created
<span class="o">[</span>root@build-master psp-example<span class="o">]</span><span class="c"># kubectl -n psp-example create rolebinding fake-user:psp:unprivileged &amp;ndash;role=psp:unprivileged &amp;ndash;serviceaccount=psp-example:fake-user</span>
rolebinding.rbac.authorization.k8s.io/fake-user:psp:unprivileged created
<span class="o">[</span>root@build-master psp-example<span class="o">]</span><span class="c">#  kubectl &amp;ndash;as=system:serviceaccount:psp-example:fake-user -n psp-example auth can-i use podsecuritypolicy/example</span>
yes</code></pre></div></p>

<p>等待片刻，再次查看刚才创建的deploy，发现已经调度成功
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master psp-example<span class="o">]</span><span class="c"># kubectl get deploy -n psp-example psp-pod-nginx-1</span>
NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
psp-pod-nginx-1   <span class="m">1</span>         <span class="m">1</span>         <span class="m">1</span>            <span class="m">1</span>           22m</code></pre></div></p>

<p>此时我们尝试在psp-example下创建一个privileged的特权容器，
<div class="highlight"><pre><code class="language-bash" data-lang="bash">apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    run: psp-pod-nginx-privileged
  name: psp-pod-nginx-privileged
  namespace: psp-example
spec:
  selector:
    matchLabels:
      run: psp-pod-nginx-privileged
  template:
    metadata:
      labels:
        run: psp-pod-nginx-privileged
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: psp-pod-nginx-privileged
      securityContext:
        privileged: <span class="nb">true</span>
<span class="nb">      </span>serviceAccount: fake-user</code></pre></div>
创建并查看结果
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@build-master psp-example<span class="o">]</span><span class="c"># kubectl create -f privileged-pod.yaml</span>
deployment.extensions/psp-pod-nginx-privileged created
<span class="o">[</span>root@build-master psp-example<span class="o">]</span><span class="c"># kubectl get deploy -n psp-example psp-pod-nginx-privileged</span>
psp-pod-nginx-privileged   <span class="m">1</span>         <span class="m">0</span>         <span class="m">0</span>            <span class="m">0</span>           24s
<span class="o">[</span>root@build-master ~<span class="o">]</span><span class="c"># kubectl logs &amp;ndash;tail=1 -f kube-controller-manager-build-master -n kube-system</span>
I0318 04:32:42.777200       <span class="m">1</span> event.go:221<span class="o">]</span> Event<span class="o">(</span>v1.ObjectReference<span class="o">{</span>Kind:<span class="p">&amp;</span>ldquo<span class="p">;</span>ReplicaSet<span class="p">&amp;</span>rdquo<span class="p">;</span>, Namespace:<span class="p">&amp;</span>ldquo<span class="p">;</span>psp-example<span class="p">&amp;</span>rdquo<span class="p">;</span>, Name:<span class="p">&amp;</span>ldquo<span class="p">;</span>psp-pod-nginx-privileged-d7f8dbd75<span class="p">&amp;</span>rdquo<span class="p">;</span>, UID:<span class="p">&amp;</span>ldquo<span class="p">;</span>c66fb10f-4936-11e9-9c58-00163e004fd2<span class="p">&amp;</span>rdquo<span class="p">;</span>, APIVersion:<span class="p">&amp;</span>ldquo<span class="p">;</span>apps/v1<span class="p">&amp;</span>rdquo<span class="p">;</span>, ResourceVersion:<span class="p">&amp;</span>ldquo<span class="p">;</span>773292<span class="p">&amp;</span>rdquo<span class="p">;</span>, FieldPath:<span class="p">&amp;</span>ldquo<span class="p">;&amp;</span>rdquo<span class="p">;</span><span class="o">})</span>: <span class="nb">type</span>: <span class="p">&amp;</span>lsquo<span class="p">;</span>Warning<span class="p">&amp;</span>rsquo<span class="p">;</span> reason: <span class="p">&amp;</span>lsquo<span class="p">;</span>FailedCreate<span class="p">&amp;</span>rsquo<span class="p">;</span> Error creating: pods <span class="p">&amp;</span>ldquo<span class="p">;</span>psp-pod-nginx-privileged-d7f8dbd75-<span class="p">&amp;</span>rdquo<span class="p">;</span> is forbidden: unable to validate against any pod security policy: <span class="o">[</span>spec.containers<span class="o">[</span>0<span class="o">]</span>.securityContext.privileged: Invalid value: <span class="nb">true</span>: Privileged containers are not allowed<span class="o">]</span></code></pre></div>
查看controller-manager的日志，发现Privileged containers are not allowed</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[somethings about kubernetes]]></title>
    <link href="http://liyongxin.github.io/blog/2019/02/28/k8sxiao-zhi-shi/"/>
    <updated>2019-02-28T11:12:56+08:00</updated>
    <id>http://liyongxin.github.io/blog/2019/02/28/k8sxiao-zhi-shi</id>
    <content type="html"><![CDATA[<ul>
<li><p>k8s默认驱逐设置
<div class="highlight"><pre><code class="language-bash" data-lang="bash">// DefaultEvictionHard includes default options <span class="k">for</span> hard eviction.
var <span class="nv">DefaultEvictionHard</span> <span class="o">=</span> map<span class="o">[</span>string<span class="o">]</span>string<span class="o">{</span>
      <span class="p">&amp;</span>ldquo<span class="p">;</span>memory.available<span class="p">&amp;</span>rdquo<span class="p">;</span>:  <span class="p">&amp;</span>ldquo<span class="p">;</span>100Mi<span class="p">&amp;</span>rdquo<span class="p">;</span>,
      <span class="p">&amp;</span>ldquo<span class="p">;</span>nodefs.available<span class="p">&amp;</span>rdquo<span class="p">;</span>:  <span class="p">&amp;</span>ldquo<span class="p">;</span>10%<span class="p">&amp;</span>rdquo<span class="p">;</span>,
      <span class="p">&amp;</span>ldquo<span class="p">;</span>nodefs.inodesFree<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>5%<span class="p">&amp;</span>rdquo<span class="p">;</span>,
      <span class="p">&amp;</span>ldquo<span class="p">;</span>imagefs.available<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>15%<span class="p">&amp;</span>rdquo<span class="p">;</span>,
<span class="o">}</span></code></pre></div></p></li>
<li><p>kubernetes 滚动升级
<div class="highlight"><pre><code class="language-bash" data-lang="bash">&lt;/p&gt;

&lt;h1&gt;run <span class="nb">test </span>deploy&lt;/h1&gt;

<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl run &amp;ndash;image=nginx &amp;ndash;port=80 &amp;ndash;replicas=2 yxli-nginx</span>

&lt;h1&gt;scale replica&lt;/h1&gt;

<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl scale &amp;ndash;replicas=1 deploy/yxli-nginx</span>
<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl get deploy</span>
NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
busybox      <span class="m">2</span>         <span class="m">2</span>         <span class="m">2</span>            <span class="m">2</span>           39d
busybox1     <span class="m">1</span>         <span class="m">1</span>         <span class="m">1</span>            <span class="m">1</span>           39d
yxli-nginx   <span class="m">1</span>         <span class="m">1</span>         <span class="m">1</span>            <span class="m">1</span>           2h

&lt;h1&gt;update image&lt;/h1&gt;

<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl set image  deploy/yxli-nginx yxli-nginx=nginx:alpine</span>

&lt;h1&gt;查看升级历史&lt;/h1&gt;

<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl rollout history deploy/yxli-nginx</span>
deployments <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-nginx<span class="p">&amp;</span>rdquo<span class="p">;</span>
REVISION  CHANGE-CAUSE
<span class="m">1</span>         &lt;none&gt;
<span class="m">2</span>         &lt;none&gt;

&lt;h1&gt;回顾至上次版本&lt;/h1&gt;

<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl rollout undo deploy/yxli-nginx</span>
<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl rollout history deploy/yxli-nginx</span>
deployments <span class="p">&amp;</span>ldquo<span class="p">;</span>yxli-nginx<span class="p">&amp;</span>rdquo<span class="p">;</span>
REVISION  CHANGE-CAUSE
<span class="m">2</span>         &lt;none&gt;
<span class="m">3</span>         &lt;none&gt;

&lt;h1&gt;回滚至指定版本&lt;/h1&gt;

&lt;p&gt;<span class="o">[</span>root@k8s-master ~<span class="o">]</span><span class="c"># kubectl rolloutundo deployment/lykops-dpm &amp;ndash;to-revision=2</span></code></pre></div></p></li>
</ul>


<!--more-->


<ul>
<li>查看docker使用的cpu核心数
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@yxli-onebox docker<span class="o">]</span><span class="c"># pwd</span>
/sys/fs/cgroup/cpuset/docker
<span class="o">[</span>root@yxli-onebox docker<span class="o">]</span><span class="c"># cat cpuset.cpus</span>
0-7</code></pre></div></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kubernetes 问题整理]]></title>
    <link href="http://liyongxin.github.io/blog/2018/08/31/kubernetes-wen-ti-zheng-li/"/>
    <updated>2018-08-31T10:36:27+08:00</updated>
    <id>http://liyongxin.github.io/blog/2018/08/31/kubernetes-wen-ti-zheng-li</id>
    <content type="html"><![CDATA[<p>本文用以kubernetes 运维过程中遇到问题汇总，方便日后回顾～</p>

<h2>kubernetes多网卡导致的问题</h2>

<p>部署机器是阿里云，有两块网卡，<code>eth0</code>外网，<code>eth1 vpc</code>内网，集群的路由信息如下
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@10 src<span class="o">]</span><span class="c"># route</span>
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
default         gateway         0.0.0.0         UG    <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> eth1
10.0.0.0        10.81.35.247    255.0.0.0       UG    <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> eth0
10.81.32.0      0.0.0.0         255.255.252.0   U     <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> eth0
39.107.40.0     0.0.0.0         255.255.252.0   U     <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> eth1
100.64.0.0      10.81.35.247    255.192.0.0     UG    <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> eth0
link-local      0.0.0.0         255.255.0.0     U     <span class="m">1002</span>   <span class="m">0</span>        <span class="m">0</span> eth0
link-local      0.0.0.0         255.255.0.0     U     <span class="m">1003</span>   <span class="m">0</span>        <span class="m">0</span> eth1
172.16.0.0      10.81.35.247    255.240.0.0     UG    <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> eth0
192.168.0.0     0.0.0.0         255.255.255.0   U     <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> cni0
192.168.0.0     0.0.0.0         255.255.240.0   U     <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> docker0
192.168.1.0     192.168.1.0     255.255.255.0   UG    <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> flannel.1
192.168.2.0     192.168.2.0     255.255.255.0   UG    <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> flannel.1</code></pre></div></p>

<ul>
<li><h4>docker0网段与cni0网段冲突问题</h4></li>
</ul>


<p>docker启动时没有指定bip，从上述路由规则发现，docker0使用了192.168的段,刚好给flannel设置的cidr段冲突，
所以需要给docker修改默认的网段，解决方法是给docker配置bip网段,然后重启docker，观察docker0的route规则</p>

<p><div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@10 ~<span class="o">]</span><span class="c"># cat /etc/docker/daemon.json</span>
<span class="o">{</span>
    <span class="p">&amp;</span>ldquo<span class="p">;</span>insecure-registries<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="o">[]</span>,
    <span class="p">&amp;</span>ldquo<span class="p">;</span>graph<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>/var/lib/docker<span class="p">&amp;</span>rdquo<span class="p">;</span>,
    <span class="p">&amp;</span>ldquo<span class="p">;</span>bip<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>172.17.0.1/16<span class="p">&amp;</span>rdquo<span class="p">;</span>,
    <span class="p">&amp;</span>ldquo<span class="p">;</span>registry-mirrors<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="o">[</span><span class="p">&amp;</span>ldquo<span class="p">;</span>&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://registry.docker-cn.com&quot;</span>&gt;https://registry.docker-cn.com&lt;/a&gt;<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">]</span>,
    <span class="p">&amp;</span>ldquo<span class="p">;</span>storage-driver<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>devicemapper<span class="p">&amp;</span>rdquo<span class="p">;</span>,
    <span class="p">&amp;</span>ldquo<span class="p">;</span>storage-opts<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="o">[</span><span class="p">&amp;</span>ldquo<span class="p">;</span>dm.use_deferred_removal<span class="o">=</span><span class="nb">true</span><span class="p">&amp;</span>rdquo<span class="p">;</span>, <span class="p">&amp;</span>ldquo<span class="p">;</span>dm.use_deferred_deletion<span class="o">=</span><span class="nb">true</span><span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">]</span>,
    <span class="p">&amp;</span>ldquo<span class="p">;</span>storage-opts<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="o">[</span>
        <span class="p">&amp;</span>ldquo<span class="p">;</span>dm.thinpooldev<span class="o">=</span>/dev/mapper/docker-thinpool<span class="p">&amp;</span>rdquo<span class="p">;</span>,
        <span class="p">&amp;</span>ldquo<span class="p">;</span>dm.min_free_space<span class="o">=</span>0%<span class="p">&amp;</span>rdquo<span class="p">;</span>,
        <span class="p">&amp;</span>ldquo<span class="p">;</span>dm.use_deferred_deletion<span class="o">=</span><span class="nb">true</span><span class="p">&amp;</span>rdquo<span class="p">;</span>,
        <span class="p">&amp;</span>ldquo<span class="p">;</span>dm.use_deferred_removal<span class="o">=</span><span class="nb">true</span><span class="p">&amp;</span>rdquo<span class="p">;</span>,
        <span class="p">&amp;</span>ldquo<span class="p">;</span>dm.fs<span class="o">=</span>ext4<span class="p">&amp;</span>rdquo<span class="p">;</span>
    <span class="o">]</span>,
    <span class="p">&amp;</span>ldquo<span class="p">;</span>log-driver<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>fluentd<span class="p">&amp;</span>rdquo<span class="p">;</span>,
    <span class="p">&amp;</span>ldquo<span class="p">;</span>log-opts<span class="p">&amp;</span>rdquo<span class="p">;</span>:
    <span class="o">{</span>
        <span class="p">&amp;</span>ldquo<span class="p">;</span>fluentd-address<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>localhost:24224<span class="p">&amp;</span>rdquo<span class="p">;</span>,
        <span class="p">&amp;</span>ldquo<span class="p">;</span>tag<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>docker.<span class="p">&amp;</span>rdquo<span class="p">;</span>,
        <span class="p">&amp;</span>ldquo<span class="p">;</span>fluentd-async-connect<span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span><span class="nb">true</span><span class="p">&amp;</span>rdquo<span class="p">;</span>
    <span class="o">}</span>
<span class="o">}</span></code></pre></div></p>

<ul>
<li><h4>集群初始化问题</h4></li>
</ul>


<p>使用<code>kubeadm</code>搭建，若未指定&ndash;advertise-address地址则k8s默认拿default网卡，
而机器的default网卡刚好是外网eth0，所以初始化集群使用的地址是外网地址，导致一堆端口需要开，然后Node加入集群失败，解决办法是kubeadm初始化的
时候指定&ndash;advertise-address为内网地址,下面为kubeadm init使用的conf文件</p>

<!--more-->


<p><div class="highlight"><pre><code class="language-bash" data-lang="bash">apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
api:
  bindPort: 6443
etcd:
  endpoints:
  <span class="c">#sed -i &amp;ldquo;/#ETCD_ENDPOINTS/a\  - &lt;a href=&quot;http://123.456:2379/g&quot;&gt;http://123.456:2379/g&lt;/a&gt;&amp;rdquo; ./abc.yml</span>
  <span class="c">#ETCD ENDPOINTS</span>
  - &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;http://10.81.32.150:2379&quot;</span>&gt;http://10.81.32.150:2379&lt;/a&gt;
apiServerExtraArgs:
  apiserver-count: <span class="p">&amp;</span>ldquo<span class="p">;</span>1<span class="p">&amp;</span>rdquo<span class="p">;</span>
  insecure-port: <span class="p">&amp;</span>ldquo<span class="p">;</span>8080<span class="p">&amp;</span>rdquo<span class="p">;</span>
  advertise-address<span class="o">=</span>10.81.32.150
  service-node-port-range: <span class="p">&amp;</span>ldquo<span class="p">;</span>30000-32000<span class="p">&amp;</span>rdquo<span class="p">;</span>
  admission-control: <span class="p">&amp;</span>ldquo<span class="p">;</span>Initializers,NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,DefaultTolerationSeconds,ResourceQuota<span class="p">&amp;</span>rdquo<span class="p">;</span>
  feature-gates: <span class="p">&amp;</span>ldquo<span class="p">;</span><span class="nv">MountPropagation</span><span class="o">=</span><span class="nb">true</span><span class="p">&amp;</span>rdquo<span class="p">;</span>
  endpoint-reconciler-type: <span class="p">&amp;</span>ldquo<span class="p">;</span>lease<span class="p">&amp;</span>rdquo<span class="p">;</span>
controllerManagerExtraArgs:
  pod-eviction-timeout: <span class="p">&amp;</span>ldquo<span class="p">;</span>30s<span class="p">&amp;</span>rdquo<span class="p">;</span>
  node-monitor-period: <span class="p">&amp;</span>ldquo<span class="p">;</span>2s<span class="p">&amp;</span>rdquo<span class="p">;</span>
  node-monitor-grace-period: <span class="p">&amp;</span>ldquo<span class="p">;</span>16s<span class="p">&amp;</span>rdquo<span class="p">;</span>
controllerManagerExtraVolumes:
- name: k8s
  hostPath: /etc/kubernetes
  mountPath: /etc/kubernetes
imageRepository: index.docker.cn/claas
networking:
  podSubnet: 192.168.0.0/16
kubernetesVersion: v1.9.6
token: 8d775a.8f70da6999842a27
tokenTTL: <span class="p">&amp;</span>ldquo<span class="p">;</span>0<span class="p">&amp;</span>rdquo<span class="p">;</span>
apiServerCertSANs:
- 127.0.0.1
- amazonaws.com.cn
- amazonaws.com
- 10.81.32.150
- 10.81.32.150</code></pre></div>
- ####flannel网络问题
多网卡导致flannel网络选择网卡错误,flannel在初始化的时候会默认找defalut网卡，如果需要指定，则在flannel的
初始化yaml文件中通过&ndash;iface指定网卡，
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="p">&amp;</span>hellip<span class="p">;</span> <span class="p">&amp;</span>hellip<span class="p">;</span>
      containers:
      - name: kube-flannel
        image: index.alauda.cn/claas/flannel:v0.9.1
        <span class="nb">command</span>: <span class="o">[</span> <span class="p">&amp;</span>ldquo<span class="p">;</span>/opt/bin/flanneld<span class="p">&amp;</span>rdquo<span class="p">;</span>, <span class="p">&amp;</span>ldquo<span class="p">;&amp;</span>ndash<span class="p">;</span>ip-masq<span class="p">&amp;</span>rdquo<span class="p">;</span>, <span class="p">&amp;</span>ldquo<span class="p">;&amp;</span>ndash<span class="p">;</span>kube-subnet-mgr<span class="p">&amp;</span>rdquo<span class="p">;</span>, <span class="p">&amp;</span>ldquo<span class="p">;&amp;</span>ndash<span class="p">;</span><span class="nv">iface</span><span class="o">=</span>eth0<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">]</span>
        securityContext:
          privileged: <span class="nb">true</span>
<span class="nb">        </span>env:
  <span class="p">&amp;</span>hellip<span class="p">;</span> <span class="p">&amp;</span>hellip<span class="p">;</span></code></pre></div></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kubelet的认证]]></title>
    <link href="http://liyongxin.github.io/blog/2018/08/25/kubeletde-ren-zheng/"/>
    <updated>2018-08-25T18:17:17+08:00</updated>
    <id>http://liyongxin.github.io/blog/2018/08/25/kubeletde-ren-zheng</id>
    <content type="html"><![CDATA[<p>研究完<a href="../kuberneteszhong-de-ren-zheng-xiang-guan">kubectl的认证与授权</a>，使用相同的方式去找kubelet的访问，
首先定位配置文件<code>/etc/kubernetes/kubelet.conf</code>，然后用相同的方式对<code>client-key-data</code>做base64解码，保存为kubelet.crt文件。</p>

<p>openssl查看crt证书，
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@k8s-master kubernetes<span class="o">]</span><span class="c"># openssl x509 -text -in kubelet.crt -noout</span>
Certificate:
    Data:
        Version: <span class="m">3</span> <span class="o">(</span>0x2<span class="o">)</span>
        Serial Number: <span class="m">8126553944389053218</span> <span class="o">(</span>0x70c751c18f5beb22<span class="o">)</span>
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: <span class="nv">CN</span><span class="o">=</span>kubernetes
        Validity
            Not Before: Aug <span class="m">20</span> 05:50:39 <span class="m">2018</span> GMT
            Not After : Aug <span class="m">20</span> 05:50:42 <span class="m">2019</span> GMT
        Subject: <span class="nv">O</span><span class="o">=</span>system:nodes, <span class="nv">CN</span><span class="o">=</span>system:node:k8s-master
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: <span class="o">(</span><span class="m">2048</span> bit<span class="o">)</span>
                Modulus:
                    00:9f:92:83:49:aa:cc:52:0e:de:bd:af:a6:fd:ef:
    <span class="p">&amp;</span>hellip<span class="p">;</span> <span class="p">&amp;</span>hellip<span class="p">;</span></code></pre></div></p>

<p>得到我们期望的内容：
<div class="highlight"><pre><code class="language-bash" data-lang="bash">Subject: <span class="nv">O</span><span class="o">=</span>system:nodes, <span class="nv">CN</span><span class="o">=</span>system:node:k8s-master</code></pre></div></p>

<blockquote><p>关于Subject，在k8s中可以理解为角色绑定主体，RoleBinding或者ClusterRoleBinding可以将角色绑定到角色绑定主体（Subject）。
角色绑定主体可以是用户组（Group）、用户（User）或者服务账户（Service Accounts）</p></blockquote>

<p>然后我尝试去k8s中找到一些关于<code>system:nodes</code>的RoleBindings或者ClusterRoleBindings,
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@k8s-master kubernetes<span class="o">]</span><span class="c"># for bd in &lt;code&gt;kubectl get clusterrolebindings |awk &#39;{print $1}&#39;&lt;/code&gt;; do echo $bd;kubectl get clusterrolebindings $bd -o yaml|grep &amp;lsquo;system:nodes&amp;rsquo;;done</span>
NAME
Error from server <span class="o">(</span>NotFound<span class="o">)</span>: clusterrolebindings.rbac.authorization.k8s.io <span class="p">&amp;</span>ldquo<span class="p">;</span>NAME<span class="p">&amp;</span>rdquo<span class="p">;</span> not found
cluster-admin
flannel
kubeadm:kubelet-bootstrap
kubeadm:node-autoapprove-bootstrap
kubeadm:node-autoapprove-certificate-rotation
  name: system:nodes
kubeadm:node-proxier
system:aws-cloud-provider
system:basic-user
<span class="p">&amp;</span>hellip<span class="p">;</span> <span class="p">&amp;</span>hellip<span class="p">;</span></code></pre></div></p>

<p>结局有点意外，除了<code>kubeadm:node-autoapprove-certificate-rotation</code>外，没有找到system相关的rolebindings，显然和我们的理解不一样。
尝试去找<a href="https://github.com/rootsongjc/kubernetes-handbook/blob/master/guide/rbac.md">资料</a>，发现了这么一段</p>

<p><img src="/images/rbac.png" alt="" /></p>

<p>Kubernetes 1.7开始, apiserver的启动中默认增加了<code>--authorization-mode=Node,RBAC</code>,也就是说，除了RBAC外，还有Node这种特殊的授权方式，</p>

<p>继续查找<a href="https://kubernetes.io/docs/reference/access-authn-authz/node/">kubernetes官方的信息</a>,得知
<div class="highlight"><pre><code class="language-bash" data-lang="bash">Node authorization is a special-purpose authorization mode that specifically authorizes API requests made by kubelets.</code></pre></div></p>
]]></content>
  </entry>
  
</feed>
